{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"themes/landscape/source/css/style.styl","path":"css/style.styl","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/blank.gif","path":"fancybox/blank.gif","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_overlay.png","path":"fancybox/fancybox_overlay.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_loading.gif","path":"fancybox/fancybox_loading.gif","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_loading@2x.gif","path":"fancybox/fancybox_loading@2x.gif","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_sprite.png","path":"fancybox/fancybox_sprite.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_sprite@2x.png","path":"fancybox/fancybox_sprite@2x.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.css","path":"fancybox/jquery.fancybox.css","modified":0,"renderable":1},{"_id":"themes/landscape/source/js/script.js","path":"js/script.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.js","path":"fancybox/jquery.fancybox.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.pack.js","path":"fancybox/jquery.fancybox.pack.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/FontAwesome.otf","path":"css/fonts/FontAwesome.otf","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.eot","path":"css/fonts/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.woff","path":"css/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/images/banner.jpg","path":"css/images/banner.jpg","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/fancybox_buttons.png","path":"fancybox/helpers/fancybox_buttons.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.css","path":"fancybox/helpers/jquery.fancybox-buttons.css","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.js","path":"fancybox/helpers/jquery.fancybox-buttons.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.js","path":"fancybox/helpers/jquery.fancybox-thumbs.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.css","path":"fancybox/helpers/jquery.fancybox-thumbs.css","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-media.js","path":"fancybox/helpers/jquery.fancybox-media.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.ttf","path":"css/fonts/fontawesome-webfont.ttf","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.svg","path":"css/fonts/fontawesome-webfont.svg","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/images/banner-bak.jpg","path":"css/images/banner-bak.jpg","modified":0,"renderable":1}],"Cache":[{"_id":"source/.DS_Store","hash":"853a0a79f0801961c7879e3079833d3bf90714c2","modified":1492520722000},{"_id":"themes/landscape/.gitignore","hash":"58d26d4b5f2f94c2d02a4e4a448088e4a2527c77","modified":1484126821000},{"_id":"themes/landscape/Gruntfile.js","hash":"71adaeaac1f3cc56e36c49d549b8d8a72235c9b9","modified":1484126821000},{"_id":"themes/landscape/LICENSE","hash":"c480fce396b23997ee23cc535518ffaaf7f458f8","modified":1484126821000},{"_id":"themes/landscape/.DS_Store","hash":"9457f542cf2c3f2a50b9ecd64858fc6f50b0d0c4","modified":1484147080000},{"_id":"themes/landscape/README.md","hash":"c7e83cfe8f2c724fc9cac32bd71bb5faf9ceeddb","modified":1484126821000},{"_id":"themes/landscape/_config.yml","hash":"285fb948f9f50e0d2c040283251b1ce20119bfaa","modified":1484214819000},{"_id":"themes/landscape/package.json","hash":"85358dc34311c6662e841584e206a4679183943f","modified":1484126821000},{"_id":"source/_posts/bigdata-hadoop-portal.md","hash":"eff5d635890d8e3469987eddb9ce65540d521c36","modified":1492521320000},{"_id":"source/_posts/bigdata-ml-portal.md","hash":"7c76655889c83b3a1dfbd8f324dd76c8bb5e0e6a","modified":1492521317000},{"_id":"source/_posts/bigdata-storm-portal.md","hash":"e1210278d50f73368ca5d1e8b13b1f2c169dafa7","modified":1492521314000},{"_id":"source/_posts/cloud--portal.md","hash":"44eb0d6a31142f44f36b293710636a1745f06540","modified":1492521307000},{"_id":"source/_posts/cloud-iaas-openstack-core.md","hash":"0d4166c96995270266b771997fd120cf64fbce54","modified":1492521285000},{"_id":"source/_posts/cloud-paas-cf-core.md","hash":"a61ee4e57c0a3363d768c13503afc9375fe038c1","modified":1492521333000},{"_id":"source/_posts/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1492521779000},{"_id":"source/_posts/blockchain-portal.md","hash":"7c9544e68778603f0ed6720858a96735e552d271","modified":1491187563000},{"_id":"source/_posts/cloud-paas-portal.md","hash":"3a52bd5760694504c2e782248e589f77b322ffb9","modified":1492521343000},{"_id":"source/_posts/cloud-paas-openshift-core.md","hash":"cf9717a999dab5322a0d43c294f655d6bfba731f","modified":1492521337000},{"_id":"source/_posts/cloud-saas-portal.md","hash":"ec1287d84de3ae7cb9545e8963617eacf502f098","modified":1492521384000},{"_id":"source/_posts/community.md","hash":"42fe8993296a2ec1a93c369dd14a2b1bdc4e3b6c","modified":1492521389000},{"_id":"source/_posts/bigdata--portal.md","hash":"24a07091d9b8436f253f5d2b44714e466d9f92c2","modified":1486644435000},{"_id":"source/_posts/cloud-iaas-portal.md","hash":"891e7a4e7d25ca8433b2166842e8bf5043aca256","modified":1487397085000},{"_id":"source/_posts/db--portal.md","hash":"571d55ed1ff551646f2dfe0330792d267727c440","modified":1486198547000},{"_id":"source/_posts/db-mysql-portal.md","hash":"a2be628fedb9c27726a58151963cddd6877cf1fb","modified":1492521413000},{"_id":"source/_posts/db-nosql-portal.md","hash":"bc87f4e0c2ed8f054ab860531942fb7b8a8b429c","modified":1492521422000},{"_id":"source/_posts/devops--portal.md","hash":"576b51b1bebca7cf957b40583fb72915c7a04f37","modified":1492521430000},{"_id":"source/_posts/docker-cloud-detail.md","hash":"805ab2f8efd8b78edaa59be5fe60bbac56d2aa1a","modified":1492521467000},{"_id":"source/_posts/docker-compose-detail.md","hash":"5ba4bfe293e5ad18e801d97656eb41ea3d4de743","modified":1486738987000},{"_id":"source/_posts/docker--portal.md","hash":"f8b7bfd54a7f5ca35dbb1011c704c6dd3988820b","modified":1486698059000},{"_id":"source/_posts/docker-compose-file-detail.md","hash":"3ebd283b24cef8504c8137dbb219ca68dd1e1c9e","modified":1492521481000},{"_id":"source/_posts/docker-dockerfile-detail.md","hash":"b4deb4790f33f13fa3c27d05dc5ceb33c7eea003","modified":1492521505000},{"_id":"source/_posts/docker-dockerhub-detail.md","hash":"d73c876795bd2175c53d186b816dd2585f00a254","modified":1484307994000},{"_id":"source/_posts/docker-engine-detail.md","hash":"633500ff5a32f2ea0f2cd07b784086da26ff1973","modified":1486704843000},{"_id":"source/_posts/docker-cookbook.md","hash":"8caa9454e90d4ade8268c325399b17a29c48c945","modified":1484304719000},{"_id":"source/_posts/docker-core.md","hash":"371801da82979044f60999285279fc46cc8c8b12","modified":1484461693000},{"_id":"source/_posts/docker-filesystem-core.md","hash":"f57c094ccfa1ff199415a9bdc571f65fc25651e1","modified":1484465420000},{"_id":"source/_posts/docker-k8s-core.md","hash":"6a593001c315f1071991f94bc18f8f6b3b8a09a3","modified":1492521522000},{"_id":"source/_posts/docker-image-detail.md","hash":"21421ab56a9a54fe66765d998db6df3dab394563","modified":1484308232000},{"_id":"source/_posts/docker-k8s-install.md","hash":"5f3a87642cc04d6429da1161f5eaed2b161030f8","modified":1487414202000},{"_id":"source/_posts/docker-machine-detail.md","hash":"29ffe0a3514e51cd741778fbf87becfcf759f828","modified":1486705720000},{"_id":"source/_posts/docker-network-core.md","hash":"0d826cbb578c9f328298c31b57b8665490935a0a","modified":1492521677000},{"_id":"source/_posts/docker-security-core.md","hash":"e464af5b6e1b00d03ea5a74b01eca0152c3ffdc9","modified":1492521669000},{"_id":"source/_posts/docker-store-detail.md","hash":"437d6b7e6bd8e1e5da6778db1c02fed22eaf3a0a","modified":1492521652000},{"_id":"source/_posts/docker-storage-core.md","hash":"eb414918163bb59b704a742acb305de04d83d278","modified":1492521673000},{"_id":"source/_posts/docker-swarm-detail.md","hash":"39f88e03b35b996464f2807928df751ba67f21ae","modified":1492521681000},{"_id":"source/_posts/docker-swarmkit-detail.md","hash":"6cabdc36247cce71c389c83923f8e749c8e94053","modified":1492521708000},{"_id":"source/_posts/docker-mesos-portal.md","hash":"79cceb9acef6d9b3b33fd3825493f6f62e078b98","modified":1492521558000},{"_id":"source/_posts/docker-yaml-detail.md","hash":"0cf6506578da4d91d14743192fd717574dad77da","modified":1492521738000},{"_id":"source/_posts/docker-mesos-core.md","hash":"d8a3215f44c3d11510bad3bc45813e695847ff0e","modified":1486707416000},{"_id":"source/_posts/iot-portal.md","hash":"b901e676dc3bfe534a45d08e29b5a21d66ddcd56","modified":1492521761000},{"_id":"source/_posts/lang--portal.md","hash":"350400dfcd80340a55a235dc8e8d845bf08aede2","modified":1492521797000},{"_id":"source/_posts/lang-cx-portal.md","hash":"30415d2dc4e1d73f59e32b44d7f415ae36262f5a","modified":1492521837000},{"_id":"source/_posts/lang-go-portal.md","hash":"4bca144199135650b72c835f999ea0c806b11adc","modified":1492521833000},{"_id":"source/_posts/lang-java-portal.md","hash":"2fb5739fefac1515cb279c4b0ea01b4255d8524e","modified":1492521829000},{"_id":"source/_posts/lang-js-nodejs-core.md","hash":"765f006786e38f695d6f349f6ee0542d98993f42","modified":1492521848000},{"_id":"source/_posts/lang-js-portal.md","hash":"d2280fb6a286069bf22e1943a4072ae111bdfdde","modified":1492521864000},{"_id":"source/_posts/lang-php-portal.md","hash":"5e9aa95083281d95a735e2262bd0543b10785cb2","modified":1492521878000},{"_id":"source/_posts/hello-world.md","hash":"2821cef1e0b6723d18e7f2162653fe696c4dbdc0","modified":1484152366000},{"_id":"source/_posts/docker-swarm-portal.md","hash":"bf6b2396067641ad231be2c88ffdded4a226f032","modified":1486705236000},{"_id":"source/_posts/lang-python-django-core.md","hash":"b58bfdbcca877f4c3041e6e25fa36b7f2be82140","modified":1484214459000},{"_id":"source/_posts/docker-swarmnext-detail.md","hash":"859394563adc3ede668fa8e47d0c826e4b249199","modified":1484486949000},{"_id":"source/_posts/lang-python-django-install.md","hash":"7f922a7b5eb9ee80cb5cd6884eb3d87f81a0d595","modified":1484214542000},{"_id":"source/_posts/lang-python-portal.md","hash":"2278727305b54f93d9b7015688a6b0d9da78f07d","modified":1488165095000},{"_id":"source/_posts/lang-ruby-rails-core.md","hash":"409d5e7c9bada410b4818fb0f2c1b364623e0e05","modified":1488127169000},{"_id":"source/_posts/microservice-portal.md","hash":"e42d0d7927147598756d9f5b2f6ea7428105fde2","modified":1486196553000},{"_id":"source/about/index.md","hash":"472e437a5f1ed0d518d5bb40002da88297e1daa7","modified":1492522084000},{"_id":"source/_posts/network-http-core.md","hash":"a5853d8ae8a690d707ee6e5e669ea3470c492940","modified":1484152471000},{"_id":"source/_posts/network-portal.md","hash":"1d97325d88b500484030a30cafd44c41b51c358c","modified":1484486959000},{"_id":"source/_posts/system-linux-portal.md","hash":"4e023a556e18a248bb932ff4067513be2779b1c0","modified":1484152446000},{"_id":"themes/landscape/languages/default.yml","hash":"3083f319b352d21d80fc5e20113ddf27889c9d11","modified":1484126821000},{"_id":"themes/landscape/languages/fr.yml","hash":"84ab164b37c6abf625473e9a0c18f6f815dd5fd9","modified":1484126821000},{"_id":"themes/landscape/languages/nl.yml","hash":"12ed59faba1fc4e8cdd1d42ab55ef518dde8039c","modified":1484126821000},{"_id":"themes/landscape/languages/no.yml","hash":"965a171e70347215ec726952e63f5b47930931ef","modified":1484126821000},{"_id":"themes/landscape/languages/ru.yml","hash":"4fda301bbd8b39f2c714e2c934eccc4b27c0a2b0","modified":1484126821000},{"_id":"themes/landscape/languages/zh-CN.yml","hash":"ca40697097ab0b3672a80b455d3f4081292d1eed","modified":1484126821000},{"_id":"themes/landscape/languages/zh-TW.yml","hash":"53ce3000c5f767759c7d2c4efcaa9049788599c3","modified":1484126821000},{"_id":"themes/landscape/layout/archive.ejs","hash":"2703b07cc8ac64ae46d1d263f4653013c7e1666b","modified":1484126821000},{"_id":"themes/landscape/layout/index.ejs","hash":"aa1b4456907bdb43e629be3931547e2d29ac58c8","modified":1484126821000},{"_id":"themes/landscape/layout/category.ejs","hash":"765426a9c8236828dc34759e604cc2c52292835a","modified":1484126821000},{"_id":"themes/landscape/layout/layout.ejs","hash":"f155824ca6130080bb057fa3e868a743c69c4cf5","modified":1484126821000},{"_id":"themes/landscape/scripts/fancybox.js","hash":"aa411cd072399df1ddc8e2181a3204678a5177d9","modified":1484126821000},{"_id":"themes/landscape/layout/post.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1484126821000},{"_id":"themes/landscape/layout/tag.ejs","hash":"eaa7b4ccb2ca7befb90142e4e68995fb1ea68b2e","modified":1484126821000},{"_id":"themes/landscape/layout/page.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1484126821000},{"_id":"themes/landscape/source/.DS_Store","hash":"1429b9c08aeb8c15b3499d150e011720f8ff275c","modified":1484147087000},{"_id":"themes/landscape/layout/_partial/article.ejs","hash":"3ff1260ab513c523a610f1d83b20961b5d140d6b","modified":1484213973000},{"_id":"themes/landscape/layout/_partial/after-footer.ejs","hash":"9292c640bdf7c8eb6fed2e8a1800f1cc7f43722b","modified":1484145585000},{"_id":"themes/landscape/layout/_partial/archive-post.ejs","hash":"c7a71425a946d05414c069ec91811b5c09a92c47","modified":1484126821000},{"_id":"themes/landscape/layout/_partial/archive.ejs","hash":"931aaaffa0910a48199388ede576184ff15793ee","modified":1484126821000},{"_id":"themes/landscape/layout/_partial/footer.ejs","hash":"f6975a227829834c026b17ee2493d06a16202b94","modified":1484144946000},{"_id":"themes/landscape/layout/_partial/head.ejs","hash":"bfd64f2a831a6acb7f5bae852cae3098a91e1997","modified":1484145371000},{"_id":"themes/landscape/layout/_partial/mobile-nav.ejs","hash":"e952a532dfc583930a666b9d4479c32d4a84b44e","modified":1484126821000},{"_id":"themes/landscape/layout/_partial/header.ejs","hash":"9d7b18ae2a5479d9ae0eb053ea7043ab8a9bd642","modified":1484145235000},{"_id":"themes/landscape/layout/_widget/archive.ejs","hash":"beb4a86fcc82a9bdda9289b59db5a1988918bec3","modified":1484126821000},{"_id":"themes/landscape/layout/_partial/sidebar.ejs","hash":"930da35cc2d447a92e5ee8f835735e6fd2232469","modified":1484126821000},{"_id":"themes/landscape/layout/_widget/category.ejs","hash":"dd1e5af3c6af3f5d6c85dfd5ca1766faed6a0b05","modified":1484126821000},{"_id":"themes/landscape/layout/_widget/recent_posts.ejs","hash":"0d4f064733f8b9e45c0ce131fe4a689d570c883a","modified":1484126821000},{"_id":"themes/landscape/layout/_widget/tagcloud.ejs","hash":"b4a2079101643f63993dcdb32925c9b071763b46","modified":1484126821000},{"_id":"themes/landscape/layout/_widget/tag.ejs","hash":"2de380865df9ab5f577f7d3bcadf44261eb5faae","modified":1484126821000},{"_id":"themes/landscape/source/css/.DS_Store","hash":"0c4a669591bf1723e84d44bb15e7b684e25ff531","modified":1484146229000},{"_id":"themes/landscape/source/css/_variables.styl","hash":"06e2f44b92c26c5d71abf01e7b43ee0dfd2010c7","modified":1484148828000},{"_id":"themes/landscape/source/css/style.styl","hash":"a70d9c44dac348d742702f6ba87e5bb3084d65db","modified":1484126821000},{"_id":"themes/landscape/source/css/_extend.styl","hash":"222fbe6d222531d61c1ef0f868c90f747b1c2ced","modified":1484126821000},{"_id":"themes/landscape/source/fancybox/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1484126821000},{"_id":"themes/landscape/source/fancybox/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1484126821000},{"_id":"themes/landscape/source/fancybox/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1484126821000},{"_id":"themes/landscape/source/fancybox/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1484126821000},{"_id":"themes/landscape/source/fancybox/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1484126821000},{"_id":"themes/landscape/source/fancybox/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1484126821000},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.css","hash":"aaa582fb9eb4b7092dc69fcb2d5b1c20cca58ab6","modified":1484126821000},{"_id":"themes/landscape/source/js/script.js","hash":"2876e0b19ce557fca38d7c6f49ca55922ab666a1","modified":1484126821000},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.js","hash":"d08b03a42d5c4ba456ef8ba33116fdbb7a9cabed","modified":1484126821000},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.pack.js","hash":"9e0d51ca1dbe66f6c0c7aefd552dc8122e694a6e","modified":1484126821000},{"_id":"themes/landscape/layout/_partial/post/gallery.ejs","hash":"3d9d81a3c693ff2378ef06ddb6810254e509de5b","modified":1484126821000},{"_id":"themes/landscape/layout/_partial/post/category.ejs","hash":"c6bcd0e04271ffca81da25bcff5adf3d46f02fc0","modified":1484213953000},{"_id":"themes/landscape/layout/_partial/post/nav.ejs","hash":"16a904de7bceccbb36b4267565f2215704db2880","modified":1484126821000},{"_id":"themes/landscape/layout/_partial/post/date.ejs","hash":"6197802873157656e3077c5099a7dda3d3b01c29","modified":1484126821000},{"_id":"themes/landscape/layout/_partial/post/tag.ejs","hash":"2fcb0bf9c8847a644167a27824c9bb19ac74dd14","modified":1484126821000},{"_id":"themes/landscape/layout/_partial/post/title.ejs","hash":"2f275739b6f1193c123646a5a31f37d48644c667","modified":1484126821000},{"_id":"themes/landscape/source/css/_partial/archive.styl","hash":"db15f5677dc68f1730e82190bab69c24611ca292","modified":1484126821000},{"_id":"themes/landscape/source/css/_partial/article.styl","hash":"10685f8787a79f79c9a26c2f943253450c498e3e","modified":1484126821000},{"_id":"themes/landscape/source/css/_partial/comment.styl","hash":"79d280d8d203abb3bd933ca9b8e38c78ec684987","modified":1484126821000},{"_id":"themes/landscape/source/css/_partial/footer.styl","hash":"e35a060b8512031048919709a8e7b1ec0e40bc1b","modified":1484126821000},{"_id":"themes/landscape/source/css/_partial/header.styl","hash":"9658cb416b434dc6c3a8c2c15511eb170f363a3d","modified":1484148524000},{"_id":"themes/landscape/source/css/_partial/highlight.styl","hash":"bf4e7be1968dad495b04e83c95eac14c4d0ad7c0","modified":1484126821000},{"_id":"themes/landscape/source/css/_partial/mobile.styl","hash":"a399cf9e1e1cec3e4269066e2948d7ae5854d745","modified":1484126821000},{"_id":"themes/landscape/source/css/_partial/sidebar-bottom.styl","hash":"8fd4f30d319542babfd31f087ddbac550f000a8a","modified":1484126821000},{"_id":"themes/landscape/source/css/_partial/sidebar.styl","hash":"404ec059dc674a48b9ab89cd83f258dec4dcb24d","modified":1484126821000},{"_id":"themes/landscape/source/css/_util/mixin.styl","hash":"44f32767d9fd3c1c08a60d91f181ee53c8f0dbb3","modified":1484126821000},{"_id":"themes/landscape/source/css/_partial/sidebar-aside.styl","hash":"890349df5145abf46ce7712010c89237900b3713","modified":1484126821000},{"_id":"themes/landscape/source/css/fonts/FontAwesome.otf","hash":"b5b4f9be85f91f10799e87a083da1d050f842734","modified":1484126821000},{"_id":"themes/landscape/source/css/_util/grid.styl","hash":"0bf55ee5d09f193e249083602ac5fcdb1e571aed","modified":1484126821000},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.eot","hash":"7619748fe34c64fb157a57f6d4ef3678f63a8f5e","modified":1484126821000},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.woff","hash":"04c3bf56d87a0828935bd6b4aee859995f321693","modified":1484126821000},{"_id":"themes/landscape/source/css/images/banner.jpg","hash":"e2c9ff91ca7c221c23e41dba0d4b8dfd90d28a6c","modified":1484146333000},{"_id":"themes/landscape/source/fancybox/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1484126821000},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1484126821000},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.js","hash":"dc3645529a4bf72983a39fa34c1eb9146e082019","modified":1484126821000},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.js","hash":"47da1ae5401c24b5c17cc18e2730780f5c1a7a0c","modified":1484126821000},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1484126821000},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-media.js","hash":"294420f9ff20f4e3584d212b0c262a00a96ecdb3","modified":1484126821000},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.ttf","hash":"7f09c97f333917034ad08fa7295e916c9f72fd3f","modified":1484126821000},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.svg","hash":"46fcc0194d75a0ddac0a038aee41b23456784814","modified":1484126821000},{"_id":"themes/landscape/source/css/images/banner-bak.jpg","hash":"f44aa591089fcb3ec79770a1e102fd3289a7c6a6","modified":1484126821000},{"_id":"public/about/index.html","hash":"e1826ebadb92d980c4fecc1a37edcc67194f2d94","modified":1492522120153},{"_id":"public/blockchain/blockchain-portal/index.html","hash":"c912309e81516b9a02cbf3a49ff84db09d319da4","modified":1492522120154},{"_id":"public/python/lang-python-django-install/index.html","hash":"0de566612e9072adf1af86c86640f039614a2f6a","modified":1492522120154},{"_id":"public/docker/docker-k8s-install/index.html","hash":"b758b9d8d7e7a114d81e940ce00c4c15eceba9c3","modified":1492522120154},{"_id":"public/paas/cloud-paas-cf-core/index.html","hash":"4134508200c6c589f76365c213682c424a5dac8b","modified":1492522120154},{"_id":"public/paas/cloud-paas-portal/index.html","hash":"8acf41251125bd654cc4d7e31a919fb3195ea355","modified":1492522120154},{"_id":"public/iaas/cloud-iaas-openstack-core/index.html","hash":"4f0dfbc4c37254629024a3d43cce0e63e345de61","modified":1492522120154},{"_id":"public/paas/cloud-paas-openshift-core/index.html","hash":"6cf57a15cd617b2a0b124f6d6ac4b1568efe3b4c","modified":1492522120154},{"_id":"public/saas/cloud-saas-portal/index.html","hash":"f249e65ef87391c7b85cb663cc10b305d35b95cd","modified":1492522120154},{"_id":"public/community/community/index.html","hash":"3632059416c9177523ac167446e53d500405629a","modified":1492522120154},{"_id":"public/python/bigdata--portal/index.html","hash":"5e59a767eb340d44944ce54153fb69b9f28c8ef9","modified":1492522120155},{"_id":"public/iaas/cloud-iaas-portal/index.html","hash":"e3f0bf2198c66892525525f7deeb27a6a7000b88","modified":1492522120155},{"_id":"public/java/db--portal/index.html","hash":"e9d370409c1d3bf96347709acdf4778285a551d3","modified":1492522120155},{"_id":"public/mysql/db-mysql-portal/index.html","hash":"9725fff0ae0a4646cb5fbf2a09fee9d987705696","modified":1492522120155},{"_id":"public/nosql/db-nosql-portal/index.html","hash":"b307fab32312ea2c959e298699fd053fa810b5ba","modified":1492522120155},{"_id":"public/docker/docker-compose-detail/index.html","hash":"b2c784375482cee549e1fa14ec6d709ce99fc1a9","modified":1492522120155},{"_id":"public/devops/devops--portal/index.html","hash":"e7064048b61e0f6702c4e49122a5fdbedd045c6a","modified":1492522120155},{"_id":"public/docker/docker-cloud-detail/index.html","hash":"014c3241afcb726427ba3e836a9ec2be419218cb","modified":1492522120155},{"_id":"public/docker/docker-compose-file-detail/index.html","hash":"1d13af9a57e84036b7ccd91ad4a0754e59d7a056","modified":1492522120155},{"_id":"public/docker/docker-dockerfile-detail/index.html","hash":"eef8c30fa4f26c0016ecd912f38c4bc2d8b0c9a8","modified":1492522120155},{"_id":"public/docker/docker-dockerhub-detail/index.html","hash":"e904135d62c1f561635c3c74803a63023a609d22","modified":1492522120155},{"_id":"public/docker/docker-engine-detail/index.html","hash":"28dd08c9d0a81f9f26ac6d0db81d4d733e8738ba","modified":1492522120155},{"_id":"public/docker/docker-cookbook/index.html","hash":"81373c9e4ff767e2e3006c5ff8fcadd18059ad85","modified":1492522120155},{"_id":"public/docker/docker-core/index.html","hash":"f820f349b83c8b9284d67eb2a832b975b13fc161","modified":1492522120155},{"_id":"public/docker/docker-filesystem-core/index.html","hash":"993bb89ca125e7d7b707fdc26b6cbcc503d3ed5e","modified":1492522120155},{"_id":"public/docker/docker-k8s-core/index.html","hash":"c4e6737636845031c82a8f0097edf589c7cf8294","modified":1492522120155},{"_id":"public/docker/docker-image-detail/index.html","hash":"81ee02cbcb71d2377e38005d9a77217b9e975d55","modified":1492522120155},{"_id":"public/bigdata/bigdata-ml-portal/index.html","hash":"833e71dafe4cb2ef59cf638fadf4c6b427b501c6","modified":1492522120156},{"_id":"public/docker/docker-machine-detail/index.html","hash":"0a46805060424b9fdfcfbedbf749178b37e4816e","modified":1492522120156},{"_id":"public/docker/docker-network-core/index.html","hash":"6c9b912fe3b45d010b6ec13371498e6b4bdcf7ea","modified":1492522120156},{"_id":"public/docker/docker-security-core/index.html","hash":"1bccde2237d15b3a5f338433bb033f0ab5f74a28","modified":1492522120156},{"_id":"public/docker/docker-store-detail/index.html","hash":"39d849731d253a4f0776641a43fac65df583fb63","modified":1492522120156},{"_id":"public/docker/docker-storage-core/index.html","hash":"57c3e7129d94d392953101d72a3551dab3cc2269","modified":1492522120156},{"_id":"public/docker/docker-swarm-detail/index.html","hash":"470dc328e50780fbae8c260c1778df6592fb90d9","modified":1492522120156},{"_id":"public/iaas/docker-swarmkit-detail/index.html","hash":"23ae866236dfd46168dc5a3ab12ef68220b00b25","modified":1492522120156},{"_id":"public/docker/docker-mesos-portal/index.html","hash":"8daeb2d1422b02bf5c3a15f949233c2674d50ef1","modified":1492522120156},{"_id":"public/docker/docker-yaml-detail/index.html","hash":"553e357ced96c23e1b806c61b718417e5877e4ee","modified":1492522120156},{"_id":"public/docker/docker-mesos-core/index.html","hash":"f22f271af617ba7a27f4743fcbc4ca01e3bceaea","modified":1492522120156},{"_id":"public/iot/iot-portal/index.html","hash":"b2198cc8963ed4afae28ce9d693f17e5639be30b","modified":1492522120156},{"_id":"public/programming-language/lang--portal/index.html","hash":"3af9bdf0ab88d5f19a19b5d6eeb2a036a82a52d2","modified":1492522120156},{"_id":"public/programming-language/lang-go-portal/index.html","hash":"a9b5d8d821332a88f9069d4b0f3fda5f9d2cdd4c","modified":1492522120156},{"_id":"public/programming-language/lang-java-portal/index.html","hash":"b9d227b4b4758e41b44bb4ae58fad85d3142d73d","modified":1492522120156},{"_id":"public/nodejs/lang-js-nodejs-core/index.html","hash":"72d1a068ca7ded152c0e207e13d228928a13ffdb","modified":1492522120156},{"_id":"public/programming-language/lang-js-portal/index.html","hash":"982edc1680a1d5a0cd81e81bbe338bd2657abe65","modified":1492522120156},{"_id":"public/programming-language/lang-cx-portal/index.html","hash":"7e558643cdfe08dd133aa268f3d2cc14800fc273","modified":1492522120156},{"_id":"public/php/lang-php-portal/index.html","hash":"2c06b942389849f3a3270a15aefe201c5162e7c6","modified":1492522120157},{"_id":"public/network/network-portal/index.html","hash":"fb0cfe95ab446c7fb5745ac29abf25c004443399","modified":1492522120157},{"_id":"public/docker/docker-swarm-portal/index.html","hash":"99c38aeaf6d054e3b62e1e770f637134384e9ae6","modified":1492522120157},{"_id":"public/python/lang-python-django-core/index.html","hash":"d6659cfbf347ad175e8bedbd80f5a56d5fd10eb9","modified":1492522120157},{"_id":"public/bigdata/bigdata-storm-portal/index.html","hash":"2208504385ab4f0920171f98a14907eb54449bc0","modified":1492522120157},{"_id":"public/python/lang-python-portal/index.html","hash":"8d45d0c27ed5faa7c0dbbaa75f9f91bb7caefd43","modified":1492522120157},{"_id":"public/ruby/lang-ruby-rails-core/index.html","hash":"01822eab91d862e4d0d439428b89f2c54bb328e3","modified":1492522120157},{"_id":"public/microservice/microservice-portal/index.html","hash":"c524e2f9fc7829cf92ac276ccdb973be196e509f","modified":1492522120157},{"_id":"public/network/network-http-core/index.html","hash":"fd554c4f5e3ef2b762ae712bb51021e41b4cfd43","modified":1492522120157},{"_id":"public/linux/system-linux-portal/index.html","hash":"88562982c1f333cc0a12b060df9faa45f94ca56e","modified":1492522120157},{"_id":"public/tmp/hello-world/index.html","hash":"c4812a7b3fc133ba1a67c575f8b5530fbf161189","modified":1492522120157},{"_id":"public/archives/index.html","hash":"21d76128ef26a546a414b3464d5cf784adebfad9","modified":1492522120157},{"_id":"public/archives/page/2/index.html","hash":"dd366996bb4d810fd4eaa02f6286b818a1b75e8b","modified":1492522120157},{"_id":"public/archives/page/3/index.html","hash":"6e4cf0e9dec3eb2f84319860090fd466e943375d","modified":1492522120157},{"_id":"public/archives/page/4/index.html","hash":"1802f23a5cc993c8d84efe1c18e868fde9ac3e32","modified":1492522120157},{"_id":"public/archives/page/5/index.html","hash":"95e07e24757c7d71ddf48c805212e86bcca9c46d","modified":1492522120157},{"_id":"public/archives/page/6/index.html","hash":"b12f5658354309e15fe4c1ddbd6ce5b4e2edcaaf","modified":1492522120157},{"_id":"public/archives/2017/index.html","hash":"85695653ddb81f05c7cdffc7000ec477982f2cf7","modified":1492522120157},{"_id":"public/archives/2017/page/2/index.html","hash":"fb78b57d767927d82aea81d8b9a89802ef2c51e0","modified":1492522120157},{"_id":"public/archives/2017/page/3/index.html","hash":"da61a4e3db72b45c826fbbf36ee711f2f00da2d3","modified":1492522120157},{"_id":"public/archives/2017/page/4/index.html","hash":"3528e085187ff8d40e4622f31660f8f81454af6d","modified":1492522120158},{"_id":"public/archives/2017/page/5/index.html","hash":"c0d6e33ddfb6016e41449a485fd6ec6c439de0b9","modified":1492522120158},{"_id":"public/archives/2017/page/6/index.html","hash":"80dbcc7c9716f0a19d05500406843a1594fc2355","modified":1492522120158},{"_id":"public/archives/2017/01/index.html","hash":"b21736009d80ca06f0f42eddae49c5a2c310daf7","modified":1492522120158},{"_id":"public/archives/2017/01/page/2/index.html","hash":"e3fe1fc3bd693ac925f222bc2b60be5d224a66c0","modified":1492522120158},{"_id":"public/archives/2017/01/page/3/index.html","hash":"73f5b9e9c7c49ec6a3d7282c4069435d79af4ad1","modified":1492522120158},{"_id":"public/archives/2017/01/page/4/index.html","hash":"5600efaa8f9d5cf3036d0abaed651eb2da956819","modified":1492522120158},{"_id":"public/archives/2017/01/page/5/index.html","hash":"d694dfff63440f3ec3fa0c59c8343f14abc2d5bb","modified":1492522120158},{"_id":"public/archives/2017/01/page/6/index.html","hash":"abc69458124633552514d90acb99e254b51c1af7","modified":1492522120158},{"_id":"public/archives/2017/04/index.html","hash":"347d34818a5727ddccdd9a22a84fcc079e4a1802","modified":1492522120158},{"_id":"public/categories/bigdata/index.html","hash":"73ae9b66e023d15869eaaebe913e1beccf9bfe77","modified":1492522120158},{"_id":"public/categories/cloud/index.html","hash":"fe11eb23a10da18ff5d85a101f78472d68df87f7","modified":1492522120158},{"_id":"public/categories/paas/index.html","hash":"7ed0c51af7b742d11adc1ae45455e608c7332096","modified":1492522120158},{"_id":"public/categories/blockchain/index.html","hash":"43304493ebe15274df08dac92ddd5803490890b0","modified":1492522120158},{"_id":"public/categories/iaas/index.html","hash":"5287303338ca9ecd6bb15135798e371badff7d07","modified":1492522120158},{"_id":"public/categories/saas/index.html","hash":"760558fa3727b630b9cf4d9dbd80c5493c5f28ee","modified":1492522120158},{"_id":"public/categories/community/index.html","hash":"fc4e71a0aaeb21c56e033ad1986e21611cd0c740","modified":1492522120158},{"_id":"public/categories/python/index.html","hash":"ca989984510ba5c8c25375ccdc2e321b6dc12e5e","modified":1492522120158},{"_id":"public/categories/java/index.html","hash":"7c45bfc7311601b6282ea07b3a4a2653c857406b","modified":1492522120158},{"_id":"public/categories/mysql/index.html","hash":"4a4d83e3bb883bc2a18accc1d60284f57b2600bc","modified":1492522120158},{"_id":"public/categories/nosql/index.html","hash":"a62089dbabe6d1c215fa45882712725192dbc274","modified":1492522120158},{"_id":"public/categories/devops/index.html","hash":"7a282b24815452d7a0e5481c0fbd57ac7c28d1ef","modified":1492522120159},{"_id":"public/categories/docker/index.html","hash":"3764bca6193af1f5b83f62bf0c632308fbd6cf66","modified":1492522120159},{"_id":"public/categories/docker/page/2/index.html","hash":"6ef587ae38c8ce64aad58dd8ce91293e8266ca05","modified":1492522120159},{"_id":"public/categories/docker/page/3/index.html","hash":"5ff6a3d81a7682ccc7e47923ab3a84491b2bc4a7","modified":1492522120159},{"_id":"public/categories/iot/index.html","hash":"142b1f28b165c74559372f1abe335eaa58565697","modified":1492522120159},{"_id":"public/categories/programming-language/index.html","hash":"5238824b1b49826aea0cdc9c663504bce763d8c3","modified":1492522120159},{"_id":"public/categories/nodejs/index.html","hash":"726ffeeb692d84e0ef463c3ae1c916f9c787f839","modified":1492522120159},{"_id":"public/categories/php/index.html","hash":"aeb313f0d747e384c02c88564740c95d8fe6e6f9","modified":1492522120159},{"_id":"public/categories/tmp/index.html","hash":"266e8df34e83826a837a2a75bd0b934b7a754c72","modified":1492522120159},{"_id":"public/categories/ruby/index.html","hash":"94c4c5f9a731c7c4d69025b24eae31f494d281a7","modified":1492522120159},{"_id":"public/categories/microservice/index.html","hash":"fdeb75e4644033e224c3ab06c511b591218c9405","modified":1492522120159},{"_id":"public/categories/network/index.html","hash":"0d85d98615981b3228e0af5d4295656776f75290","modified":1492522120159},{"_id":"public/categories/linux/index.html","hash":"0946aeadafc5b86d9967e262a2e80e2472120b76","modified":1492522120159},{"_id":"public/tags/portal/index.html","hash":"d5554021f72d5d68cb146c19bae6497a7d3c8b57","modified":1492522120159},{"_id":"public/tags/portal/page/2/index.html","hash":"90612ab2aa46e8b6e9187b223737463e38a1bfb3","modified":1492522120159},{"_id":"public/tags/portal/page/3/index.html","hash":"636c5e6c6420e6a575d1d280949d1de8c5531555","modified":1492522120159},{"_id":"public/tags/hadoop/index.html","hash":"56a92a3c8d1037ad190b5f7dc10b87862841536e","modified":1492522120159},{"_id":"public/tags/machine-learning/index.html","hash":"22e21b2b05cf26464b656443ebe87de4e707e5a5","modified":1492522120159},{"_id":"public/tags/storm/index.html","hash":"6df21d3d6372bf8a2c2ffdb316f06bf7ecaef63b","modified":1492522120159},{"_id":"public/tags/cloud/index.html","hash":"ae1e925eb49d5eb3ac7831aa1fc46935c2d155b4","modified":1492522120159},{"_id":"public/tags/core/index.html","hash":"1f4b4afaafaae86e85d32ef0bc35381d0275af21","modified":1492522120159},{"_id":"public/tags/core/page/2/index.html","hash":"c70da08d08e8d7072938bdb62d5929b5ade3de92","modified":1492522120160},{"_id":"public/tags/cloudfoundry/index.html","hash":"700258cad0a81bdbde0970c86e25c7f15ed7ad33","modified":1492522120160},{"_id":"public/tags/blockchain/index.html","hash":"22bf2168d8e4e973cb44de47d742b6244499f6ab","modified":1492522120160},{"_id":"public/tags/openstack/index.html","hash":"50d9e69111c4da52cdd8b3b215fe1cd0280062fa","modified":1492522120160},{"_id":"public/tags/openshift/index.html","hash":"c73d00c4fd8808eab0a62825452eed9be5dab846","modified":1492522120160},{"_id":"public/tags/opensource/index.html","hash":"0f6465e98f92fa206ff08d76c78d954d0ebeaf93","modified":1492522120160},{"_id":"public/tags/detail/index.html","hash":"dc288c0c42e03be07b2b6b1c9301db5278051c5e","modified":1492522120160},{"_id":"public/tags/detail/page/2/index.html","hash":"0b85f38b1952807b06fa09772cb5b053d793f64c","modified":1492522120160},{"_id":"public/tags/compose/index.html","hash":"5671884a2c833b8a3bbedc83af688bd1bbb5cca9","modified":1492522120160},{"_id":"public/tags/dockerfile/index.html","hash":"3ac1ed6a29529e296dc37785b2c2e13679631abb","modified":1492522120160},{"_id":"public/tags/cookbook/index.html","hash":"8bd1d8f42d60e2395abdf8af27722d1e969bd25b","modified":1492522120160},{"_id":"public/tags/filesystem/index.html","hash":"912d503c50463ffbc4bf874c59bc710d902fa701","modified":1492522120160},{"_id":"public/tags/k8s/index.html","hash":"10806229faba2df63d96299fad475a19a64d78f3","modified":1492522120160},{"_id":"public/tags/network/index.html","hash":"7f77769bdf687b1d4880990ca89cbb2ec0f2aa62","modified":1492522120161},{"_id":"public/tags/security/index.html","hash":"e4540222494d9d078b2185513ae585e93b0c18b6","modified":1492522120161},{"_id":"public/tags/storage/index.html","hash":"8fbf205fe981f4ccb2d06cc2d08e4e9ed8c3b75b","modified":1492522120161},{"_id":"public/tags/docker/index.html","hash":"4aea11f1b2e9bf4c92eb1b5bf946b4c108bfc839","modified":1492522120161},{"_id":"public/tags/swarmkit/index.html","hash":"42e142de0816a28c45e42a39ec8d5593e0327e67","modified":1492522120161},{"_id":"public/tags/mesos/index.html","hash":"730401c715871ad64654817ab724d9faaea8af5f","modified":1492522120161},{"_id":"public/tags/yaml/index.html","hash":"f9e8d765f89d60a2b48119ceae52b15e618e3c57","modified":1492522120161},{"_id":"public/tags/iot/index.html","hash":"4458b8129974a90a8c3142fd7aae8b8e1dca5881","modified":1492522120161},{"_id":"public/tags/programming-language/index.html","hash":"e7cd2f79f5545d42e76c4cbaff203a726c957b57","modified":1492522120161},{"_id":"public/tags/django/index.html","hash":"8a33c1aee69bec33e272aa876b5607898bb09bee","modified":1492522120161},{"_id":"public/tags/swarm/index.html","hash":"91b2cefd4cf8dd095d0615d2fedcee174d33dd2f","modified":1492522120161},{"_id":"public/tags/install/index.html","hash":"23f9160d91ecebe1a110573640b6214278c107b3","modified":1492522120161},{"_id":"public/tags/python/index.html","hash":"186d59d3f931a652618e1c1da30fbd8fa4fbd9cb","modified":1492522120161},{"_id":"public/tags/rails/index.html","hash":"c1b95da11597b13b13fa641d6b6515654b2a66e2","modified":1492522120163},{"_id":"public/tags/http/index.html","hash":"5d3184d9e8b1af38ad6d7f1b0bb6802bf00047dc","modified":1492522120163},{"_id":"public/cloud/cloud--portal/index.html","hash":"7641c53040d69d02ce52f1ea1048fa4a81ff71a3","modified":1492522120163},{"_id":"public/bigdata/bigdata-hadoop-portal/index.html","hash":"e097eb37755d017eb56ab21d0505db63893fe753","modified":1492522120163},{"_id":"public/docker/docker--portal/index.html","hash":"cd9a067369b8c8253b7a6a980401848738b7d36d","modified":1492522120163},{"_id":"public/docker/docker-swarmnext-detail/index.html","hash":"310f0f2e8b9f72e7bc3ad8f06dd7439df371fa83","modified":1492522120163},{"_id":"public/index.html","hash":"f93e5f0830b4fa9275b6f695365bd8f426e1d426","modified":1492522120163},{"_id":"public/page/2/index.html","hash":"d916f88364a672306d015a47934bcc002cd31f58","modified":1492522120163},{"_id":"public/page/3/index.html","hash":"5d93faf376e3bd4d57e4f5ae22f6a4ef61b54e24","modified":1492522120163},{"_id":"public/page/4/index.html","hash":"7c2b5086e0c1e19e0f561eaa82a6501a2773b489","modified":1492522120163},{"_id":"public/page/5/index.html","hash":"ee5c52793f2836b9d5fa839cf97101dc2485810d","modified":1492522120163},{"_id":"public/page/6/index.html","hash":"14c1339c5386f784dd1d20d83d42df7cdcbd9900","modified":1492522120164},{"_id":"public/fancybox/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1492522120182},{"_id":"public/fancybox/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1492522120182},{"_id":"public/fancybox/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1492522120182},{"_id":"public/fancybox/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1492522120182},{"_id":"public/fancybox/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1492522120182},{"_id":"public/fancybox/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1492522120183},{"_id":"public/css/fonts/fontawesome-webfont.eot","hash":"7619748fe34c64fb157a57f6d4ef3678f63a8f5e","modified":1492522120183},{"_id":"public/css/fonts/FontAwesome.otf","hash":"b5b4f9be85f91f10799e87a083da1d050f842734","modified":1492522120183},{"_id":"public/css/images/banner.jpg","hash":"e2c9ff91ca7c221c23e41dba0d4b8dfd90d28a6c","modified":1492522120183},{"_id":"public/css/fonts/fontawesome-webfont.woff","hash":"04c3bf56d87a0828935bd6b4aee859995f321693","modified":1492522120183},{"_id":"public/fancybox/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1492522120183},{"_id":"public/css/fonts/fontawesome-webfont.ttf","hash":"7f09c97f333917034ad08fa7295e916c9f72fd3f","modified":1492522122108},{"_id":"public/fancybox/jquery.fancybox.css","hash":"aaa582fb9eb4b7092dc69fcb2d5b1c20cca58ab6","modified":1492522122118},{"_id":"public/js/script.js","hash":"2876e0b19ce557fca38d7c6f49ca55922ab666a1","modified":1492522122118},{"_id":"public/fancybox/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1492522122118},{"_id":"public/fancybox/helpers/jquery.fancybox-thumbs.js","hash":"47da1ae5401c24b5c17cc18e2730780f5c1a7a0c","modified":1492522122118},{"_id":"public/fancybox/helpers/jquery.fancybox-buttons.js","hash":"dc3645529a4bf72983a39fa34c1eb9146e082019","modified":1492522122118},{"_id":"public/fancybox/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1492522122118},{"_id":"public/fancybox/helpers/jquery.fancybox-media.js","hash":"294420f9ff20f4e3584d212b0c262a00a96ecdb3","modified":1492522122118},{"_id":"public/css/style.css","hash":"f1bdee26c06f8dca10fa0df089e978e0469d755a","modified":1492522122118},{"_id":"public/fancybox/jquery.fancybox.js","hash":"d08b03a42d5c4ba456ef8ba33116fdbb7a9cabed","modified":1492522122118},{"_id":"public/fancybox/jquery.fancybox.pack.js","hash":"9e0d51ca1dbe66f6c0c7aefd552dc8122e694a6e","modified":1492522122119},{"_id":"public/css/fonts/fontawesome-webfont.svg","hash":"46fcc0194d75a0ddac0a038aee41b23456784814","modified":1492522122129},{"_id":"public/css/images/banner-bak.jpg","hash":"f44aa591089fcb3ec79770a1e102fd3289a7c6a6","modified":1492522122129}],"Category":[{"name":"bigdata","_id":"cj1nl4rak0002kxsvcygcd1t7"},{"name":"cloud","_id":"cj1nl4rbo000ikxsvjbv2z6d3"},{"name":"paas","_id":"cj1nl4rbv000pkxsvohgya92l"},{"name":"blockchain","_id":"cj1nl4rc2000wkxsvovoq1x80"},{"name":"iaas","_id":"cj1nl4rcd0014kxsv0yj8i0j5"},{"name":"saas","_id":"cj1nl4rd4001qkxsv82besi28"},{"name":"community","_id":"cj1nl4rdb001xkxsv795sdv5l"},{"name":"python","_id":"cj1nl4rdh0024kxsvs4zdognq"},{"name":"java","_id":"cj1nl4rdy002jkxsv2hvj87nq"},{"name":"mysql","_id":"cj1nl4ree002rkxsvcp05s859"},{"name":"nosql","_id":"cj1nl4rem002ykxsvh7q0sd8h"},{"name":"devops","_id":"cj1nl4rev0034kxsv9utwmdoq"},{"name":"docker","_id":"cj1nl4rf2003bkxsvey1g8zph"},{"name":"iot","_id":"cj1nl4rhf006gkxsvnopw7f8u"},{"name":"programming language","_id":"cj1nl4rhi006mkxsveyxcmf6u"},{"name":"nodejs","_id":"cj1nl4rhq0071kxsvpd8lvhb5"},{"name":"php","_id":"cj1nl4rhx007bkxsv69buwdje"},{"name":"tmp","_id":"cj1nl4rhz007kkxsvf43mqixo"},{"name":"ruby","_id":"cj1nl4riv007wkxsv41mm1gag"},{"name":"microservice","_id":"cj1nl4rj20083kxsvjulair8n"},{"name":"network","_id":"cj1nl4rj40088kxsv442ou335"},{"name":"linux","_id":"cj1nl4rj6008hkxsvoaxl1ke2"}],"Data":[],"Page":[{"title":"about","date":"2017-04-18T17:49:37.000Z","_content":"\n我是Allen Galler，阿里云系统工程师，关注云计算与DevOps运动。\nMail: allengaller 括号 at 括号 gmail.com\n","source":"about/index.md","raw":"---\ntitle: about\ndate: 2017-04-18 17:49:37\n---\n\n我是Allen Galler，阿里云系统工程师，关注云计算与DevOps运动。\nMail: allengaller 括号 at 括号 gmail.com\n","updated":"2017-04-18T13:28:04.000Z","path":"about/index.html","comments":1,"layout":"page","_id":"cj1nl4rih007pkxsvihd7mwt2","content":"<p>我是Allen Galler，阿里云系统工程师，关注云计算与DevOps运动。<br>Mail: allengaller 括号 at 括号 gmail.com</p>\n","excerpt":"","more":"<p>我是Allen Galler，阿里云系统工程师，关注云计算与DevOps运动。<br>Mail: allengaller 括号 at 括号 gmail.com</p>\n"}],"Post":[{"title":"hadoop portal","_content":"\n# about hadoop\n\n    head first:\n        离线计算框架;\n        开源分布式系统实现，完整的mapreduce计算框架实现，山寨Google计算系统中的强者;\n\n\tusage\n\t    搭建大型数据仓库，PB数据存储处理分析统计等\n\t    搜索引擎，商业智能，日志分析，数据挖掘\n\tadvantage\n\t   高扩展，低成本，成熟的生态圈\n       业界大数据平台的首选\n\t   人才缺口：开发与运维\n\tversion\n\t\t1.2 stable\n\t\t2.x\n\tfounder\n\t\tDoug_Cutting\n\t\t\thttps://en.wikipedia.org/wiki/Doug_Cutting\n\t\t\thttps://www.linkedin.com/in/cutting\n\n# resource\n\nresource\n\tlink\n\t\thttp://hadoop.apache.org\n\t\t小象学院\n\t\t\thttp://www.chinahadoop.cn/\n\t\thttps://hadoopecosystemtable.github.io/\n\tawesome\n\t\thttps://github.com/youngwookim/awesome-hadoop\n\tdoc\n\t\thttp://hadoop.apache.org/docs/current/\n\tread\n\t\tbook\n\t\t\tHadoop: The Definitive Guide\n\t\t\tHadoop Operations\n\t\t\tApache Hadoop Yarn\n\t\t\tHBase: The Definitive Guide\n\t\t\tProgramming Pig\n\t\t\tProgramming Hive\n\t\t\tHadoop in Practice, Second Edition\n\t\t\tHadoop in Action, Second Edition\n\t\tblog\n\t\t\thttp://zh.hortonworks.com/blog/hadoop-perect-app-for-openstack/\n\t\t\t\tSavanna/Sahara\n\t\t\thttp://hadoop123.com/\n\t\t\thttp://dongxicheng.org/\n\t\t\thttp://blog.fens.me/hadoop-family-roadmap/\n\t\tzhihu\n\t\t\thttps://www.zhihu.com/topic/19563390/hot\n\tproject\n\t\tmodules\n\t\t\tHadoop Common\n\t\t\t\tThe common utilities that support the other Hadoop modules.\n\t\t\tHDFS\n\t\t\t\tabout\n\t\t\t\t\tHadoop Distributed File System (HDFS™)\n\t\t\t\t\tA distributed file system that provides high-throughput access to application data.\n\t\t\t\t\t存储海量数据的分布式文件系统，支持MapReduce操作，不支持标准POSIX接口，不操作裸磁盘\n\n\t\t\t\t\t局限性\n\t\t\t\t\t\t不适合低延迟数据访问\n\t\t\t\t\t\t无法高效存储大量小文件\n\t\t\t\t\t\t不支持多用户写入和任意修改\n\t\t\t\tconcept\n\t\t\t\t\tblock\n\t\t\t\t\t\t块，默认大小 64M，是文件存储的基本逻辑单元\n\t\t\t\t\t\t\t2.x 默认128M\n\t\t\t\t\t\thdfs=site.xml\n\t\t\t\t\tname node\n\t\t\t\t\t\t管理节点，存放文件元数据：文件与数据块的映射表，数据块与数据节点的映射表\n\n\t\t\t\t\tdata node\n\t\t\t\t\t\t工作节点，存放数据块\n\n\t\t\t\t\t数据管理与容错\n\t\t\t\t\t\t每个数据块三份副本，分布在两个机架内的三个节点\n\t\t\t\t\t\tdata node定期向 name node 发送心跳消息\n\t\t\t\t\t\t二级name node 定期同步元数据映像文件和修改日志，name node故障时转正\n\t\t\t\t\t\t\tfsimage, editlog\n\t\t\t\t\t文件读取流程\n\n\n\t\t\t\t\t文件写入流程\n\n\n\t\t\t\t\tjob  and task\n\t\t\t\t\t\tjob\n\t\t\t\t\t\t\t1 job -> many task\n\t\t\t\t\t\ttask\n\t\t\t\t\t\t\tmap task\n\t\t\t\t\t\t\treduce task\n\t\t\t\t\tjob tracker\n\t\t\t\t\t\tmaster 管理节点\n\t\t\t\t\t\t\t监控TaskTracker状态\n\t\t\t\t\t\t\t分配任务，监控任务执行进度\n\t\t\t\t\t\t\t作业调度\n\n\t\t\t\t\ttask tracker\n\t\t\t\t\t\tdata node 在同一组节点\n\t\t\t\t\t\t执行任务\n\t\t\t\t\t\t汇报任务状态\n\t\t\t\t\t作业执行\n\n\t\t\t\t\t容错机制\n\t\t\t\t\t\t重复执行\n\t\t\t\t\t\t\t四次失败后放弃\n\t\t\t\t\t\t推测执行\n\t\t\t\t\t\t\t所有map端完成后reduce端才开始；如果个别特别慢，则新建一个task tracker同步执行，认最快算完的tracker\n\t\t\t\t\tsample\n\t\t\t\t\t\tlink\n\t\t\t\t\t\t\thttp://www.imooc.com/video/7642\n\t\t\t\t\t\tword count\n\n\n\t\t\t\t\t\tsort\n\n\t\t\t\tfeature\n\t\t\t\t\t数据冗余，硬件容错\n\t\t\t\t\t流式数据访问。一次写入，多次读取。一旦写入无法修改，只能追加写。\n\t\t\t\t\t适合存储大文件\n\t\t\t\t\t适合数据批量读写，吞吐高。不适合交互式应用，低延迟很难满足\n\t\t\t\t\t适合一次写入多次读取，顺序读写。不支持多用户并发读写相同文件\n\t\t\tHadoop YARN\n\t\t\t\t A framework for job scheduling and cluster resource management.\n\t\t\tHadoop MapReduce\n\t\t\t\tabout\n\t\t\t\t\tA YARN-based system for parallel processing of large data sets.\n\t\t\t\t\t分布式并行数据处理和计算模型和框架，实现任务分解和调度，可以分析海量数据\n\t\t\t\t\t用户只需要实现map() reduce() 两个函数即可\n\t\t\t\t\t\t形参是KV\n\t\t\t\t\t高容错，高扩展，编程简单，适合大数据离线批量处理PB+\n\t\t\t\t\t\t实时需求：storm，hbase\n\t\t\t\tsolution\n\t\t\t\t\tmapr\n\t\t\t\t\t\thttps://www.mapr.com\n\t\t\t\t\t\tThe MapR Converged Data Platform integrates the power of Hadoop and Spark with global event streaming, real-time database capabilities, and enterprise storage for developing and running innovative data applications.\nThe MapR Platform is powered by the industry’s fastest, most reliable, secure, and open data infrastructure that dramatically lowers TCO and enables global real-time data applications.\n\t\t\t\tcore\n\t\t\t\t\t分而治之思想\n\t\t\t\t\t\t一个大任务分成多个子任务（map），并行执行后，合并结果（reduce）\n\n\t\t\t\t\t\tsample: 100G访问日志，找出访问次数最多的IP地址\n\n\t\t\t\tmap任务处理\n\n\t\t\t\treduce任务处理\n\n\t\t\t\tmap,reduce KV对格式\n\n\t\t\t\tMR流程\n\n\t\t\t\tMR角色\n\t\t\t\t\tjob client\n\t\t\t\t\t\t提交作业\n\t\t\t\t\tjob tracker\n\t\t\t\t\t\t初始化作业，分配作业，task tracker与其通讯，协调监控整个作业\n\t\t\t\t\ttask tracker\n\t\t\t\t\t\t定期与job tracker通讯，执行MR任务\n\t\t\t\t\thdfs\n\t\t\t\t\t\t读取和保存作业数据，配置，jar包，结果\n\t\t\t\t作业提交流程\n\t\t\t\t\t准备\n\t\t\t\t\t\t编写mr程序\n\t\t\t\t\t\t配置作业，输入输出路径等\n\t\t\t\t\t提交作业\n\t\t\t\t\t\t通过job client提交\n\n\t\t\t\t\t\t可以提交至YARN，也可以本地运行\n\t\t\t\t\t作业初始化\n\n\t\t\t\t\t任务分配\n\n\t\t\t\t\t错误处理\n\t\t\t\t\t\tjob tracker失败\n\t\t\t\t\t\t\t存在单点故障，hadoop2.0解决\n\t\t\t\t\t\ttask tracker失败\n\n\t\t\t\t\t\ttask 失败\n\t\t\t\t\t\t\t任务失败，会想task tracker抛出异常，最后任务挂起\n\t\t\t\t\tmap 输入\n\n\n\t\t\t\t\treduce输入\n\t\t\t\t\t\t框架自动完成\n\n\t\tHadoop-related projects at Apache\n\t\t\tAmbari\n\t\t\t\tA web-based tool for provisioning, managing, and monitoring Apache Hadoop clusters which includes support for Hadoop HDFS,\nHadoop MapReduce, Hive, HCatalog, HBase, ZooKeeper, Oozie, Pig and Sqoop. Ambari also provides a dashboard for viewing cluster health\nsuch as heatmaps and ability to view MapReduce, Pig and Hive applications visually alongwith features to diagnose their performance characteristics in a user-friendly manner.\n\t\t\tAvro\n\t\t\t\t A data serialization system.\n\t\t\t\tRPC\n\t\t\tCassandra\n\t\t\t\tA scalable multi-master database with no single points of failure.\n\t\t\tChukwa\n\t\t\t\tA data collection system for managing large distributed systems.\n\t\t\tHBase\n\t\t\t\tabout\n\t\t\t\t\t实时KV存储,放弃事务特性，最求更高的扩展，提供数据随机读写和实时访问，实现对表数据的读写功能\n\t\t\t\t\tA scalable, distributed database that supports structured data storage for large tables.\n\t\t\t\tlink\n\t\t\t\t\thttp://hbase.apache.org/\n\t\t\tHive\n\t\t\t\tabout\n\t\t\t\t\tSQL on Hadoop\n\t\t\t\t\t A data warehouse infrastructure that provides data summarization and ad hoc querying.\n\t\t\t\t\tSQL语句->Hadoop任务\n\t\t\t\tlink\n\t\t\t\t\thttp://hive.apache.org/\n\t\t\tMahout\n\t\t\t\tA Scalable machine learning and data mining library.\n\t\t\tPig\n\t\t\t\tA high-level data-flow language and execution framework for parallel computation.\n\t\t\tSpark\n\t\t\t\tA fast and general compute engine for Hadoop data. Spark provides a simple and expressive programming model that supports a wide range of applications, including ETL, machine learning, stream processing, and graph computation.\n\t\t\tTez\n\t\t\t\tA generalized data-flow programming framework, built on Hadoop YARN,\nwhich provides a powerful and flexible engine to execute an arbitrary DAG of tasks to process data for both batch and interactive use-cases.\nTez is being adopted by Hive™, Pig™ and other frameworks in the Hadoop ecosystem, and also by other commercial software (e.g. ETL tools), to replace Hadoop™ MapReduce as the underlying execution engine.\n\t\t\tZooKeeper\n\t\t\t\tabout\n\t\t\t\t\t锁服务，管理集群配置，监控每个节点状态，数据一致性\n\t\t\t\t\tA high-performance coordination service for distributed applications.\n\t\t\t\tlink\n\t\tawesome\n\t\t\thadoop\n\t\t\t\tApache Hadoop - Apache Hadoop\n\t\t\t\tApache Tez - A Framework for YARN-based, Data Processing Applications In Hadoop\n\t\t\t\tSpatialHadoop - SpatialHadoop is a MapReduce extension to Apache Hadoop designed specially to work with spatial data.\n\t\t\t\tGIS Tools for Hadoop - Big Data Spatial Analytics for the Hadoop Framework\n\t\t\t\tElasticsearch Hadoop - Elasticsearch real-time search and analytics natively integrated with Hadoop. Supports Map/Reduce, Cascading, Apache Hive and Apache Pig.\n\t\t\t\tdumbo - Python module that allows you to easily write and run Hadoop programs.\n\t\t\t\thadoopy - Python MapReduce library written in Cython.\n\t\t\t\tmrjob - mrjob is a Python 2.5+ package that helps you write and run Hadoop Streaming jobs.\n\t\t\t\tpydoop - Pydoop is a package that provides a Python API for Hadoop.\n\t\t\t\thdfs-du - HDFS-DU is an interactive visualization of the Hadoop distributed file system.\n\t\t\t\tWhite Elephant - Hadoop log aggregator and dashboard\n\t\t\t\tKiji Project\n\t\t\t\tGenie - Genie provides REST-ful APIs to run Hadoop, Hive and Pig jobs, and to manage multiple Hadoop resources and perform job submissions across them.\n\t\t\t\tApache Kylin - Apache Kylin is an open source Distributed Analytics Engine from eBay Inc. that provides SQL interface and multi-dimensional analysis (OLAP) on Hadoop supporting extremely large datasets\n\t\t\t\tCrunch - Go-based toolkit for ETL and feature extraction on Hadoop\n\t\t\t\tApache Ignite - Distributed in-memory platform\n\t\t\tYARN\n\t\t\t\tApache Slider - Apache Slider is a project in incubation at the Apache Software Foundation with the goal of making it possible and easy to deploy existing applications onto a YARN cluster.\n\t\t\t\tApache Twill - Apache Twill is an abstraction over Apache Hadoop® YARN that reduces the complexity of developing distributed applications, allowing developers to focus more on their application logic.\n\t\t\t\tmpich2-yarn - Running MPICH2 on Yarn\n\t\t\tnosql\n\t\t\t\tApache HBase - Apache HBase\n\t\t\t\tApache Phoenix - A SQL skin over HBase supporting secondary indices\n\t\t\t\thappybase - A developer-friendly Python library to interact with Apache HBase.\n\t\t\t\tHannibal - Hannibal is tool to help monitor and maintain HBase-Clusters that are configured for manual splitting.\n\t\t\t\tHaeinsa - Haeinsa is linearly scalable multi-row, multi-table transaction library for HBase\n\t\t\t\thindex - Secondary Index for HBase\n\t\t\t\tApache Accumulo - The Apache Accumulo™ sorted, distributed key/value store is a robust, scalable, high performance data storage and retrieval system.\n\t\t\t\tOpenTSDB - The Scalable Time Series Database\n\t\t\t\tApache Cassandra\n\t\t\tdata management\n\t\t\t\tApache Calcite - A Dynamic Data Management Framework\n\t\t\t\tApache Atlas - Metadata tagging & lineage capture suppoting complex business data taxonomies\n\t\t\tSQL on Hadoop\n\t\t\t\tApache Hive\n\t\t\t\tApache Phoenix A SQL skin over HBase supporting secondary indices\n\t\t\t\tPivotal HAWQ - Parallel Postgres on Hadoop\n\t\t\t\tLingual - SQL interface for Cascading (MR/Tez job generator)\n\t\t\t\tCloudera Impala\n\t\t\t\tPresto - Distributed SQL Query Engine for Big Data. Open sourced by Facebook.\n\t\t\t\tApache Tajo - Data warehouse system for Apache Hadoop\n\t\t\t\tApache Drill\n\t\t\tWorkflow, Lifecycle and Governance\n\t\t\t\tApache Oozie - Apache Oozie\n\t\t\t\tAzkaban\n\t\t\t\tApache Falcon - Data management and processing platform\n\t\t\t\tApache NiFi - A dataflow system\n\t\t\t\tAirFlow - AirFlow is a platform to programmaticaly author, schedule and monitor data pipelines\n\t\t\t\tLuigi - Python package that helps you build complex pipelines of batch jobs\n\t\t\tData Ingestion and Integration\n\t\t\t\tApache Flume - Apache Flume\n\t\t\t\tSuro - Netflix's distributed Data Pipeline\n\t\t\t\tApache Sqoop - Apache Sqoop\n\t\t\t\tApache Kafka - Apache Kafka\n\t\t\t\tGobblin from LinkedIn - Universal data ingestion framework for Hadoop\n\t\t\tDSL\n\t\t\t\tApache Pig - Apache Pig\n\t\t\t\tApache DataFu - A collection of libraries for working with large-scale data in Hadoop\n\t\t\t\tvahara - Machine learning and natural language processing with Apache Pig\n\t\t\t\tpacketpig - Open Source Big Data Security Analytics\n\t\t\t\takela - Mozilla's utility library for Hadoop, HBase, Pig, etc.\n\t\t\t\tseqpig - Simple and scalable scripting for large sequencing data set(ex: bioinfomation) in Hadoop\n\t\t\t\tLipstick - Pig workflow visualization tool. Introducing Lipstick on A(pache) Pig\n\t\t\t\tPigPen - PigPen is map-reduce for Clojure, or distributed Clojure. It compiles to Apache Pig, but you don't need to\n\t\t\tLibraries and Tools\n\t\t\t\tKite Software Development Kit - A set of libraries, tools, examples, and documentation\n\t\t\t\tgohadoop - Native go clients for Apache Hadoop YARN.\n\t\t\t\tHue - A Web interface for analyzing data with Apache Hadoop.\n\t\t\t\tApache Zeppelin - A web-based notebook that enables interactive data analytics\n\t\t\t\tJumbune - Jumbune is an open-source product built for analyzing Hadoop cluster and MapReduce jobs.\n\t\t\t\tApache Thrift\n\t\t\t\tApache Avro - Apache Avro is a data serialization system.\n\t\t\t\tElephant Bird - Twitter's collection of LZO and Protocol Buffer-related Hadoop, Pig, Hive, and HBase code.\n\t\t\t\tSpring for Apache Hadoop\n\t\t\t\thdfs - A native go client for HDFS\n\t\t\t\tOozie Eclipse Plugin - A graphical editor for editing Apache Oozie workflows inside Eclipse.\n\t\t\tRealtime Data Processing\n\t\t\t\tApache Storm\n\t\t\t\tApache Samza\n\t\t\t\tApache Spark\n\t\t\t\tApache Flink - Apache Flink is a platform for efficient, distributed, general-purpose data processing. It supports exactly once stream processing.\n\t\t\tDistributed Computing and Programming\n\t\t\t\tApache Spark\n\t\t\t\t\tSpark Packages - A community index of packages for Apache Spark\n\t\t\t\t\tSparkHub - A community site for Apache Spark\n\t\t\t\tApache Crunch\n\t\t\t\tCascading - Cascading is the proven application development platform for building data applications on Hadoop.\n\t\t\t\tApache Flink - Apache Flink is a platform for efficient, distributed, general-purpose data processing.\n\t\t\t\tApache Apex (incubating) - Enterprise-grade unified stream and batch processing engine.\n\t\t\tPackaging, Provisioning and Monitoring\n\t\t\t\tApache Bigtop - Apache Bigtop: Packaging and tests of the Apache Hadoop ecosystem\n\t\t\t\tApache Ambari - Apache Ambari\n\t\t\t\tGanglia Monitoring System\n\t\t\t\tankush - A big data cluster management tool that creates and manages clusters of different technologies.\n\t\t\t\tApache Zookeeper - Apache Zookeeper\n\t\t\t\tApache Curator - ZooKeeper client wrapper and rich ZooKeeper framework\n\t\t\t\tBuildoop - Hadoop Ecosystem Builder\n\t\t\t\tDeploop - The Hadoop Deploy System\n\t\t\t\tJumbune - An open source MapReduce profiling, MapReduce flow debugging, HDFS data quality validation and Hadoop cluster monitoring tool.\n\t\t\t\tinviso - Inviso is a lightweight tool that provides the ability to search for Hadoop jobs, visualize the performance, and view cluster utilization.\n\t\t\tSearch\n\t\t\t\tElasticSearch\n\t\t\t\tApache Solr\n\t\t\t\tSenseiDB - Open-source, distributed, realtime, semi-structured database\n\t\t\t\tBanana - Kibana port for Apache Solr\n\t\t\tSearch Engine Framework\n\t\t\t\tApache Nutch - Apache Nutch is a highly extensible and scalable open source web crawler software project.\n\t\t\tSecurity\n\t\t\t\tApache Ranger - Ranger is a framework to enable, monitor and manage comprehensive data security across the Hadoop platform.\n\t\t\t\tApache Sentry - An authorization module for Hadoop\n\t\t\t\tApache Knox Gateway - A REST API Gateway for interacting with Hadoop clusters.\n\t\t\tBenchmark\n\t\t\t\tBig Data Benchmark\n\t\t\t\tHiBench\n\t\t\t\tBig-Bench\n\t\t\t\thive-benchmarks\n\t\t\t\thive-testbench - Testbench for experimenting with Apache Hive at any data scale.\n\t\t\t\tYCSB - The Yahoo! Cloud Serving Benchmark (YCSB) is an open-source specification and program suite for evaluating retrieval and maintenance capabilities of computer programs. It is often used to compare relative performance of NoSQL database management systems.\n\t\t\tMachine learning and Big Data analytics\n\t\t\t\tApache Mahout\n\t\t\t\tOryx 2 - Lambda architecture on Spark, Kafka for real-time large scale machine learning\n\t\t\t\tMLlib - MLlib is Apache Spark's scalable machine learning library.\n\t\t\t\tR - R is a free software environment for statistical computing and graphics.\n\t\t\t\tRHadoop including RHDFS, RHBase, RMR2, plyrmr\n\t\t\t\tRHive RHive, for launching Hive queries from R\n\t\t\t\tApache Lens\n\t\tEasyHadoop\n\t\t\t快速部署工具\n\t\t\thttps://github.com/xianglei/easyhadoop\n\tsolution\n\t\thorton\n\t\t\thttp://hortonworks.com/\n\t\t星环\n\t\t\thttp://www.transwarp.cn/\n\tconf\n\t\tApacheCon\n\t\tStrata + Hadoop World\n\t\tHadoop Summit\n\t\t\thttp://hadoopsummit.org/\n\tcourse\n\t\thttp://www.infoq.com/cn/presentations/baidu-open-cloud-big-data-technology-evolution\n","source":"_posts/bigdata-hadoop-portal.md","raw":"---\ntitle: hadoop portal\ncategories:\n- bigdata\ntags:\n- portal\n- hadoop\n---\n\n# about hadoop\n\n    head first:\n        离线计算框架;\n        开源分布式系统实现，完整的mapreduce计算框架实现，山寨Google计算系统中的强者;\n\n\tusage\n\t    搭建大型数据仓库，PB数据存储处理分析统计等\n\t    搜索引擎，商业智能，日志分析，数据挖掘\n\tadvantage\n\t   高扩展，低成本，成熟的生态圈\n       业界大数据平台的首选\n\t   人才缺口：开发与运维\n\tversion\n\t\t1.2 stable\n\t\t2.x\n\tfounder\n\t\tDoug_Cutting\n\t\t\thttps://en.wikipedia.org/wiki/Doug_Cutting\n\t\t\thttps://www.linkedin.com/in/cutting\n\n# resource\n\nresource\n\tlink\n\t\thttp://hadoop.apache.org\n\t\t小象学院\n\t\t\thttp://www.chinahadoop.cn/\n\t\thttps://hadoopecosystemtable.github.io/\n\tawesome\n\t\thttps://github.com/youngwookim/awesome-hadoop\n\tdoc\n\t\thttp://hadoop.apache.org/docs/current/\n\tread\n\t\tbook\n\t\t\tHadoop: The Definitive Guide\n\t\t\tHadoop Operations\n\t\t\tApache Hadoop Yarn\n\t\t\tHBase: The Definitive Guide\n\t\t\tProgramming Pig\n\t\t\tProgramming Hive\n\t\t\tHadoop in Practice, Second Edition\n\t\t\tHadoop in Action, Second Edition\n\t\tblog\n\t\t\thttp://zh.hortonworks.com/blog/hadoop-perect-app-for-openstack/\n\t\t\t\tSavanna/Sahara\n\t\t\thttp://hadoop123.com/\n\t\t\thttp://dongxicheng.org/\n\t\t\thttp://blog.fens.me/hadoop-family-roadmap/\n\t\tzhihu\n\t\t\thttps://www.zhihu.com/topic/19563390/hot\n\tproject\n\t\tmodules\n\t\t\tHadoop Common\n\t\t\t\tThe common utilities that support the other Hadoop modules.\n\t\t\tHDFS\n\t\t\t\tabout\n\t\t\t\t\tHadoop Distributed File System (HDFS™)\n\t\t\t\t\tA distributed file system that provides high-throughput access to application data.\n\t\t\t\t\t存储海量数据的分布式文件系统，支持MapReduce操作，不支持标准POSIX接口，不操作裸磁盘\n\n\t\t\t\t\t局限性\n\t\t\t\t\t\t不适合低延迟数据访问\n\t\t\t\t\t\t无法高效存储大量小文件\n\t\t\t\t\t\t不支持多用户写入和任意修改\n\t\t\t\tconcept\n\t\t\t\t\tblock\n\t\t\t\t\t\t块，默认大小 64M，是文件存储的基本逻辑单元\n\t\t\t\t\t\t\t2.x 默认128M\n\t\t\t\t\t\thdfs=site.xml\n\t\t\t\t\tname node\n\t\t\t\t\t\t管理节点，存放文件元数据：文件与数据块的映射表，数据块与数据节点的映射表\n\n\t\t\t\t\tdata node\n\t\t\t\t\t\t工作节点，存放数据块\n\n\t\t\t\t\t数据管理与容错\n\t\t\t\t\t\t每个数据块三份副本，分布在两个机架内的三个节点\n\t\t\t\t\t\tdata node定期向 name node 发送心跳消息\n\t\t\t\t\t\t二级name node 定期同步元数据映像文件和修改日志，name node故障时转正\n\t\t\t\t\t\t\tfsimage, editlog\n\t\t\t\t\t文件读取流程\n\n\n\t\t\t\t\t文件写入流程\n\n\n\t\t\t\t\tjob  and task\n\t\t\t\t\t\tjob\n\t\t\t\t\t\t\t1 job -> many task\n\t\t\t\t\t\ttask\n\t\t\t\t\t\t\tmap task\n\t\t\t\t\t\t\treduce task\n\t\t\t\t\tjob tracker\n\t\t\t\t\t\tmaster 管理节点\n\t\t\t\t\t\t\t监控TaskTracker状态\n\t\t\t\t\t\t\t分配任务，监控任务执行进度\n\t\t\t\t\t\t\t作业调度\n\n\t\t\t\t\ttask tracker\n\t\t\t\t\t\tdata node 在同一组节点\n\t\t\t\t\t\t执行任务\n\t\t\t\t\t\t汇报任务状态\n\t\t\t\t\t作业执行\n\n\t\t\t\t\t容错机制\n\t\t\t\t\t\t重复执行\n\t\t\t\t\t\t\t四次失败后放弃\n\t\t\t\t\t\t推测执行\n\t\t\t\t\t\t\t所有map端完成后reduce端才开始；如果个别特别慢，则新建一个task tracker同步执行，认最快算完的tracker\n\t\t\t\t\tsample\n\t\t\t\t\t\tlink\n\t\t\t\t\t\t\thttp://www.imooc.com/video/7642\n\t\t\t\t\t\tword count\n\n\n\t\t\t\t\t\tsort\n\n\t\t\t\tfeature\n\t\t\t\t\t数据冗余，硬件容错\n\t\t\t\t\t流式数据访问。一次写入，多次读取。一旦写入无法修改，只能追加写。\n\t\t\t\t\t适合存储大文件\n\t\t\t\t\t适合数据批量读写，吞吐高。不适合交互式应用，低延迟很难满足\n\t\t\t\t\t适合一次写入多次读取，顺序读写。不支持多用户并发读写相同文件\n\t\t\tHadoop YARN\n\t\t\t\t A framework for job scheduling and cluster resource management.\n\t\t\tHadoop MapReduce\n\t\t\t\tabout\n\t\t\t\t\tA YARN-based system for parallel processing of large data sets.\n\t\t\t\t\t分布式并行数据处理和计算模型和框架，实现任务分解和调度，可以分析海量数据\n\t\t\t\t\t用户只需要实现map() reduce() 两个函数即可\n\t\t\t\t\t\t形参是KV\n\t\t\t\t\t高容错，高扩展，编程简单，适合大数据离线批量处理PB+\n\t\t\t\t\t\t实时需求：storm，hbase\n\t\t\t\tsolution\n\t\t\t\t\tmapr\n\t\t\t\t\t\thttps://www.mapr.com\n\t\t\t\t\t\tThe MapR Converged Data Platform integrates the power of Hadoop and Spark with global event streaming, real-time database capabilities, and enterprise storage for developing and running innovative data applications.\nThe MapR Platform is powered by the industry’s fastest, most reliable, secure, and open data infrastructure that dramatically lowers TCO and enables global real-time data applications.\n\t\t\t\tcore\n\t\t\t\t\t分而治之思想\n\t\t\t\t\t\t一个大任务分成多个子任务（map），并行执行后，合并结果（reduce）\n\n\t\t\t\t\t\tsample: 100G访问日志，找出访问次数最多的IP地址\n\n\t\t\t\tmap任务处理\n\n\t\t\t\treduce任务处理\n\n\t\t\t\tmap,reduce KV对格式\n\n\t\t\t\tMR流程\n\n\t\t\t\tMR角色\n\t\t\t\t\tjob client\n\t\t\t\t\t\t提交作业\n\t\t\t\t\tjob tracker\n\t\t\t\t\t\t初始化作业，分配作业，task tracker与其通讯，协调监控整个作业\n\t\t\t\t\ttask tracker\n\t\t\t\t\t\t定期与job tracker通讯，执行MR任务\n\t\t\t\t\thdfs\n\t\t\t\t\t\t读取和保存作业数据，配置，jar包，结果\n\t\t\t\t作业提交流程\n\t\t\t\t\t准备\n\t\t\t\t\t\t编写mr程序\n\t\t\t\t\t\t配置作业，输入输出路径等\n\t\t\t\t\t提交作业\n\t\t\t\t\t\t通过job client提交\n\n\t\t\t\t\t\t可以提交至YARN，也可以本地运行\n\t\t\t\t\t作业初始化\n\n\t\t\t\t\t任务分配\n\n\t\t\t\t\t错误处理\n\t\t\t\t\t\tjob tracker失败\n\t\t\t\t\t\t\t存在单点故障，hadoop2.0解决\n\t\t\t\t\t\ttask tracker失败\n\n\t\t\t\t\t\ttask 失败\n\t\t\t\t\t\t\t任务失败，会想task tracker抛出异常，最后任务挂起\n\t\t\t\t\tmap 输入\n\n\n\t\t\t\t\treduce输入\n\t\t\t\t\t\t框架自动完成\n\n\t\tHadoop-related projects at Apache\n\t\t\tAmbari\n\t\t\t\tA web-based tool for provisioning, managing, and monitoring Apache Hadoop clusters which includes support for Hadoop HDFS,\nHadoop MapReduce, Hive, HCatalog, HBase, ZooKeeper, Oozie, Pig and Sqoop. Ambari also provides a dashboard for viewing cluster health\nsuch as heatmaps and ability to view MapReduce, Pig and Hive applications visually alongwith features to diagnose their performance characteristics in a user-friendly manner.\n\t\t\tAvro\n\t\t\t\t A data serialization system.\n\t\t\t\tRPC\n\t\t\tCassandra\n\t\t\t\tA scalable multi-master database with no single points of failure.\n\t\t\tChukwa\n\t\t\t\tA data collection system for managing large distributed systems.\n\t\t\tHBase\n\t\t\t\tabout\n\t\t\t\t\t实时KV存储,放弃事务特性，最求更高的扩展，提供数据随机读写和实时访问，实现对表数据的读写功能\n\t\t\t\t\tA scalable, distributed database that supports structured data storage for large tables.\n\t\t\t\tlink\n\t\t\t\t\thttp://hbase.apache.org/\n\t\t\tHive\n\t\t\t\tabout\n\t\t\t\t\tSQL on Hadoop\n\t\t\t\t\t A data warehouse infrastructure that provides data summarization and ad hoc querying.\n\t\t\t\t\tSQL语句->Hadoop任务\n\t\t\t\tlink\n\t\t\t\t\thttp://hive.apache.org/\n\t\t\tMahout\n\t\t\t\tA Scalable machine learning and data mining library.\n\t\t\tPig\n\t\t\t\tA high-level data-flow language and execution framework for parallel computation.\n\t\t\tSpark\n\t\t\t\tA fast and general compute engine for Hadoop data. Spark provides a simple and expressive programming model that supports a wide range of applications, including ETL, machine learning, stream processing, and graph computation.\n\t\t\tTez\n\t\t\t\tA generalized data-flow programming framework, built on Hadoop YARN,\nwhich provides a powerful and flexible engine to execute an arbitrary DAG of tasks to process data for both batch and interactive use-cases.\nTez is being adopted by Hive™, Pig™ and other frameworks in the Hadoop ecosystem, and also by other commercial software (e.g. ETL tools), to replace Hadoop™ MapReduce as the underlying execution engine.\n\t\t\tZooKeeper\n\t\t\t\tabout\n\t\t\t\t\t锁服务，管理集群配置，监控每个节点状态，数据一致性\n\t\t\t\t\tA high-performance coordination service for distributed applications.\n\t\t\t\tlink\n\t\tawesome\n\t\t\thadoop\n\t\t\t\tApache Hadoop - Apache Hadoop\n\t\t\t\tApache Tez - A Framework for YARN-based, Data Processing Applications In Hadoop\n\t\t\t\tSpatialHadoop - SpatialHadoop is a MapReduce extension to Apache Hadoop designed specially to work with spatial data.\n\t\t\t\tGIS Tools for Hadoop - Big Data Spatial Analytics for the Hadoop Framework\n\t\t\t\tElasticsearch Hadoop - Elasticsearch real-time search and analytics natively integrated with Hadoop. Supports Map/Reduce, Cascading, Apache Hive and Apache Pig.\n\t\t\t\tdumbo - Python module that allows you to easily write and run Hadoop programs.\n\t\t\t\thadoopy - Python MapReduce library written in Cython.\n\t\t\t\tmrjob - mrjob is a Python 2.5+ package that helps you write and run Hadoop Streaming jobs.\n\t\t\t\tpydoop - Pydoop is a package that provides a Python API for Hadoop.\n\t\t\t\thdfs-du - HDFS-DU is an interactive visualization of the Hadoop distributed file system.\n\t\t\t\tWhite Elephant - Hadoop log aggregator and dashboard\n\t\t\t\tKiji Project\n\t\t\t\tGenie - Genie provides REST-ful APIs to run Hadoop, Hive and Pig jobs, and to manage multiple Hadoop resources and perform job submissions across them.\n\t\t\t\tApache Kylin - Apache Kylin is an open source Distributed Analytics Engine from eBay Inc. that provides SQL interface and multi-dimensional analysis (OLAP) on Hadoop supporting extremely large datasets\n\t\t\t\tCrunch - Go-based toolkit for ETL and feature extraction on Hadoop\n\t\t\t\tApache Ignite - Distributed in-memory platform\n\t\t\tYARN\n\t\t\t\tApache Slider - Apache Slider is a project in incubation at the Apache Software Foundation with the goal of making it possible and easy to deploy existing applications onto a YARN cluster.\n\t\t\t\tApache Twill - Apache Twill is an abstraction over Apache Hadoop® YARN that reduces the complexity of developing distributed applications, allowing developers to focus more on their application logic.\n\t\t\t\tmpich2-yarn - Running MPICH2 on Yarn\n\t\t\tnosql\n\t\t\t\tApache HBase - Apache HBase\n\t\t\t\tApache Phoenix - A SQL skin over HBase supporting secondary indices\n\t\t\t\thappybase - A developer-friendly Python library to interact with Apache HBase.\n\t\t\t\tHannibal - Hannibal is tool to help monitor and maintain HBase-Clusters that are configured for manual splitting.\n\t\t\t\tHaeinsa - Haeinsa is linearly scalable multi-row, multi-table transaction library for HBase\n\t\t\t\thindex - Secondary Index for HBase\n\t\t\t\tApache Accumulo - The Apache Accumulo™ sorted, distributed key/value store is a robust, scalable, high performance data storage and retrieval system.\n\t\t\t\tOpenTSDB - The Scalable Time Series Database\n\t\t\t\tApache Cassandra\n\t\t\tdata management\n\t\t\t\tApache Calcite - A Dynamic Data Management Framework\n\t\t\t\tApache Atlas - Metadata tagging & lineage capture suppoting complex business data taxonomies\n\t\t\tSQL on Hadoop\n\t\t\t\tApache Hive\n\t\t\t\tApache Phoenix A SQL skin over HBase supporting secondary indices\n\t\t\t\tPivotal HAWQ - Parallel Postgres on Hadoop\n\t\t\t\tLingual - SQL interface for Cascading (MR/Tez job generator)\n\t\t\t\tCloudera Impala\n\t\t\t\tPresto - Distributed SQL Query Engine for Big Data. Open sourced by Facebook.\n\t\t\t\tApache Tajo - Data warehouse system for Apache Hadoop\n\t\t\t\tApache Drill\n\t\t\tWorkflow, Lifecycle and Governance\n\t\t\t\tApache Oozie - Apache Oozie\n\t\t\t\tAzkaban\n\t\t\t\tApache Falcon - Data management and processing platform\n\t\t\t\tApache NiFi - A dataflow system\n\t\t\t\tAirFlow - AirFlow is a platform to programmaticaly author, schedule and monitor data pipelines\n\t\t\t\tLuigi - Python package that helps you build complex pipelines of batch jobs\n\t\t\tData Ingestion and Integration\n\t\t\t\tApache Flume - Apache Flume\n\t\t\t\tSuro - Netflix's distributed Data Pipeline\n\t\t\t\tApache Sqoop - Apache Sqoop\n\t\t\t\tApache Kafka - Apache Kafka\n\t\t\t\tGobblin from LinkedIn - Universal data ingestion framework for Hadoop\n\t\t\tDSL\n\t\t\t\tApache Pig - Apache Pig\n\t\t\t\tApache DataFu - A collection of libraries for working with large-scale data in Hadoop\n\t\t\t\tvahara - Machine learning and natural language processing with Apache Pig\n\t\t\t\tpacketpig - Open Source Big Data Security Analytics\n\t\t\t\takela - Mozilla's utility library for Hadoop, HBase, Pig, etc.\n\t\t\t\tseqpig - Simple and scalable scripting for large sequencing data set(ex: bioinfomation) in Hadoop\n\t\t\t\tLipstick - Pig workflow visualization tool. Introducing Lipstick on A(pache) Pig\n\t\t\t\tPigPen - PigPen is map-reduce for Clojure, or distributed Clojure. It compiles to Apache Pig, but you don't need to\n\t\t\tLibraries and Tools\n\t\t\t\tKite Software Development Kit - A set of libraries, tools, examples, and documentation\n\t\t\t\tgohadoop - Native go clients for Apache Hadoop YARN.\n\t\t\t\tHue - A Web interface for analyzing data with Apache Hadoop.\n\t\t\t\tApache Zeppelin - A web-based notebook that enables interactive data analytics\n\t\t\t\tJumbune - Jumbune is an open-source product built for analyzing Hadoop cluster and MapReduce jobs.\n\t\t\t\tApache Thrift\n\t\t\t\tApache Avro - Apache Avro is a data serialization system.\n\t\t\t\tElephant Bird - Twitter's collection of LZO and Protocol Buffer-related Hadoop, Pig, Hive, and HBase code.\n\t\t\t\tSpring for Apache Hadoop\n\t\t\t\thdfs - A native go client for HDFS\n\t\t\t\tOozie Eclipse Plugin - A graphical editor for editing Apache Oozie workflows inside Eclipse.\n\t\t\tRealtime Data Processing\n\t\t\t\tApache Storm\n\t\t\t\tApache Samza\n\t\t\t\tApache Spark\n\t\t\t\tApache Flink - Apache Flink is a platform for efficient, distributed, general-purpose data processing. It supports exactly once stream processing.\n\t\t\tDistributed Computing and Programming\n\t\t\t\tApache Spark\n\t\t\t\t\tSpark Packages - A community index of packages for Apache Spark\n\t\t\t\t\tSparkHub - A community site for Apache Spark\n\t\t\t\tApache Crunch\n\t\t\t\tCascading - Cascading is the proven application development platform for building data applications on Hadoop.\n\t\t\t\tApache Flink - Apache Flink is a platform for efficient, distributed, general-purpose data processing.\n\t\t\t\tApache Apex (incubating) - Enterprise-grade unified stream and batch processing engine.\n\t\t\tPackaging, Provisioning and Monitoring\n\t\t\t\tApache Bigtop - Apache Bigtop: Packaging and tests of the Apache Hadoop ecosystem\n\t\t\t\tApache Ambari - Apache Ambari\n\t\t\t\tGanglia Monitoring System\n\t\t\t\tankush - A big data cluster management tool that creates and manages clusters of different technologies.\n\t\t\t\tApache Zookeeper - Apache Zookeeper\n\t\t\t\tApache Curator - ZooKeeper client wrapper and rich ZooKeeper framework\n\t\t\t\tBuildoop - Hadoop Ecosystem Builder\n\t\t\t\tDeploop - The Hadoop Deploy System\n\t\t\t\tJumbune - An open source MapReduce profiling, MapReduce flow debugging, HDFS data quality validation and Hadoop cluster monitoring tool.\n\t\t\t\tinviso - Inviso is a lightweight tool that provides the ability to search for Hadoop jobs, visualize the performance, and view cluster utilization.\n\t\t\tSearch\n\t\t\t\tElasticSearch\n\t\t\t\tApache Solr\n\t\t\t\tSenseiDB - Open-source, distributed, realtime, semi-structured database\n\t\t\t\tBanana - Kibana port for Apache Solr\n\t\t\tSearch Engine Framework\n\t\t\t\tApache Nutch - Apache Nutch is a highly extensible and scalable open source web crawler software project.\n\t\t\tSecurity\n\t\t\t\tApache Ranger - Ranger is a framework to enable, monitor and manage comprehensive data security across the Hadoop platform.\n\t\t\t\tApache Sentry - An authorization module for Hadoop\n\t\t\t\tApache Knox Gateway - A REST API Gateway for interacting with Hadoop clusters.\n\t\t\tBenchmark\n\t\t\t\tBig Data Benchmark\n\t\t\t\tHiBench\n\t\t\t\tBig-Bench\n\t\t\t\thive-benchmarks\n\t\t\t\thive-testbench - Testbench for experimenting with Apache Hive at any data scale.\n\t\t\t\tYCSB - The Yahoo! Cloud Serving Benchmark (YCSB) is an open-source specification and program suite for evaluating retrieval and maintenance capabilities of computer programs. It is often used to compare relative performance of NoSQL database management systems.\n\t\t\tMachine learning and Big Data analytics\n\t\t\t\tApache Mahout\n\t\t\t\tOryx 2 - Lambda architecture on Spark, Kafka for real-time large scale machine learning\n\t\t\t\tMLlib - MLlib is Apache Spark's scalable machine learning library.\n\t\t\t\tR - R is a free software environment for statistical computing and graphics.\n\t\t\t\tRHadoop including RHDFS, RHBase, RMR2, plyrmr\n\t\t\t\tRHive RHive, for launching Hive queries from R\n\t\t\t\tApache Lens\n\t\tEasyHadoop\n\t\t\t快速部署工具\n\t\t\thttps://github.com/xianglei/easyhadoop\n\tsolution\n\t\thorton\n\t\t\thttp://hortonworks.com/\n\t\t星环\n\t\t\thttp://www.transwarp.cn/\n\tconf\n\t\tApacheCon\n\t\tStrata + Hadoop World\n\t\tHadoop Summit\n\t\t\thttp://hadoopsummit.org/\n\tcourse\n\t\thttp://www.infoq.com/cn/presentations/baidu-open-cloud-big-data-technology-evolution\n","slug":"bigdata-hadoop-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-04-18T13:15:20.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4ra90000kxsv6h9n3k9y","content":"<h1 id=\"about-hadoop\"><a href=\"#about-hadoop\" class=\"headerlink\" title=\"about hadoop\"></a>about hadoop</h1><pre><code>head first:\n    离线计算框架;\n    开源分布式系统实现，完整的mapreduce计算框架实现，山寨Google计算系统中的强者;\n\nusage\n    搭建大型数据仓库，PB数据存储处理分析统计等\n    搜索引擎，商业智能，日志分析，数据挖掘\nadvantage\n   高扩展，低成本，成熟的生态圈\n   业界大数据平台的首选\n   人才缺口：开发与运维\nversion\n    1.2 stable\n    2.x\nfounder\n    Doug_Cutting\n        https://en.wikipedia.org/wiki/Doug_Cutting\n        https://www.linkedin.com/in/cutting\n</code></pre><h1 id=\"resource\"><a href=\"#resource\" class=\"headerlink\" title=\"resource\"></a>resource</h1><p>resource<br>    link<br>        <a href=\"http://hadoop.apache.org\" target=\"_blank\" rel=\"external\">http://hadoop.apache.org</a><br>        小象学院<br>            <a href=\"http://www.chinahadoop.cn/\" target=\"_blank\" rel=\"external\">http://www.chinahadoop.cn/</a><br>        <a href=\"https://hadoopecosystemtable.github.io/\" target=\"_blank\" rel=\"external\">https://hadoopecosystemtable.github.io/</a><br>    awesome<br>        <a href=\"https://github.com/youngwookim/awesome-hadoop\" target=\"_blank\" rel=\"external\">https://github.com/youngwookim/awesome-hadoop</a><br>    doc<br>        <a href=\"http://hadoop.apache.org/docs/current/\" target=\"_blank\" rel=\"external\">http://hadoop.apache.org/docs/current/</a><br>    read<br>        book<br>            Hadoop: The Definitive Guide<br>            Hadoop Operations<br>            Apache Hadoop Yarn<br>            HBase: The Definitive Guide<br>            Programming Pig<br>            Programming Hive<br>            Hadoop in Practice, Second Edition<br>            Hadoop in Action, Second Edition<br>        blog<br>            <a href=\"http://zh.hortonworks.com/blog/hadoop-perect-app-for-openstack/\" target=\"_blank\" rel=\"external\">http://zh.hortonworks.com/blog/hadoop-perect-app-for-openstack/</a><br>                Savanna/Sahara<br>            <a href=\"http://hadoop123.com/\" target=\"_blank\" rel=\"external\">http://hadoop123.com/</a><br>            <a href=\"http://dongxicheng.org/\" target=\"_blank\" rel=\"external\">http://dongxicheng.org/</a><br>            <a href=\"http://blog.fens.me/hadoop-family-roadmap/\" target=\"_blank\" rel=\"external\">http://blog.fens.me/hadoop-family-roadmap/</a><br>        zhihu<br>            <a href=\"https://www.zhihu.com/topic/19563390/hot\" target=\"_blank\" rel=\"external\">https://www.zhihu.com/topic/19563390/hot</a><br>    project<br>        modules<br>            Hadoop Common<br>                The common utilities that support the other Hadoop modules.<br>            HDFS<br>                about<br>                    Hadoop Distributed File System (HDFS™)<br>                    A distributed file system that provides high-throughput access to application data.<br>                    存储海量数据的分布式文件系统，支持MapReduce操作，不支持标准POSIX接口，不操作裸磁盘</p>\n<pre><code>        局限性\n            不适合低延迟数据访问\n            无法高效存储大量小文件\n            不支持多用户写入和任意修改\n    concept\n        block\n            块，默认大小 64M，是文件存储的基本逻辑单元\n                2.x 默认128M\n            hdfs=site.xml\n        name node\n            管理节点，存放文件元数据：文件与数据块的映射表，数据块与数据节点的映射表\n\n        data node\n            工作节点，存放数据块\n\n        数据管理与容错\n            每个数据块三份副本，分布在两个机架内的三个节点\n            data node定期向 name node 发送心跳消息\n            二级name node 定期同步元数据映像文件和修改日志，name node故障时转正\n                fsimage, editlog\n        文件读取流程\n\n\n        文件写入流程\n\n\n        job  and task\n            job\n                1 job -&gt; many task\n            task\n                map task\n                reduce task\n        job tracker\n            master 管理节点\n                监控TaskTracker状态\n                分配任务，监控任务执行进度\n                作业调度\n\n        task tracker\n            data node 在同一组节点\n            执行任务\n            汇报任务状态\n        作业执行\n\n        容错机制\n            重复执行\n                四次失败后放弃\n            推测执行\n                所有map端完成后reduce端才开始；如果个别特别慢，则新建一个task tracker同步执行，认最快算完的tracker\n        sample\n            link\n                http://www.imooc.com/video/7642\n            word count\n\n\n            sort\n\n    feature\n        数据冗余，硬件容错\n        流式数据访问。一次写入，多次读取。一旦写入无法修改，只能追加写。\n        适合存储大文件\n        适合数据批量读写，吞吐高。不适合交互式应用，低延迟很难满足\n        适合一次写入多次读取，顺序读写。不支持多用户并发读写相同文件\nHadoop YARN\n     A framework for job scheduling and cluster resource management.\nHadoop MapReduce\n    about\n        A YARN-based system for parallel processing of large data sets.\n        分布式并行数据处理和计算模型和框架，实现任务分解和调度，可以分析海量数据\n        用户只需要实现map() reduce() 两个函数即可\n            形参是KV\n        高容错，高扩展，编程简单，适合大数据离线批量处理PB+\n            实时需求：storm，hbase\n    solution\n        mapr\n            https://www.mapr.com\n            The MapR Converged Data Platform integrates the power of Hadoop and Spark with global event streaming, real-time database capabilities, and enterprise storage for developing and running innovative data applications.\n</code></pre><p>The MapR Platform is powered by the industry’s fastest, most reliable, secure, and open data infrastructure that dramatically lowers TCO and enables global real-time data applications.<br>                core<br>                    分而治之思想<br>                        一个大任务分成多个子任务（map），并行执行后，合并结果（reduce）</p>\n<pre><code>                sample: 100G访问日志，找出访问次数最多的IP地址\n\n        map任务处理\n\n        reduce任务处理\n\n        map,reduce KV对格式\n\n        MR流程\n\n        MR角色\n            job client\n                提交作业\n            job tracker\n                初始化作业，分配作业，task tracker与其通讯，协调监控整个作业\n            task tracker\n                定期与job tracker通讯，执行MR任务\n            hdfs\n                读取和保存作业数据，配置，jar包，结果\n        作业提交流程\n            准备\n                编写mr程序\n                配置作业，输入输出路径等\n            提交作业\n                通过job client提交\n\n                可以提交至YARN，也可以本地运行\n            作业初始化\n\n            任务分配\n\n            错误处理\n                job tracker失败\n                    存在单点故障，hadoop2.0解决\n                task tracker失败\n\n                task 失败\n                    任务失败，会想task tracker抛出异常，最后任务挂起\n            map 输入\n\n\n            reduce输入\n                框架自动完成\n\nHadoop-related projects at Apache\n    Ambari\n        A web-based tool for provisioning, managing, and monitoring Apache Hadoop clusters which includes support for Hadoop HDFS,\n</code></pre><p>Hadoop MapReduce, Hive, HCatalog, HBase, ZooKeeper, Oozie, Pig and Sqoop. Ambari also provides a dashboard for viewing cluster health<br>such as heatmaps and ability to view MapReduce, Pig and Hive applications visually alongwith features to diagnose their performance characteristics in a user-friendly manner.<br>            Avro<br>                 A data serialization system.<br>                RPC<br>            Cassandra<br>                A scalable multi-master database with no single points of failure.<br>            Chukwa<br>                A data collection system for managing large distributed systems.<br>            HBase<br>                about<br>                    实时KV存储,放弃事务特性，最求更高的扩展，提供数据随机读写和实时访问，实现对表数据的读写功能<br>                    A scalable, distributed database that supports structured data storage for large tables.<br>                link<br>                    <a href=\"http://hbase.apache.org/\" target=\"_blank\" rel=\"external\">http://hbase.apache.org/</a><br>            Hive<br>                about<br>                    SQL on Hadoop<br>                     A data warehouse infrastructure that provides data summarization and ad hoc querying.<br>                    SQL语句-&gt;Hadoop任务<br>                link<br>                    <a href=\"http://hive.apache.org/\" target=\"_blank\" rel=\"external\">http://hive.apache.org/</a><br>            Mahout<br>                A Scalable machine learning and data mining library.<br>            Pig<br>                A high-level data-flow language and execution framework for parallel computation.<br>            Spark<br>                A fast and general compute engine for Hadoop data. Spark provides a simple and expressive programming model that supports a wide range of applications, including ETL, machine learning, stream processing, and graph computation.<br>            Tez<br>                A generalized data-flow programming framework, built on Hadoop YARN,<br>which provides a powerful and flexible engine to execute an arbitrary DAG of tasks to process data for both batch and interactive use-cases.<br>Tez is being adopted by Hive™, Pig™ and other frameworks in the Hadoop ecosystem, and also by other commercial software (e.g. ETL tools), to replace Hadoop™ MapReduce as the underlying execution engine.<br>            ZooKeeper<br>                about<br>                    锁服务，管理集群配置，监控每个节点状态，数据一致性<br>                    A high-performance coordination service for distributed applications.<br>                link<br>        awesome<br>            hadoop<br>                Apache Hadoop - Apache Hadoop<br>                Apache Tez - A Framework for YARN-based, Data Processing Applications In Hadoop<br>                SpatialHadoop - SpatialHadoop is a MapReduce extension to Apache Hadoop designed specially to work with spatial data.<br>                GIS Tools for Hadoop - Big Data Spatial Analytics for the Hadoop Framework<br>                Elasticsearch Hadoop - Elasticsearch real-time search and analytics natively integrated with Hadoop. Supports Map/Reduce, Cascading, Apache Hive and Apache Pig.<br>                dumbo - Python module that allows you to easily write and run Hadoop programs.<br>                hadoopy - Python MapReduce library written in Cython.<br>                mrjob - mrjob is a Python 2.5+ package that helps you write and run Hadoop Streaming jobs.<br>                pydoop - Pydoop is a package that provides a Python API for Hadoop.<br>                hdfs-du - HDFS-DU is an interactive visualization of the Hadoop distributed file system.<br>                White Elephant - Hadoop log aggregator and dashboard<br>                Kiji Project<br>                Genie - Genie provides REST-ful APIs to run Hadoop, Hive and Pig jobs, and to manage multiple Hadoop resources and perform job submissions across them.<br>                Apache Kylin - Apache Kylin is an open source Distributed Analytics Engine from eBay Inc. that provides SQL interface and multi-dimensional analysis (OLAP) on Hadoop supporting extremely large datasets<br>                Crunch - Go-based toolkit for ETL and feature extraction on Hadoop<br>                Apache Ignite - Distributed in-memory platform<br>            YARN<br>                Apache Slider - Apache Slider is a project in incubation at the Apache Software Foundation with the goal of making it possible and easy to deploy existing applications onto a YARN cluster.<br>                Apache Twill - Apache Twill is an abstraction over Apache Hadoop® YARN that reduces the complexity of developing distributed applications, allowing developers to focus more on their application logic.<br>                mpich2-yarn - Running MPICH2 on Yarn<br>            nosql<br>                Apache HBase - Apache HBase<br>                Apache Phoenix - A SQL skin over HBase supporting secondary indices<br>                happybase - A developer-friendly Python library to interact with Apache HBase.<br>                Hannibal - Hannibal is tool to help monitor and maintain HBase-Clusters that are configured for manual splitting.<br>                Haeinsa - Haeinsa is linearly scalable multi-row, multi-table transaction library for HBase<br>                hindex - Secondary Index for HBase<br>                Apache Accumulo - The Apache Accumulo™ sorted, distributed key/value store is a robust, scalable, high performance data storage and retrieval system.<br>                OpenTSDB - The Scalable Time Series Database<br>                Apache Cassandra<br>            data management<br>                Apache Calcite - A Dynamic Data Management Framework<br>                Apache Atlas - Metadata tagging &amp; lineage capture suppoting complex business data taxonomies<br>            SQL on Hadoop<br>                Apache Hive<br>                Apache Phoenix A SQL skin over HBase supporting secondary indices<br>                Pivotal HAWQ - Parallel Postgres on Hadoop<br>                Lingual - SQL interface for Cascading (MR/Tez job generator)<br>                Cloudera Impala<br>                Presto - Distributed SQL Query Engine for Big Data. Open sourced by Facebook.<br>                Apache Tajo - Data warehouse system for Apache Hadoop<br>                Apache Drill<br>            Workflow, Lifecycle and Governance<br>                Apache Oozie - Apache Oozie<br>                Azkaban<br>                Apache Falcon - Data management and processing platform<br>                Apache NiFi - A dataflow system<br>                AirFlow - AirFlow is a platform to programmaticaly author, schedule and monitor data pipelines<br>                Luigi - Python package that helps you build complex pipelines of batch jobs<br>            Data Ingestion and Integration<br>                Apache Flume - Apache Flume<br>                Suro - Netflix’s distributed Data Pipeline<br>                Apache Sqoop - Apache Sqoop<br>                Apache Kafka - Apache Kafka<br>                Gobblin from LinkedIn - Universal data ingestion framework for Hadoop<br>            DSL<br>                Apache Pig - Apache Pig<br>                Apache DataFu - A collection of libraries for working with large-scale data in Hadoop<br>                vahara - Machine learning and natural language processing with Apache Pig<br>                packetpig - Open Source Big Data Security Analytics<br>                akela - Mozilla’s utility library for Hadoop, HBase, Pig, etc.<br>                seqpig - Simple and scalable scripting for large sequencing data set(ex: bioinfomation) in Hadoop<br>                Lipstick - Pig workflow visualization tool. Introducing Lipstick on A(pache) Pig<br>                PigPen - PigPen is map-reduce for Clojure, or distributed Clojure. It compiles to Apache Pig, but you don’t need to<br>            Libraries and Tools<br>                Kite Software Development Kit - A set of libraries, tools, examples, and documentation<br>                gohadoop - Native go clients for Apache Hadoop YARN.<br>                Hue - A Web interface for analyzing data with Apache Hadoop.<br>                Apache Zeppelin - A web-based notebook that enables interactive data analytics<br>                Jumbune - Jumbune is an open-source product built for analyzing Hadoop cluster and MapReduce jobs.<br>                Apache Thrift<br>                Apache Avro - Apache Avro is a data serialization system.<br>                Elephant Bird - Twitter’s collection of LZO and Protocol Buffer-related Hadoop, Pig, Hive, and HBase code.<br>                Spring for Apache Hadoop<br>                hdfs - A native go client for HDFS<br>                Oozie Eclipse Plugin - A graphical editor for editing Apache Oozie workflows inside Eclipse.<br>            Realtime Data Processing<br>                Apache Storm<br>                Apache Samza<br>                Apache Spark<br>                Apache Flink - Apache Flink is a platform for efficient, distributed, general-purpose data processing. It supports exactly once stream processing.<br>            Distributed Computing and Programming<br>                Apache Spark<br>                    Spark Packages - A community index of packages for Apache Spark<br>                    SparkHub - A community site for Apache Spark<br>                Apache Crunch<br>                Cascading - Cascading is the proven application development platform for building data applications on Hadoop.<br>                Apache Flink - Apache Flink is a platform for efficient, distributed, general-purpose data processing.<br>                Apache Apex (incubating) - Enterprise-grade unified stream and batch processing engine.<br>            Packaging, Provisioning and Monitoring<br>                Apache Bigtop - Apache Bigtop: Packaging and tests of the Apache Hadoop ecosystem<br>                Apache Ambari - Apache Ambari<br>                Ganglia Monitoring System<br>                ankush - A big data cluster management tool that creates and manages clusters of different technologies.<br>                Apache Zookeeper - Apache Zookeeper<br>                Apache Curator - ZooKeeper client wrapper and rich ZooKeeper framework<br>                Buildoop - Hadoop Ecosystem Builder<br>                Deploop - The Hadoop Deploy System<br>                Jumbune - An open source MapReduce profiling, MapReduce flow debugging, HDFS data quality validation and Hadoop cluster monitoring tool.<br>                inviso - Inviso is a lightweight tool that provides the ability to search for Hadoop jobs, visualize the performance, and view cluster utilization.<br>            Search<br>                ElasticSearch<br>                Apache Solr<br>                SenseiDB - Open-source, distributed, realtime, semi-structured database<br>                Banana - Kibana port for Apache Solr<br>            Search Engine Framework<br>                Apache Nutch - Apache Nutch is a highly extensible and scalable open source web crawler software project.<br>            Security<br>                Apache Ranger - Ranger is a framework to enable, monitor and manage comprehensive data security across the Hadoop platform.<br>                Apache Sentry - An authorization module for Hadoop<br>                Apache Knox Gateway - A REST API Gateway for interacting with Hadoop clusters.<br>            Benchmark<br>                Big Data Benchmark<br>                HiBench<br>                Big-Bench<br>                hive-benchmarks<br>                hive-testbench - Testbench for experimenting with Apache Hive at any data scale.<br>                YCSB - The Yahoo! Cloud Serving Benchmark (YCSB) is an open-source specification and program suite for evaluating retrieval and maintenance capabilities of computer programs. It is often used to compare relative performance of NoSQL database management systems.<br>            Machine learning and Big Data analytics<br>                Apache Mahout<br>                Oryx 2 - Lambda architecture on Spark, Kafka for real-time large scale machine learning<br>                MLlib - MLlib is Apache Spark’s scalable machine learning library.<br>                R - R is a free software environment for statistical computing and graphics.<br>                RHadoop including RHDFS, RHBase, RMR2, plyrmr<br>                RHive RHive, for launching Hive queries from R<br>                Apache Lens<br>        EasyHadoop<br>            快速部署工具<br>            <a href=\"https://github.com/xianglei/easyhadoop\" target=\"_blank\" rel=\"external\">https://github.com/xianglei/easyhadoop</a><br>    solution<br>        horton<br>            <a href=\"http://hortonworks.com/\" target=\"_blank\" rel=\"external\">http://hortonworks.com/</a><br>        星环<br>            <a href=\"http://www.transwarp.cn/\" target=\"_blank\" rel=\"external\">http://www.transwarp.cn/</a><br>    conf<br>        ApacheCon<br>        Strata + Hadoop World<br>        Hadoop Summit<br>            <a href=\"http://hadoopsummit.org/\" target=\"_blank\" rel=\"external\">http://hadoopsummit.org/</a><br>    course<br>        <a href=\"http://www.infoq.com/cn/presentations/baidu-open-cloud-big-data-technology-evolution\" target=\"_blank\" rel=\"external\">http://www.infoq.com/cn/presentations/baidu-open-cloud-big-data-technology-evolution</a></p>\n","excerpt":"","more":"<h1 id=\"about-hadoop\"><a href=\"#about-hadoop\" class=\"headerlink\" title=\"about hadoop\"></a>about hadoop</h1><pre><code>head first:\n    离线计算框架;\n    开源分布式系统实现，完整的mapreduce计算框架实现，山寨Google计算系统中的强者;\n\nusage\n    搭建大型数据仓库，PB数据存储处理分析统计等\n    搜索引擎，商业智能，日志分析，数据挖掘\nadvantage\n   高扩展，低成本，成熟的生态圈\n   业界大数据平台的首选\n   人才缺口：开发与运维\nversion\n    1.2 stable\n    2.x\nfounder\n    Doug_Cutting\n        https://en.wikipedia.org/wiki/Doug_Cutting\n        https://www.linkedin.com/in/cutting\n</code></pre><h1 id=\"resource\"><a href=\"#resource\" class=\"headerlink\" title=\"resource\"></a>resource</h1><p>resource<br>    link<br>        <a href=\"http://hadoop.apache.org\">http://hadoop.apache.org</a><br>        小象学院<br>            <a href=\"http://www.chinahadoop.cn/\">http://www.chinahadoop.cn/</a><br>        <a href=\"https://hadoopecosystemtable.github.io/\">https://hadoopecosystemtable.github.io/</a><br>    awesome<br>        <a href=\"https://github.com/youngwookim/awesome-hadoop\">https://github.com/youngwookim/awesome-hadoop</a><br>    doc<br>        <a href=\"http://hadoop.apache.org/docs/current/\">http://hadoop.apache.org/docs/current/</a><br>    read<br>        book<br>            Hadoop: The Definitive Guide<br>            Hadoop Operations<br>            Apache Hadoop Yarn<br>            HBase: The Definitive Guide<br>            Programming Pig<br>            Programming Hive<br>            Hadoop in Practice, Second Edition<br>            Hadoop in Action, Second Edition<br>        blog<br>            <a href=\"http://zh.hortonworks.com/blog/hadoop-perect-app-for-openstack/\">http://zh.hortonworks.com/blog/hadoop-perect-app-for-openstack/</a><br>                Savanna/Sahara<br>            <a href=\"http://hadoop123.com/\">http://hadoop123.com/</a><br>            <a href=\"http://dongxicheng.org/\">http://dongxicheng.org/</a><br>            <a href=\"http://blog.fens.me/hadoop-family-roadmap/\">http://blog.fens.me/hadoop-family-roadmap/</a><br>        zhihu<br>            <a href=\"https://www.zhihu.com/topic/19563390/hot\">https://www.zhihu.com/topic/19563390/hot</a><br>    project<br>        modules<br>            Hadoop Common<br>                The common utilities that support the other Hadoop modules.<br>            HDFS<br>                about<br>                    Hadoop Distributed File System (HDFS™)<br>                    A distributed file system that provides high-throughput access to application data.<br>                    存储海量数据的分布式文件系统，支持MapReduce操作，不支持标准POSIX接口，不操作裸磁盘</p>\n<pre><code>        局限性\n            不适合低延迟数据访问\n            无法高效存储大量小文件\n            不支持多用户写入和任意修改\n    concept\n        block\n            块，默认大小 64M，是文件存储的基本逻辑单元\n                2.x 默认128M\n            hdfs=site.xml\n        name node\n            管理节点，存放文件元数据：文件与数据块的映射表，数据块与数据节点的映射表\n\n        data node\n            工作节点，存放数据块\n\n        数据管理与容错\n            每个数据块三份副本，分布在两个机架内的三个节点\n            data node定期向 name node 发送心跳消息\n            二级name node 定期同步元数据映像文件和修改日志，name node故障时转正\n                fsimage, editlog\n        文件读取流程\n\n\n        文件写入流程\n\n\n        job  and task\n            job\n                1 job -&gt; many task\n            task\n                map task\n                reduce task\n        job tracker\n            master 管理节点\n                监控TaskTracker状态\n                分配任务，监控任务执行进度\n                作业调度\n\n        task tracker\n            data node 在同一组节点\n            执行任务\n            汇报任务状态\n        作业执行\n\n        容错机制\n            重复执行\n                四次失败后放弃\n            推测执行\n                所有map端完成后reduce端才开始；如果个别特别慢，则新建一个task tracker同步执行，认最快算完的tracker\n        sample\n            link\n                http://www.imooc.com/video/7642\n            word count\n\n\n            sort\n\n    feature\n        数据冗余，硬件容错\n        流式数据访问。一次写入，多次读取。一旦写入无法修改，只能追加写。\n        适合存储大文件\n        适合数据批量读写，吞吐高。不适合交互式应用，低延迟很难满足\n        适合一次写入多次读取，顺序读写。不支持多用户并发读写相同文件\nHadoop YARN\n     A framework for job scheduling and cluster resource management.\nHadoop MapReduce\n    about\n        A YARN-based system for parallel processing of large data sets.\n        分布式并行数据处理和计算模型和框架，实现任务分解和调度，可以分析海量数据\n        用户只需要实现map() reduce() 两个函数即可\n            形参是KV\n        高容错，高扩展，编程简单，适合大数据离线批量处理PB+\n            实时需求：storm，hbase\n    solution\n        mapr\n            https://www.mapr.com\n            The MapR Converged Data Platform integrates the power of Hadoop and Spark with global event streaming, real-time database capabilities, and enterprise storage for developing and running innovative data applications.\n</code></pre><p>The MapR Platform is powered by the industry’s fastest, most reliable, secure, and open data infrastructure that dramatically lowers TCO and enables global real-time data applications.<br>                core<br>                    分而治之思想<br>                        一个大任务分成多个子任务（map），并行执行后，合并结果（reduce）</p>\n<pre><code>                sample: 100G访问日志，找出访问次数最多的IP地址\n\n        map任务处理\n\n        reduce任务处理\n\n        map,reduce KV对格式\n\n        MR流程\n\n        MR角色\n            job client\n                提交作业\n            job tracker\n                初始化作业，分配作业，task tracker与其通讯，协调监控整个作业\n            task tracker\n                定期与job tracker通讯，执行MR任务\n            hdfs\n                读取和保存作业数据，配置，jar包，结果\n        作业提交流程\n            准备\n                编写mr程序\n                配置作业，输入输出路径等\n            提交作业\n                通过job client提交\n\n                可以提交至YARN，也可以本地运行\n            作业初始化\n\n            任务分配\n\n            错误处理\n                job tracker失败\n                    存在单点故障，hadoop2.0解决\n                task tracker失败\n\n                task 失败\n                    任务失败，会想task tracker抛出异常，最后任务挂起\n            map 输入\n\n\n            reduce输入\n                框架自动完成\n\nHadoop-related projects at Apache\n    Ambari\n        A web-based tool for provisioning, managing, and monitoring Apache Hadoop clusters which includes support for Hadoop HDFS,\n</code></pre><p>Hadoop MapReduce, Hive, HCatalog, HBase, ZooKeeper, Oozie, Pig and Sqoop. Ambari also provides a dashboard for viewing cluster health<br>such as heatmaps and ability to view MapReduce, Pig and Hive applications visually alongwith features to diagnose their performance characteristics in a user-friendly manner.<br>            Avro<br>                 A data serialization system.<br>                RPC<br>            Cassandra<br>                A scalable multi-master database with no single points of failure.<br>            Chukwa<br>                A data collection system for managing large distributed systems.<br>            HBase<br>                about<br>                    实时KV存储,放弃事务特性，最求更高的扩展，提供数据随机读写和实时访问，实现对表数据的读写功能<br>                    A scalable, distributed database that supports structured data storage for large tables.<br>                link<br>                    <a href=\"http://hbase.apache.org/\">http://hbase.apache.org/</a><br>            Hive<br>                about<br>                    SQL on Hadoop<br>                     A data warehouse infrastructure that provides data summarization and ad hoc querying.<br>                    SQL语句-&gt;Hadoop任务<br>                link<br>                    <a href=\"http://hive.apache.org/\">http://hive.apache.org/</a><br>            Mahout<br>                A Scalable machine learning and data mining library.<br>            Pig<br>                A high-level data-flow language and execution framework for parallel computation.<br>            Spark<br>                A fast and general compute engine for Hadoop data. Spark provides a simple and expressive programming model that supports a wide range of applications, including ETL, machine learning, stream processing, and graph computation.<br>            Tez<br>                A generalized data-flow programming framework, built on Hadoop YARN,<br>which provides a powerful and flexible engine to execute an arbitrary DAG of tasks to process data for both batch and interactive use-cases.<br>Tez is being adopted by Hive™, Pig™ and other frameworks in the Hadoop ecosystem, and also by other commercial software (e.g. ETL tools), to replace Hadoop™ MapReduce as the underlying execution engine.<br>            ZooKeeper<br>                about<br>                    锁服务，管理集群配置，监控每个节点状态，数据一致性<br>                    A high-performance coordination service for distributed applications.<br>                link<br>        awesome<br>            hadoop<br>                Apache Hadoop - Apache Hadoop<br>                Apache Tez - A Framework for YARN-based, Data Processing Applications In Hadoop<br>                SpatialHadoop - SpatialHadoop is a MapReduce extension to Apache Hadoop designed specially to work with spatial data.<br>                GIS Tools for Hadoop - Big Data Spatial Analytics for the Hadoop Framework<br>                Elasticsearch Hadoop - Elasticsearch real-time search and analytics natively integrated with Hadoop. Supports Map/Reduce, Cascading, Apache Hive and Apache Pig.<br>                dumbo - Python module that allows you to easily write and run Hadoop programs.<br>                hadoopy - Python MapReduce library written in Cython.<br>                mrjob - mrjob is a Python 2.5+ package that helps you write and run Hadoop Streaming jobs.<br>                pydoop - Pydoop is a package that provides a Python API for Hadoop.<br>                hdfs-du - HDFS-DU is an interactive visualization of the Hadoop distributed file system.<br>                White Elephant - Hadoop log aggregator and dashboard<br>                Kiji Project<br>                Genie - Genie provides REST-ful APIs to run Hadoop, Hive and Pig jobs, and to manage multiple Hadoop resources and perform job submissions across them.<br>                Apache Kylin - Apache Kylin is an open source Distributed Analytics Engine from eBay Inc. that provides SQL interface and multi-dimensional analysis (OLAP) on Hadoop supporting extremely large datasets<br>                Crunch - Go-based toolkit for ETL and feature extraction on Hadoop<br>                Apache Ignite - Distributed in-memory platform<br>            YARN<br>                Apache Slider - Apache Slider is a project in incubation at the Apache Software Foundation with the goal of making it possible and easy to deploy existing applications onto a YARN cluster.<br>                Apache Twill - Apache Twill is an abstraction over Apache Hadoop® YARN that reduces the complexity of developing distributed applications, allowing developers to focus more on their application logic.<br>                mpich2-yarn - Running MPICH2 on Yarn<br>            nosql<br>                Apache HBase - Apache HBase<br>                Apache Phoenix - A SQL skin over HBase supporting secondary indices<br>                happybase - A developer-friendly Python library to interact with Apache HBase.<br>                Hannibal - Hannibal is tool to help monitor and maintain HBase-Clusters that are configured for manual splitting.<br>                Haeinsa - Haeinsa is linearly scalable multi-row, multi-table transaction library for HBase<br>                hindex - Secondary Index for HBase<br>                Apache Accumulo - The Apache Accumulo™ sorted, distributed key/value store is a robust, scalable, high performance data storage and retrieval system.<br>                OpenTSDB - The Scalable Time Series Database<br>                Apache Cassandra<br>            data management<br>                Apache Calcite - A Dynamic Data Management Framework<br>                Apache Atlas - Metadata tagging &amp; lineage capture suppoting complex business data taxonomies<br>            SQL on Hadoop<br>                Apache Hive<br>                Apache Phoenix A SQL skin over HBase supporting secondary indices<br>                Pivotal HAWQ - Parallel Postgres on Hadoop<br>                Lingual - SQL interface for Cascading (MR/Tez job generator)<br>                Cloudera Impala<br>                Presto - Distributed SQL Query Engine for Big Data. Open sourced by Facebook.<br>                Apache Tajo - Data warehouse system for Apache Hadoop<br>                Apache Drill<br>            Workflow, Lifecycle and Governance<br>                Apache Oozie - Apache Oozie<br>                Azkaban<br>                Apache Falcon - Data management and processing platform<br>                Apache NiFi - A dataflow system<br>                AirFlow - AirFlow is a platform to programmaticaly author, schedule and monitor data pipelines<br>                Luigi - Python package that helps you build complex pipelines of batch jobs<br>            Data Ingestion and Integration<br>                Apache Flume - Apache Flume<br>                Suro - Netflix’s distributed Data Pipeline<br>                Apache Sqoop - Apache Sqoop<br>                Apache Kafka - Apache Kafka<br>                Gobblin from LinkedIn - Universal data ingestion framework for Hadoop<br>            DSL<br>                Apache Pig - Apache Pig<br>                Apache DataFu - A collection of libraries for working with large-scale data in Hadoop<br>                vahara - Machine learning and natural language processing with Apache Pig<br>                packetpig - Open Source Big Data Security Analytics<br>                akela - Mozilla’s utility library for Hadoop, HBase, Pig, etc.<br>                seqpig - Simple and scalable scripting for large sequencing data set(ex: bioinfomation) in Hadoop<br>                Lipstick - Pig workflow visualization tool. Introducing Lipstick on A(pache) Pig<br>                PigPen - PigPen is map-reduce for Clojure, or distributed Clojure. It compiles to Apache Pig, but you don’t need to<br>            Libraries and Tools<br>                Kite Software Development Kit - A set of libraries, tools, examples, and documentation<br>                gohadoop - Native go clients for Apache Hadoop YARN.<br>                Hue - A Web interface for analyzing data with Apache Hadoop.<br>                Apache Zeppelin - A web-based notebook that enables interactive data analytics<br>                Jumbune - Jumbune is an open-source product built for analyzing Hadoop cluster and MapReduce jobs.<br>                Apache Thrift<br>                Apache Avro - Apache Avro is a data serialization system.<br>                Elephant Bird - Twitter’s collection of LZO and Protocol Buffer-related Hadoop, Pig, Hive, and HBase code.<br>                Spring for Apache Hadoop<br>                hdfs - A native go client for HDFS<br>                Oozie Eclipse Plugin - A graphical editor for editing Apache Oozie workflows inside Eclipse.<br>            Realtime Data Processing<br>                Apache Storm<br>                Apache Samza<br>                Apache Spark<br>                Apache Flink - Apache Flink is a platform for efficient, distributed, general-purpose data processing. It supports exactly once stream processing.<br>            Distributed Computing and Programming<br>                Apache Spark<br>                    Spark Packages - A community index of packages for Apache Spark<br>                    SparkHub - A community site for Apache Spark<br>                Apache Crunch<br>                Cascading - Cascading is the proven application development platform for building data applications on Hadoop.<br>                Apache Flink - Apache Flink is a platform for efficient, distributed, general-purpose data processing.<br>                Apache Apex (incubating) - Enterprise-grade unified stream and batch processing engine.<br>            Packaging, Provisioning and Monitoring<br>                Apache Bigtop - Apache Bigtop: Packaging and tests of the Apache Hadoop ecosystem<br>                Apache Ambari - Apache Ambari<br>                Ganglia Monitoring System<br>                ankush - A big data cluster management tool that creates and manages clusters of different technologies.<br>                Apache Zookeeper - Apache Zookeeper<br>                Apache Curator - ZooKeeper client wrapper and rich ZooKeeper framework<br>                Buildoop - Hadoop Ecosystem Builder<br>                Deploop - The Hadoop Deploy System<br>                Jumbune - An open source MapReduce profiling, MapReduce flow debugging, HDFS data quality validation and Hadoop cluster monitoring tool.<br>                inviso - Inviso is a lightweight tool that provides the ability to search for Hadoop jobs, visualize the performance, and view cluster utilization.<br>            Search<br>                ElasticSearch<br>                Apache Solr<br>                SenseiDB - Open-source, distributed, realtime, semi-structured database<br>                Banana - Kibana port for Apache Solr<br>            Search Engine Framework<br>                Apache Nutch - Apache Nutch is a highly extensible and scalable open source web crawler software project.<br>            Security<br>                Apache Ranger - Ranger is a framework to enable, monitor and manage comprehensive data security across the Hadoop platform.<br>                Apache Sentry - An authorization module for Hadoop<br>                Apache Knox Gateway - A REST API Gateway for interacting with Hadoop clusters.<br>            Benchmark<br>                Big Data Benchmark<br>                HiBench<br>                Big-Bench<br>                hive-benchmarks<br>                hive-testbench - Testbench for experimenting with Apache Hive at any data scale.<br>                YCSB - The Yahoo! Cloud Serving Benchmark (YCSB) is an open-source specification and program suite for evaluating retrieval and maintenance capabilities of computer programs. It is often used to compare relative performance of NoSQL database management systems.<br>            Machine learning and Big Data analytics<br>                Apache Mahout<br>                Oryx 2 - Lambda architecture on Spark, Kafka for real-time large scale machine learning<br>                MLlib - MLlib is Apache Spark’s scalable machine learning library.<br>                R - R is a free software environment for statistical computing and graphics.<br>                RHadoop including RHDFS, RHBase, RMR2, plyrmr<br>                RHive RHive, for launching Hive queries from R<br>                Apache Lens<br>        EasyHadoop<br>            快速部署工具<br>            <a href=\"https://github.com/xianglei/easyhadoop\">https://github.com/xianglei/easyhadoop</a><br>    solution<br>        horton<br>            <a href=\"http://hortonworks.com/\">http://hortonworks.com/</a><br>        星环<br>            <a href=\"http://www.transwarp.cn/\">http://www.transwarp.cn/</a><br>    conf<br>        ApacheCon<br>        Strata + Hadoop World<br>        Hadoop Summit<br>            <a href=\"http://hadoopsummit.org/\">http://hadoopsummit.org/</a><br>    course<br>        <a href=\"http://www.infoq.com/cn/presentations/baidu-open-cloud-big-data-technology-evolution\">http://www.infoq.com/cn/presentations/baidu-open-cloud-big-data-technology-evolution</a></p>\n"},{"title":"machine learning portal","_content":"\n# machine learning portal\n","source":"_posts/bigdata-ml-portal.md","raw":"---\ntitle: machine learning portal\ncategories:\n- bigdata\ntags:\n- portal\n- machine learning\n---\n\n# machine learning portal\n","slug":"bigdata-ml-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-04-18T13:15:17.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rag0001kxsvg9dkqgif","content":"<h1 id=\"machine-learning-portal\"><a href=\"#machine-learning-portal\" class=\"headerlink\" title=\"machine learning portal\"></a>machine learning portal</h1>","excerpt":"","more":"<h1 id=\"machine-learning-portal\"><a href=\"#machine-learning-portal\" class=\"headerlink\" title=\"machine learning portal\"></a>machine learning portal</h1>"},{"title":"storm portal","_content":"\n# storm portal\n","source":"_posts/bigdata-storm-portal.md","raw":"---\ntitle: storm portal\ncategories:\n- bigdata\ntags:\n- portal\n- storm\n---\n\n# storm portal\n","slug":"bigdata-storm-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-04-18T13:15:14.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rar0004kxsvv2nhnujf","content":"<h1 id=\"storm-portal\"><a href=\"#storm-portal\" class=\"headerlink\" title=\"storm portal\"></a>storm portal</h1>","excerpt":"","more":"<h1 id=\"storm-portal\"><a href=\"#storm-portal\" class=\"headerlink\" title=\"storm portal\"></a>storm portal</h1>"},{"title":"cloud portal","_content":"\n# aws\nhttps://github.com/donnemartin/saws\n\n# top resource\ninfoq: http://2017.qconbeijing.com/tracks\n\nhttps://www.oschina.net/p/dubbo\nhttps://github.com/alibaba/dubbo\n\n\n## zookeeper\n\n- about\n\n        ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services. All of these kinds of services are used in some form or another by distributed applications. Each time they are implemented there is a lot of work that goes into fixing the bugs and race conditions that are inevitable. Because of the difficulty of implementing these kinds of services, applications initially usually skimp on them ,which make them brittle in the presence of change and difficult to manage. Even when done correctly, different implementations of these services lead to management complexity when the applications are deployed.\n\n        考虑一下有多个服务器的分布式系统，每台服务器都负责保存数据，在数据上执行操作。这样的潜在例子包括分布式搜索引擎、分布式构建系统或者已知的系统如Apache Hadoop。所有这些分布式系统的一个常见问题是，你如何在任一时间点确定哪些服务器活着并且在工作中。最重要的是，当面对这些分布式计算的难题，例如网络失败、带宽限制、可变延迟连接、安全问题以及任何网络环境，甚至跨多个数据中心时可能发生的错误时，你如何可靠地做这些事。这些正是Apache ZooKeeper所关注的问题，它是一个快速、高可用、容错、分布式的协调服务。你可以使用ZooKeeper构建可靠的、分布式的数据结构，用于群组成员、领导人选举、协同工作流和配置服务，以及广义的分布式数据结构如锁、队列、屏障（Barrier）和锁存器（Latch）。许多知名且成功的项目依赖于ZooKeeper，其中包括HBase、Hadoop 2.0、Solr Cloud、Neo4J、Apache Blur（Incubating）和Accumulo。\n\n        ZooKeeper是一个分布式的、分层级的文件系统，能促进客户端间的松耦合，并提供最终一致的，类似于传统文件系统中文件和目录的Znode视图。它提供了基本的操作，例如创建、删除和检查Znode是否存在。它提供了事件驱动模型，客户端能观察特定Znode的变化，例如现有Znode增加了一个新的子节点。ZooKeeper运行多个ZooKeeper服务器，称为Ensemble，以获得高可用性。每个服务器都持有分布式文件系统的内存复本，为客户端的读取请求提供服务。\n\n- usage\n\n        所有分布式的协商和一致都可以利用zk实现。可以理解为一个分布式的带有订阅功能的小型元数据库。\n        使用方式自然就是通过上面这句话而定。比如你需要一个订阅功能的数据库，发布你的信息给其他客户端，享受订阅功能。又比如你有很多信息是由很多客户端同时竞争写入zk,但只允许第一个到达的写入，就可以享受zk的一致性。\n\n        现在流行的分布式系统已经完全无法脱离zookeeper。列举几个简单的例子：\n        storm中用zookeeper来协调同步集群中机器的状态（并不传递消息）。基本不会有负载，对机器性能没什么要求。jstorm也是用zk来做一致性服务。\n\n        dubbo中采用zookeeper来作注册中心，在阿里内部采用的是基于数据库的注册中心，我们自己用dubbo的时候采用了zookeeper，配置起来非常方便。当然如果采用redis或者自己写个简单的基于内存的注册中心也是可以的。从稳定性来讲，个人觉得zookeeper是比较好的选择，毕竟zookeeper集群中的机器只要不是半数以上宕掉，服务就是可用的。dubbo服务的消费者和提供者都需要用到zookeeper，然而消费者和提供者之间的长连接建立后，zookeeper参与程度就比较弱了(仅需要接受一些心跳包)，除非此时有新的提供者消费者加入或者离开(需要更新节点数据)。因此负载也是非常低的，基本不用考虑性能问题。zookeeper还同时起到了监控各个服务的作用。\n\n        HBase要用到zk，这个已经有答主提到，就不用细说了。Hadoop也是需要用到zookeeper的，用来管理集群中的NameNode。差点忘了，Kafka集群依赖于ZooKeeper。kafka通过zookeeper实现生产者在消费端的负载均衡，动态的集群扩展等等。\n\n\nresource:\n    http://zookeeper.apache.org/\n    http://www.ibm.com/developerworks/cn/opensource/os-cn-zookeeper/\n\n# distributed messaging\n\n## kafka\n\nabout\n\n    Apache Kafka是分布式发布-订阅消息系统。它最初由LinkedIn公司开发，之后成为Apache项目的一部分。Kafka是一种快速、可扩展的、设计内在就是分布式的，分区的和可复制的提交日志服务。Kafka是linkedin开源的MQ系统，主要特点是基于Pull的模式来处理消息消费，追求高吞吐量，一开始的目的就是用于日志收集和传输，0.8开始支持复制，不支持事务，适合产生大量数据的互联网服务的数据收集业务。\n\n    Apache Kafka与传统消息系统相比，有以下不同：\n    它被设计为一个分布式系统，易于向外扩展；\n    它同时为发布和订阅提供高吞吐量；\n    它支持多订阅者，当失败时能自动平衡消费者；\n    它将消息持久化到磁盘，因此可用于批量消费，例如ETL，以及实时应用程序。\n\n    Kafka 使用自己的协议。Kafka 自身服务和消费者都需要依赖 Zookeeper。RabbitMQ 在有大量消息堆积的情况下性能会下降，Kafka不会。毕竟AMQP设计的初衷不是用来持久化海量消息的，而Kafka一开始是用来处理海量日志的。\n\n\nresource：\n    http://kafka.apache.org/\n    https://github.com/apache/kafka\n\n    http://www.infoq.com/cn/articles/apache-kafka\n    http://www.infoq.com/cn/presentations/use-apache-kafka-to-transfer-key-business-message?utm_source=infoq&utm_medium=related_content_link&utm_campaign=relatedContent_articles_clk\n\n    must read:\n        https://github.com/oldratlee/translations/blob/master/log-what-every-software-engineer-should-know-about-real-time-datas-unifying/README.md\n        http://www.cnblogs.com/foreach-break/p/notes_about_distributed_system_and_The_log.html\n\n    faq：https://www.zhihu.com/question/22480085\n\n## rabbitmq\nRabbitMQ 支持 AMQP（二进制），STOMP（文本），MQTT（二进制），HTTP（里面包装其他协议）等协议。\n\n## rocketmq\nabout\n    RocketMQ 是一款分布式、队列模型的消息中间件，具有以下特点：\n\n    能够保证严格的消息顺序\n    提供丰富的消息拉取模式\n    高效的订阅者水平扩展能力\n    实时的消息订阅机制\n    亿级消息堆积能力\n    Metaq3.0 版本改名，产品名称改为RocketMQ\n\n    RMQ的结构分为四个部分：生产者、消费者、nameserver、brokerserver\n    nameserver：nameserver接收broker的请求注册broker路由信息。收client的请求根据某个topic获取所有到broker的路由信息。\n    brokerserver：消息的接收和推送，\n    生产者：发送消息，将消息推送给brokerserver。\n    消费者：接收消息，从brokerserver上获取消息。\n\nresource:\n    https://www.aliyun.com/product/ons\n    https://github.com/apache/incubator-rocketmq\n    https://rocketmq.incubator.apache.org/\n    http://www.jianshu.com/p/453c6e7ff81c\n    http://www.jialeens.com/archives/681.html?utm_source=tuicool&utm_medium=referral\n\n## zeromq\nZeroMQ 和 RabbitMQ/Kafka 不同，它只是一个异步消息库，在套接字的基础上提供了类似于消息代理的机制。使用 ZeroMQ 的话，需要对自己的业务代码进行改造，不利于服务解耦。\n\n\n# distribute kv\n\n## etcd\nabout\n        etcd 是一个应用在分布式环境下的 key/value 存储服务。利用 etcd 的特性，应用程序可以在集群中共享信息、配置或作服务发现，etcd 会在集群的各个节点中复制这些数据并保证这些数据始终正确。etcd 无论是在 CoreOS 还是 Kubernetes 体系中都是不可或缺的一环。\n\n        2. 规范词汇表\n        etcd 0.5.0 版首次对 etcd 代码、文档及 CLI 中使用的术语进行了定义。\n\n        2.1. node\n\n        node 指一个 raft 状态机实例。每个 node 都具有唯一的标识，并在处于 leader 状态时记录其它节点的步进数。\n\n        2.2. member\n\n        member 指一个 etcd 实例。member 运行在每个 node 上，并向这一 node 上的其它应用程序提供服务。\n\n        2.3. Cluster\n\n        Cluster 由多个 member 组成。每个 member 中的 node 遵循 raft 共识协议来复制日志。Cluster 接收来自 member 的提案消息，将其提交并存储于本地磁盘。\n\n        2.4. Peer\n\n        同一 Cluster 中的其它 member。\n\n        2.5. Client\n\n        Client 指调用 Cluster API 的对象。\n\n        3. Raft 共识算法\n        etcd 集群的工作原理基于 raft 共识算法 (The Raft Consensus Algorithm)。etcd 在 0.5.0 版本中重新实现了 raft 算法，而非像之前那样依赖于第三方库 go-raft 。raft 共识算法的优点在于可以在高效的解决分布式系统中各个节点日志内容一致性问题的同时，也使得集群具备一定的容错能力。即使集群中出现部分节点故障、网络故障等问题，仍可保证其余大多数节点正确的步进。甚至当更多的节点（一般来说超过集群节点总数的一半）出现故障而导致集群不可用时，依然可以保证节点中的数据不会出现错误的结果。\n\n\nhttps://github.com/coreos/etcd\nhttp://www.infoq.com/cn/articles/coreos-analyse-etcd/\n\n\n# IOE resource\n\nIBM doc:\nhttp://www.ibm.com/analytics/cn/zh/\n\nOracle doc: http://docs.oracle.com/en/\nSAP doc: http://www.sap.com/developer.html\nSAP case: http://www.bestsapchina.com/ResourceCenter/i-t-s.html\n","source":"_posts/cloud--portal.md","raw":"---\ntitle: cloud portal\ncategories:\n- cloud\ntags:\n- portal\n- cloud\n---\n\n# aws\nhttps://github.com/donnemartin/saws\n\n# top resource\ninfoq: http://2017.qconbeijing.com/tracks\n\nhttps://www.oschina.net/p/dubbo\nhttps://github.com/alibaba/dubbo\n\n\n## zookeeper\n\n- about\n\n        ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services. All of these kinds of services are used in some form or another by distributed applications. Each time they are implemented there is a lot of work that goes into fixing the bugs and race conditions that are inevitable. Because of the difficulty of implementing these kinds of services, applications initially usually skimp on them ,which make them brittle in the presence of change and difficult to manage. Even when done correctly, different implementations of these services lead to management complexity when the applications are deployed.\n\n        考虑一下有多个服务器的分布式系统，每台服务器都负责保存数据，在数据上执行操作。这样的潜在例子包括分布式搜索引擎、分布式构建系统或者已知的系统如Apache Hadoop。所有这些分布式系统的一个常见问题是，你如何在任一时间点确定哪些服务器活着并且在工作中。最重要的是，当面对这些分布式计算的难题，例如网络失败、带宽限制、可变延迟连接、安全问题以及任何网络环境，甚至跨多个数据中心时可能发生的错误时，你如何可靠地做这些事。这些正是Apache ZooKeeper所关注的问题，它是一个快速、高可用、容错、分布式的协调服务。你可以使用ZooKeeper构建可靠的、分布式的数据结构，用于群组成员、领导人选举、协同工作流和配置服务，以及广义的分布式数据结构如锁、队列、屏障（Barrier）和锁存器（Latch）。许多知名且成功的项目依赖于ZooKeeper，其中包括HBase、Hadoop 2.0、Solr Cloud、Neo4J、Apache Blur（Incubating）和Accumulo。\n\n        ZooKeeper是一个分布式的、分层级的文件系统，能促进客户端间的松耦合，并提供最终一致的，类似于传统文件系统中文件和目录的Znode视图。它提供了基本的操作，例如创建、删除和检查Znode是否存在。它提供了事件驱动模型，客户端能观察特定Znode的变化，例如现有Znode增加了一个新的子节点。ZooKeeper运行多个ZooKeeper服务器，称为Ensemble，以获得高可用性。每个服务器都持有分布式文件系统的内存复本，为客户端的读取请求提供服务。\n\n- usage\n\n        所有分布式的协商和一致都可以利用zk实现。可以理解为一个分布式的带有订阅功能的小型元数据库。\n        使用方式自然就是通过上面这句话而定。比如你需要一个订阅功能的数据库，发布你的信息给其他客户端，享受订阅功能。又比如你有很多信息是由很多客户端同时竞争写入zk,但只允许第一个到达的写入，就可以享受zk的一致性。\n\n        现在流行的分布式系统已经完全无法脱离zookeeper。列举几个简单的例子：\n        storm中用zookeeper来协调同步集群中机器的状态（并不传递消息）。基本不会有负载，对机器性能没什么要求。jstorm也是用zk来做一致性服务。\n\n        dubbo中采用zookeeper来作注册中心，在阿里内部采用的是基于数据库的注册中心，我们自己用dubbo的时候采用了zookeeper，配置起来非常方便。当然如果采用redis或者自己写个简单的基于内存的注册中心也是可以的。从稳定性来讲，个人觉得zookeeper是比较好的选择，毕竟zookeeper集群中的机器只要不是半数以上宕掉，服务就是可用的。dubbo服务的消费者和提供者都需要用到zookeeper，然而消费者和提供者之间的长连接建立后，zookeeper参与程度就比较弱了(仅需要接受一些心跳包)，除非此时有新的提供者消费者加入或者离开(需要更新节点数据)。因此负载也是非常低的，基本不用考虑性能问题。zookeeper还同时起到了监控各个服务的作用。\n\n        HBase要用到zk，这个已经有答主提到，就不用细说了。Hadoop也是需要用到zookeeper的，用来管理集群中的NameNode。差点忘了，Kafka集群依赖于ZooKeeper。kafka通过zookeeper实现生产者在消费端的负载均衡，动态的集群扩展等等。\n\n\nresource:\n    http://zookeeper.apache.org/\n    http://www.ibm.com/developerworks/cn/opensource/os-cn-zookeeper/\n\n# distributed messaging\n\n## kafka\n\nabout\n\n    Apache Kafka是分布式发布-订阅消息系统。它最初由LinkedIn公司开发，之后成为Apache项目的一部分。Kafka是一种快速、可扩展的、设计内在就是分布式的，分区的和可复制的提交日志服务。Kafka是linkedin开源的MQ系统，主要特点是基于Pull的模式来处理消息消费，追求高吞吐量，一开始的目的就是用于日志收集和传输，0.8开始支持复制，不支持事务，适合产生大量数据的互联网服务的数据收集业务。\n\n    Apache Kafka与传统消息系统相比，有以下不同：\n    它被设计为一个分布式系统，易于向外扩展；\n    它同时为发布和订阅提供高吞吐量；\n    它支持多订阅者，当失败时能自动平衡消费者；\n    它将消息持久化到磁盘，因此可用于批量消费，例如ETL，以及实时应用程序。\n\n    Kafka 使用自己的协议。Kafka 自身服务和消费者都需要依赖 Zookeeper。RabbitMQ 在有大量消息堆积的情况下性能会下降，Kafka不会。毕竟AMQP设计的初衷不是用来持久化海量消息的，而Kafka一开始是用来处理海量日志的。\n\n\nresource：\n    http://kafka.apache.org/\n    https://github.com/apache/kafka\n\n    http://www.infoq.com/cn/articles/apache-kafka\n    http://www.infoq.com/cn/presentations/use-apache-kafka-to-transfer-key-business-message?utm_source=infoq&utm_medium=related_content_link&utm_campaign=relatedContent_articles_clk\n\n    must read:\n        https://github.com/oldratlee/translations/blob/master/log-what-every-software-engineer-should-know-about-real-time-datas-unifying/README.md\n        http://www.cnblogs.com/foreach-break/p/notes_about_distributed_system_and_The_log.html\n\n    faq：https://www.zhihu.com/question/22480085\n\n## rabbitmq\nRabbitMQ 支持 AMQP（二进制），STOMP（文本），MQTT（二进制），HTTP（里面包装其他协议）等协议。\n\n## rocketmq\nabout\n    RocketMQ 是一款分布式、队列模型的消息中间件，具有以下特点：\n\n    能够保证严格的消息顺序\n    提供丰富的消息拉取模式\n    高效的订阅者水平扩展能力\n    实时的消息订阅机制\n    亿级消息堆积能力\n    Metaq3.0 版本改名，产品名称改为RocketMQ\n\n    RMQ的结构分为四个部分：生产者、消费者、nameserver、brokerserver\n    nameserver：nameserver接收broker的请求注册broker路由信息。收client的请求根据某个topic获取所有到broker的路由信息。\n    brokerserver：消息的接收和推送，\n    生产者：发送消息，将消息推送给brokerserver。\n    消费者：接收消息，从brokerserver上获取消息。\n\nresource:\n    https://www.aliyun.com/product/ons\n    https://github.com/apache/incubator-rocketmq\n    https://rocketmq.incubator.apache.org/\n    http://www.jianshu.com/p/453c6e7ff81c\n    http://www.jialeens.com/archives/681.html?utm_source=tuicool&utm_medium=referral\n\n## zeromq\nZeroMQ 和 RabbitMQ/Kafka 不同，它只是一个异步消息库，在套接字的基础上提供了类似于消息代理的机制。使用 ZeroMQ 的话，需要对自己的业务代码进行改造，不利于服务解耦。\n\n\n# distribute kv\n\n## etcd\nabout\n        etcd 是一个应用在分布式环境下的 key/value 存储服务。利用 etcd 的特性，应用程序可以在集群中共享信息、配置或作服务发现，etcd 会在集群的各个节点中复制这些数据并保证这些数据始终正确。etcd 无论是在 CoreOS 还是 Kubernetes 体系中都是不可或缺的一环。\n\n        2. 规范词汇表\n        etcd 0.5.0 版首次对 etcd 代码、文档及 CLI 中使用的术语进行了定义。\n\n        2.1. node\n\n        node 指一个 raft 状态机实例。每个 node 都具有唯一的标识，并在处于 leader 状态时记录其它节点的步进数。\n\n        2.2. member\n\n        member 指一个 etcd 实例。member 运行在每个 node 上，并向这一 node 上的其它应用程序提供服务。\n\n        2.3. Cluster\n\n        Cluster 由多个 member 组成。每个 member 中的 node 遵循 raft 共识协议来复制日志。Cluster 接收来自 member 的提案消息，将其提交并存储于本地磁盘。\n\n        2.4. Peer\n\n        同一 Cluster 中的其它 member。\n\n        2.5. Client\n\n        Client 指调用 Cluster API 的对象。\n\n        3. Raft 共识算法\n        etcd 集群的工作原理基于 raft 共识算法 (The Raft Consensus Algorithm)。etcd 在 0.5.0 版本中重新实现了 raft 算法，而非像之前那样依赖于第三方库 go-raft 。raft 共识算法的优点在于可以在高效的解决分布式系统中各个节点日志内容一致性问题的同时，也使得集群具备一定的容错能力。即使集群中出现部分节点故障、网络故障等问题，仍可保证其余大多数节点正确的步进。甚至当更多的节点（一般来说超过集群节点总数的一半）出现故障而导致集群不可用时，依然可以保证节点中的数据不会出现错误的结果。\n\n\nhttps://github.com/coreos/etcd\nhttp://www.infoq.com/cn/articles/coreos-analyse-etcd/\n\n\n# IOE resource\n\nIBM doc:\nhttp://www.ibm.com/analytics/cn/zh/\n\nOracle doc: http://docs.oracle.com/en/\nSAP doc: http://www.sap.com/developer.html\nSAP case: http://www.bestsapchina.com/ResourceCenter/i-t-s.html\n","slug":"cloud--portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-04-18T13:15:07.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rau0005kxsv4e6kk5fw","content":"<h1 id=\"aws\"><a href=\"#aws\" class=\"headerlink\" title=\"aws\"></a>aws</h1><p><a href=\"https://github.com/donnemartin/saws\" target=\"_blank\" rel=\"external\">https://github.com/donnemartin/saws</a></p>\n<h1 id=\"top-resource\"><a href=\"#top-resource\" class=\"headerlink\" title=\"top resource\"></a>top resource</h1><p>infoq: <a href=\"http://2017.qconbeijing.com/tracks\" target=\"_blank\" rel=\"external\">http://2017.qconbeijing.com/tracks</a></p>\n<p><a href=\"https://www.oschina.net/p/dubbo\" target=\"_blank\" rel=\"external\">https://www.oschina.net/p/dubbo</a><br><a href=\"https://github.com/alibaba/dubbo\" target=\"_blank\" rel=\"external\">https://github.com/alibaba/dubbo</a></p>\n<h2 id=\"zookeeper\"><a href=\"#zookeeper\" class=\"headerlink\" title=\"zookeeper\"></a>zookeeper</h2><ul>\n<li><p>about</p>\n<pre><code>ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services. All of these kinds of services are used in some form or another by distributed applications. Each time they are implemented there is a lot of work that goes into fixing the bugs and race conditions that are inevitable. Because of the difficulty of implementing these kinds of services, applications initially usually skimp on them ,which make them brittle in the presence of change and difficult to manage. Even when done correctly, different implementations of these services lead to management complexity when the applications are deployed.\n\n考虑一下有多个服务器的分布式系统，每台服务器都负责保存数据，在数据上执行操作。这样的潜在例子包括分布式搜索引擎、分布式构建系统或者已知的系统如Apache Hadoop。所有这些分布式系统的一个常见问题是，你如何在任一时间点确定哪些服务器活着并且在工作中。最重要的是，当面对这些分布式计算的难题，例如网络失败、带宽限制、可变延迟连接、安全问题以及任何网络环境，甚至跨多个数据中心时可能发生的错误时，你如何可靠地做这些事。这些正是Apache ZooKeeper所关注的问题，它是一个快速、高可用、容错、分布式的协调服务。你可以使用ZooKeeper构建可靠的、分布式的数据结构，用于群组成员、领导人选举、协同工作流和配置服务，以及广义的分布式数据结构如锁、队列、屏障（Barrier）和锁存器（Latch）。许多知名且成功的项目依赖于ZooKeeper，其中包括HBase、Hadoop 2.0、Solr Cloud、Neo4J、Apache Blur（Incubating）和Accumulo。\n\nZooKeeper是一个分布式的、分层级的文件系统，能促进客户端间的松耦合，并提供最终一致的，类似于传统文件系统中文件和目录的Znode视图。它提供了基本的操作，例如创建、删除和检查Znode是否存在。它提供了事件驱动模型，客户端能观察特定Znode的变化，例如现有Znode增加了一个新的子节点。ZooKeeper运行多个ZooKeeper服务器，称为Ensemble，以获得高可用性。每个服务器都持有分布式文件系统的内存复本，为客户端的读取请求提供服务。\n</code></pre></li>\n<li><p>usage</p>\n<pre><code>所有分布式的协商和一致都可以利用zk实现。可以理解为一个分布式的带有订阅功能的小型元数据库。\n使用方式自然就是通过上面这句话而定。比如你需要一个订阅功能的数据库，发布你的信息给其他客户端，享受订阅功能。又比如你有很多信息是由很多客户端同时竞争写入zk,但只允许第一个到达的写入，就可以享受zk的一致性。\n\n现在流行的分布式系统已经完全无法脱离zookeeper。列举几个简单的例子：\nstorm中用zookeeper来协调同步集群中机器的状态（并不传递消息）。基本不会有负载，对机器性能没什么要求。jstorm也是用zk来做一致性服务。\n\ndubbo中采用zookeeper来作注册中心，在阿里内部采用的是基于数据库的注册中心，我们自己用dubbo的时候采用了zookeeper，配置起来非常方便。当然如果采用redis或者自己写个简单的基于内存的注册中心也是可以的。从稳定性来讲，个人觉得zookeeper是比较好的选择，毕竟zookeeper集群中的机器只要不是半数以上宕掉，服务就是可用的。dubbo服务的消费者和提供者都需要用到zookeeper，然而消费者和提供者之间的长连接建立后，zookeeper参与程度就比较弱了(仅需要接受一些心跳包)，除非此时有新的提供者消费者加入或者离开(需要更新节点数据)。因此负载也是非常低的，基本不用考虑性能问题。zookeeper还同时起到了监控各个服务的作用。\n\nHBase要用到zk，这个已经有答主提到，就不用细说了。Hadoop也是需要用到zookeeper的，用来管理集群中的NameNode。差点忘了，Kafka集群依赖于ZooKeeper。kafka通过zookeeper实现生产者在消费端的负载均衡，动态的集群扩展等等。\n</code></pre></li>\n</ul>\n<p>resource:<br>    <a href=\"http://zookeeper.apache.org/\" target=\"_blank\" rel=\"external\">http://zookeeper.apache.org/</a><br>    <a href=\"http://www.ibm.com/developerworks/cn/opensource/os-cn-zookeeper/\" target=\"_blank\" rel=\"external\">http://www.ibm.com/developerworks/cn/opensource/os-cn-zookeeper/</a></p>\n<h1 id=\"distributed-messaging\"><a href=\"#distributed-messaging\" class=\"headerlink\" title=\"distributed messaging\"></a>distributed messaging</h1><h2 id=\"kafka\"><a href=\"#kafka\" class=\"headerlink\" title=\"kafka\"></a>kafka</h2><p>about</p>\n<pre><code>Apache Kafka是分布式发布-订阅消息系统。它最初由LinkedIn公司开发，之后成为Apache项目的一部分。Kafka是一种快速、可扩展的、设计内在就是分布式的，分区的和可复制的提交日志服务。Kafka是linkedin开源的MQ系统，主要特点是基于Pull的模式来处理消息消费，追求高吞吐量，一开始的目的就是用于日志收集和传输，0.8开始支持复制，不支持事务，适合产生大量数据的互联网服务的数据收集业务。\n\nApache Kafka与传统消息系统相比，有以下不同：\n它被设计为一个分布式系统，易于向外扩展；\n它同时为发布和订阅提供高吞吐量；\n它支持多订阅者，当失败时能自动平衡消费者；\n它将消息持久化到磁盘，因此可用于批量消费，例如ETL，以及实时应用程序。\n\nKafka 使用自己的协议。Kafka 自身服务和消费者都需要依赖 Zookeeper。RabbitMQ 在有大量消息堆积的情况下性能会下降，Kafka不会。毕竟AMQP设计的初衷不是用来持久化海量消息的，而Kafka一开始是用来处理海量日志的。\n</code></pre><p>resource：<br>    <a href=\"http://kafka.apache.org/\" target=\"_blank\" rel=\"external\">http://kafka.apache.org/</a><br>    <a href=\"https://github.com/apache/kafka\" target=\"_blank\" rel=\"external\">https://github.com/apache/kafka</a></p>\n<pre><code>http://www.infoq.com/cn/articles/apache-kafka\nhttp://www.infoq.com/cn/presentations/use-apache-kafka-to-transfer-key-business-message?utm_source=infoq&amp;utm_medium=related_content_link&amp;utm_campaign=relatedContent_articles_clk\n\nmust read:\n    https://github.com/oldratlee/translations/blob/master/log-what-every-software-engineer-should-know-about-real-time-datas-unifying/README.md\n    http://www.cnblogs.com/foreach-break/p/notes_about_distributed_system_and_The_log.html\n\nfaq：https://www.zhihu.com/question/22480085\n</code></pre><h2 id=\"rabbitmq\"><a href=\"#rabbitmq\" class=\"headerlink\" title=\"rabbitmq\"></a>rabbitmq</h2><p>RabbitMQ 支持 AMQP（二进制），STOMP（文本），MQTT（二进制），HTTP（里面包装其他协议）等协议。</p>\n<h2 id=\"rocketmq\"><a href=\"#rocketmq\" class=\"headerlink\" title=\"rocketmq\"></a>rocketmq</h2><p>about<br>    RocketMQ 是一款分布式、队列模型的消息中间件，具有以下特点：</p>\n<pre><code>能够保证严格的消息顺序\n提供丰富的消息拉取模式\n高效的订阅者水平扩展能力\n实时的消息订阅机制\n亿级消息堆积能力\nMetaq3.0 版本改名，产品名称改为RocketMQ\n\nRMQ的结构分为四个部分：生产者、消费者、nameserver、brokerserver\nnameserver：nameserver接收broker的请求注册broker路由信息。收client的请求根据某个topic获取所有到broker的路由信息。\nbrokerserver：消息的接收和推送，\n生产者：发送消息，将消息推送给brokerserver。\n消费者：接收消息，从brokerserver上获取消息。\n</code></pre><p>resource:<br>    <a href=\"https://www.aliyun.com/product/ons\" target=\"_blank\" rel=\"external\">https://www.aliyun.com/product/ons</a><br>    <a href=\"https://github.com/apache/incubator-rocketmq\" target=\"_blank\" rel=\"external\">https://github.com/apache/incubator-rocketmq</a><br>    <a href=\"https://rocketmq.incubator.apache.org/\" target=\"_blank\" rel=\"external\">https://rocketmq.incubator.apache.org/</a><br>    <a href=\"http://www.jianshu.com/p/453c6e7ff81c\" target=\"_blank\" rel=\"external\">http://www.jianshu.com/p/453c6e7ff81c</a><br>    <a href=\"http://www.jialeens.com/archives/681.html?utm_source=tuicool&amp;utm_medium=referral\" target=\"_blank\" rel=\"external\">http://www.jialeens.com/archives/681.html?utm_source=tuicool&amp;utm_medium=referral</a></p>\n<h2 id=\"zeromq\"><a href=\"#zeromq\" class=\"headerlink\" title=\"zeromq\"></a>zeromq</h2><p>ZeroMQ 和 RabbitMQ/Kafka 不同，它只是一个异步消息库，在套接字的基础上提供了类似于消息代理的机制。使用 ZeroMQ 的话，需要对自己的业务代码进行改造，不利于服务解耦。</p>\n<h1 id=\"distribute-kv\"><a href=\"#distribute-kv\" class=\"headerlink\" title=\"distribute kv\"></a>distribute kv</h1><h2 id=\"etcd\"><a href=\"#etcd\" class=\"headerlink\" title=\"etcd\"></a>etcd</h2><p>about<br>        etcd 是一个应用在分布式环境下的 key/value 存储服务。利用 etcd 的特性，应用程序可以在集群中共享信息、配置或作服务发现，etcd 会在集群的各个节点中复制这些数据并保证这些数据始终正确。etcd 无论是在 CoreOS 还是 Kubernetes 体系中都是不可或缺的一环。</p>\n<pre><code>2. 规范词汇表\netcd 0.5.0 版首次对 etcd 代码、文档及 CLI 中使用的术语进行了定义。\n\n2.1. node\n\nnode 指一个 raft 状态机实例。每个 node 都具有唯一的标识，并在处于 leader 状态时记录其它节点的步进数。\n\n2.2. member\n\nmember 指一个 etcd 实例。member 运行在每个 node 上，并向这一 node 上的其它应用程序提供服务。\n\n2.3. Cluster\n\nCluster 由多个 member 组成。每个 member 中的 node 遵循 raft 共识协议来复制日志。Cluster 接收来自 member 的提案消息，将其提交并存储于本地磁盘。\n\n2.4. Peer\n\n同一 Cluster 中的其它 member。\n\n2.5. Client\n\nClient 指调用 Cluster API 的对象。\n\n3. Raft 共识算法\netcd 集群的工作原理基于 raft 共识算法 (The Raft Consensus Algorithm)。etcd 在 0.5.0 版本中重新实现了 raft 算法，而非像之前那样依赖于第三方库 go-raft 。raft 共识算法的优点在于可以在高效的解决分布式系统中各个节点日志内容一致性问题的同时，也使得集群具备一定的容错能力。即使集群中出现部分节点故障、网络故障等问题，仍可保证其余大多数节点正确的步进。甚至当更多的节点（一般来说超过集群节点总数的一半）出现故障而导致集群不可用时，依然可以保证节点中的数据不会出现错误的结果。\n</code></pre><p><a href=\"https://github.com/coreos/etcd\" target=\"_blank\" rel=\"external\">https://github.com/coreos/etcd</a><br><a href=\"http://www.infoq.com/cn/articles/coreos-analyse-etcd/\" target=\"_blank\" rel=\"external\">http://www.infoq.com/cn/articles/coreos-analyse-etcd/</a></p>\n<h1 id=\"IOE-resource\"><a href=\"#IOE-resource\" class=\"headerlink\" title=\"IOE resource\"></a>IOE resource</h1><p>IBM doc:<br><a href=\"http://www.ibm.com/analytics/cn/zh/\" target=\"_blank\" rel=\"external\">http://www.ibm.com/analytics/cn/zh/</a></p>\n<p>Oracle doc: <a href=\"http://docs.oracle.com/en/\" target=\"_blank\" rel=\"external\">http://docs.oracle.com/en/</a><br>SAP doc: <a href=\"http://www.sap.com/developer.html\" target=\"_blank\" rel=\"external\">http://www.sap.com/developer.html</a><br>SAP case: <a href=\"http://www.bestsapchina.com/ResourceCenter/i-t-s.html\" target=\"_blank\" rel=\"external\">http://www.bestsapchina.com/ResourceCenter/i-t-s.html</a></p>\n","excerpt":"","more":"<h1 id=\"aws\"><a href=\"#aws\" class=\"headerlink\" title=\"aws\"></a>aws</h1><p><a href=\"https://github.com/donnemartin/saws\">https://github.com/donnemartin/saws</a></p>\n<h1 id=\"top-resource\"><a href=\"#top-resource\" class=\"headerlink\" title=\"top resource\"></a>top resource</h1><p>infoq: <a href=\"http://2017.qconbeijing.com/tracks\">http://2017.qconbeijing.com/tracks</a></p>\n<p><a href=\"https://www.oschina.net/p/dubbo\">https://www.oschina.net/p/dubbo</a><br><a href=\"https://github.com/alibaba/dubbo\">https://github.com/alibaba/dubbo</a></p>\n<h2 id=\"zookeeper\"><a href=\"#zookeeper\" class=\"headerlink\" title=\"zookeeper\"></a>zookeeper</h2><ul>\n<li><p>about</p>\n<pre><code>ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services. All of these kinds of services are used in some form or another by distributed applications. Each time they are implemented there is a lot of work that goes into fixing the bugs and race conditions that are inevitable. Because of the difficulty of implementing these kinds of services, applications initially usually skimp on them ,which make them brittle in the presence of change and difficult to manage. Even when done correctly, different implementations of these services lead to management complexity when the applications are deployed.\n\n考虑一下有多个服务器的分布式系统，每台服务器都负责保存数据，在数据上执行操作。这样的潜在例子包括分布式搜索引擎、分布式构建系统或者已知的系统如Apache Hadoop。所有这些分布式系统的一个常见问题是，你如何在任一时间点确定哪些服务器活着并且在工作中。最重要的是，当面对这些分布式计算的难题，例如网络失败、带宽限制、可变延迟连接、安全问题以及任何网络环境，甚至跨多个数据中心时可能发生的错误时，你如何可靠地做这些事。这些正是Apache ZooKeeper所关注的问题，它是一个快速、高可用、容错、分布式的协调服务。你可以使用ZooKeeper构建可靠的、分布式的数据结构，用于群组成员、领导人选举、协同工作流和配置服务，以及广义的分布式数据结构如锁、队列、屏障（Barrier）和锁存器（Latch）。许多知名且成功的项目依赖于ZooKeeper，其中包括HBase、Hadoop 2.0、Solr Cloud、Neo4J、Apache Blur（Incubating）和Accumulo。\n\nZooKeeper是一个分布式的、分层级的文件系统，能促进客户端间的松耦合，并提供最终一致的，类似于传统文件系统中文件和目录的Znode视图。它提供了基本的操作，例如创建、删除和检查Znode是否存在。它提供了事件驱动模型，客户端能观察特定Znode的变化，例如现有Znode增加了一个新的子节点。ZooKeeper运行多个ZooKeeper服务器，称为Ensemble，以获得高可用性。每个服务器都持有分布式文件系统的内存复本，为客户端的读取请求提供服务。\n</code></pre></li>\n<li><p>usage</p>\n<pre><code>所有分布式的协商和一致都可以利用zk实现。可以理解为一个分布式的带有订阅功能的小型元数据库。\n使用方式自然就是通过上面这句话而定。比如你需要一个订阅功能的数据库，发布你的信息给其他客户端，享受订阅功能。又比如你有很多信息是由很多客户端同时竞争写入zk,但只允许第一个到达的写入，就可以享受zk的一致性。\n\n现在流行的分布式系统已经完全无法脱离zookeeper。列举几个简单的例子：\nstorm中用zookeeper来协调同步集群中机器的状态（并不传递消息）。基本不会有负载，对机器性能没什么要求。jstorm也是用zk来做一致性服务。\n\ndubbo中采用zookeeper来作注册中心，在阿里内部采用的是基于数据库的注册中心，我们自己用dubbo的时候采用了zookeeper，配置起来非常方便。当然如果采用redis或者自己写个简单的基于内存的注册中心也是可以的。从稳定性来讲，个人觉得zookeeper是比较好的选择，毕竟zookeeper集群中的机器只要不是半数以上宕掉，服务就是可用的。dubbo服务的消费者和提供者都需要用到zookeeper，然而消费者和提供者之间的长连接建立后，zookeeper参与程度就比较弱了(仅需要接受一些心跳包)，除非此时有新的提供者消费者加入或者离开(需要更新节点数据)。因此负载也是非常低的，基本不用考虑性能问题。zookeeper还同时起到了监控各个服务的作用。\n\nHBase要用到zk，这个已经有答主提到，就不用细说了。Hadoop也是需要用到zookeeper的，用来管理集群中的NameNode。差点忘了，Kafka集群依赖于ZooKeeper。kafka通过zookeeper实现生产者在消费端的负载均衡，动态的集群扩展等等。\n</code></pre></li>\n</ul>\n<p>resource:<br>    <a href=\"http://zookeeper.apache.org/\">http://zookeeper.apache.org/</a><br>    <a href=\"http://www.ibm.com/developerworks/cn/opensource/os-cn-zookeeper/\">http://www.ibm.com/developerworks/cn/opensource/os-cn-zookeeper/</a></p>\n<h1 id=\"distributed-messaging\"><a href=\"#distributed-messaging\" class=\"headerlink\" title=\"distributed messaging\"></a>distributed messaging</h1><h2 id=\"kafka\"><a href=\"#kafka\" class=\"headerlink\" title=\"kafka\"></a>kafka</h2><p>about</p>\n<pre><code>Apache Kafka是分布式发布-订阅消息系统。它最初由LinkedIn公司开发，之后成为Apache项目的一部分。Kafka是一种快速、可扩展的、设计内在就是分布式的，分区的和可复制的提交日志服务。Kafka是linkedin开源的MQ系统，主要特点是基于Pull的模式来处理消息消费，追求高吞吐量，一开始的目的就是用于日志收集和传输，0.8开始支持复制，不支持事务，适合产生大量数据的互联网服务的数据收集业务。\n\nApache Kafka与传统消息系统相比，有以下不同：\n它被设计为一个分布式系统，易于向外扩展；\n它同时为发布和订阅提供高吞吐量；\n它支持多订阅者，当失败时能自动平衡消费者；\n它将消息持久化到磁盘，因此可用于批量消费，例如ETL，以及实时应用程序。\n\nKafka 使用自己的协议。Kafka 自身服务和消费者都需要依赖 Zookeeper。RabbitMQ 在有大量消息堆积的情况下性能会下降，Kafka不会。毕竟AMQP设计的初衷不是用来持久化海量消息的，而Kafka一开始是用来处理海量日志的。\n</code></pre><p>resource：<br>    <a href=\"http://kafka.apache.org/\">http://kafka.apache.org/</a><br>    <a href=\"https://github.com/apache/kafka\">https://github.com/apache/kafka</a></p>\n<pre><code>http://www.infoq.com/cn/articles/apache-kafka\nhttp://www.infoq.com/cn/presentations/use-apache-kafka-to-transfer-key-business-message?utm_source=infoq&amp;utm_medium=related_content_link&amp;utm_campaign=relatedContent_articles_clk\n\nmust read:\n    https://github.com/oldratlee/translations/blob/master/log-what-every-software-engineer-should-know-about-real-time-datas-unifying/README.md\n    http://www.cnblogs.com/foreach-break/p/notes_about_distributed_system_and_The_log.html\n\nfaq：https://www.zhihu.com/question/22480085\n</code></pre><h2 id=\"rabbitmq\"><a href=\"#rabbitmq\" class=\"headerlink\" title=\"rabbitmq\"></a>rabbitmq</h2><p>RabbitMQ 支持 AMQP（二进制），STOMP（文本），MQTT（二进制），HTTP（里面包装其他协议）等协议。</p>\n<h2 id=\"rocketmq\"><a href=\"#rocketmq\" class=\"headerlink\" title=\"rocketmq\"></a>rocketmq</h2><p>about<br>    RocketMQ 是一款分布式、队列模型的消息中间件，具有以下特点：</p>\n<pre><code>能够保证严格的消息顺序\n提供丰富的消息拉取模式\n高效的订阅者水平扩展能力\n实时的消息订阅机制\n亿级消息堆积能力\nMetaq3.0 版本改名，产品名称改为RocketMQ\n\nRMQ的结构分为四个部分：生产者、消费者、nameserver、brokerserver\nnameserver：nameserver接收broker的请求注册broker路由信息。收client的请求根据某个topic获取所有到broker的路由信息。\nbrokerserver：消息的接收和推送，\n生产者：发送消息，将消息推送给brokerserver。\n消费者：接收消息，从brokerserver上获取消息。\n</code></pre><p>resource:<br>    <a href=\"https://www.aliyun.com/product/ons\">https://www.aliyun.com/product/ons</a><br>    <a href=\"https://github.com/apache/incubator-rocketmq\">https://github.com/apache/incubator-rocketmq</a><br>    <a href=\"https://rocketmq.incubator.apache.org/\">https://rocketmq.incubator.apache.org/</a><br>    <a href=\"http://www.jianshu.com/p/453c6e7ff81c\">http://www.jianshu.com/p/453c6e7ff81c</a><br>    <a href=\"http://www.jialeens.com/archives/681.html?utm_source=tuicool&amp;utm_medium=referral\">http://www.jialeens.com/archives/681.html?utm_source=tuicool&amp;utm_medium=referral</a></p>\n<h2 id=\"zeromq\"><a href=\"#zeromq\" class=\"headerlink\" title=\"zeromq\"></a>zeromq</h2><p>ZeroMQ 和 RabbitMQ/Kafka 不同，它只是一个异步消息库，在套接字的基础上提供了类似于消息代理的机制。使用 ZeroMQ 的话，需要对自己的业务代码进行改造，不利于服务解耦。</p>\n<h1 id=\"distribute-kv\"><a href=\"#distribute-kv\" class=\"headerlink\" title=\"distribute kv\"></a>distribute kv</h1><h2 id=\"etcd\"><a href=\"#etcd\" class=\"headerlink\" title=\"etcd\"></a>etcd</h2><p>about<br>        etcd 是一个应用在分布式环境下的 key/value 存储服务。利用 etcd 的特性，应用程序可以在集群中共享信息、配置或作服务发现，etcd 会在集群的各个节点中复制这些数据并保证这些数据始终正确。etcd 无论是在 CoreOS 还是 Kubernetes 体系中都是不可或缺的一环。</p>\n<pre><code>2. 规范词汇表\netcd 0.5.0 版首次对 etcd 代码、文档及 CLI 中使用的术语进行了定义。\n\n2.1. node\n\nnode 指一个 raft 状态机实例。每个 node 都具有唯一的标识，并在处于 leader 状态时记录其它节点的步进数。\n\n2.2. member\n\nmember 指一个 etcd 实例。member 运行在每个 node 上，并向这一 node 上的其它应用程序提供服务。\n\n2.3. Cluster\n\nCluster 由多个 member 组成。每个 member 中的 node 遵循 raft 共识协议来复制日志。Cluster 接收来自 member 的提案消息，将其提交并存储于本地磁盘。\n\n2.4. Peer\n\n同一 Cluster 中的其它 member。\n\n2.5. Client\n\nClient 指调用 Cluster API 的对象。\n\n3. Raft 共识算法\netcd 集群的工作原理基于 raft 共识算法 (The Raft Consensus Algorithm)。etcd 在 0.5.0 版本中重新实现了 raft 算法，而非像之前那样依赖于第三方库 go-raft 。raft 共识算法的优点在于可以在高效的解决分布式系统中各个节点日志内容一致性问题的同时，也使得集群具备一定的容错能力。即使集群中出现部分节点故障、网络故障等问题，仍可保证其余大多数节点正确的步进。甚至当更多的节点（一般来说超过集群节点总数的一半）出现故障而导致集群不可用时，依然可以保证节点中的数据不会出现错误的结果。\n</code></pre><p><a href=\"https://github.com/coreos/etcd\">https://github.com/coreos/etcd</a><br><a href=\"http://www.infoq.com/cn/articles/coreos-analyse-etcd/\">http://www.infoq.com/cn/articles/coreos-analyse-etcd/</a></p>\n<h1 id=\"IOE-resource\"><a href=\"#IOE-resource\" class=\"headerlink\" title=\"IOE resource\"></a>IOE resource</h1><p>IBM doc:<br><a href=\"http://www.ibm.com/analytics/cn/zh/\">http://www.ibm.com/analytics/cn/zh/</a></p>\n<p>Oracle doc: <a href=\"http://docs.oracle.com/en/\">http://docs.oracle.com/en/</a><br>SAP doc: <a href=\"http://www.sap.com/developer.html\">http://www.sap.com/developer.html</a><br>SAP case: <a href=\"http://www.bestsapchina.com/ResourceCenter/i-t-s.html\">http://www.bestsapchina.com/ResourceCenter/i-t-s.html</a></p>\n"},{"title":"cloudfoundry core","_content":"\n# cloudfoundry core\n","source":"_posts/cloud-paas-cf-core.md","raw":"---\ntitle: cloudfoundry core\ncategories:\n- paas\ntags:\n- core\n- cloudfoundry\n---\n\n# cloudfoundry core\n","slug":"cloud-paas-cf-core","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-04-18T13:15:33.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rb20006kxsvdyglatf6","content":"<h1 id=\"cloudfoundry-core\"><a href=\"#cloudfoundry-core\" class=\"headerlink\" title=\"cloudfoundry core\"></a>cloudfoundry core</h1>","excerpt":"","more":"<h1 id=\"cloudfoundry-core\"><a href=\"#cloudfoundry-core\" class=\"headerlink\" title=\"cloudfoundry core\"></a>cloudfoundry core</h1>"},{"title":"blockchain portal","_content":"\n# blockchain\n\nzhihu: https://www.zhihu.com/question/37290469\nIBM: https://github.com/IBM-Blockchain\n101:\n1. https://www.ibm.com/developerworks/cloud/library/cl-ibm-blockchain-101-quick-start-guide-for-developers-bluemix-trs/index.html\n2. http://blog.codinglabs.org/articles/bitcoin-mechanism-make-easy.html\n3. http://blockchaindev.org/article/introduce_blockchain.html\n\n# coin:\nhttp://www.laiteb.com/\nhttp://ethfans.org/\n\n\n# project\nhttps://github.com/IBM-Blockchain/fabric-images\n","source":"_posts/blockchain-portal.md","raw":"---\ntitle: blockchain portal\ncategories:\n- blockchain\ntags:\n- portal\n- blockchain\n---\n\n# blockchain\n\nzhihu: https://www.zhihu.com/question/37290469\nIBM: https://github.com/IBM-Blockchain\n101:\n1. https://www.ibm.com/developerworks/cloud/library/cl-ibm-blockchain-101-quick-start-guide-for-developers-bluemix-trs/index.html\n2. http://blog.codinglabs.org/articles/bitcoin-mechanism-make-easy.html\n3. http://blockchaindev.org/article/introduce_blockchain.html\n\n# coin:\nhttp://www.laiteb.com/\nhttp://ethfans.org/\n\n\n# project\nhttps://github.com/IBM-Blockchain/fabric-images\n","slug":"blockchain-portal","published":1,"date":"2017-04-02T00:50:53.000Z","updated":"2017-04-03T02:46:03.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rb80009kxsv7rqkofmc","content":"<h1 id=\"blockchain\"><a href=\"#blockchain\" class=\"headerlink\" title=\"blockchain\"></a>blockchain</h1><p>zhihu: <a href=\"https://www.zhihu.com/question/37290469\" target=\"_blank\" rel=\"external\">https://www.zhihu.com/question/37290469</a><br>IBM: <a href=\"https://github.com/IBM-Blockchain\" target=\"_blank\" rel=\"external\">https://github.com/IBM-Blockchain</a><br>101:</p>\n<ol>\n<li><a href=\"https://www.ibm.com/developerworks/cloud/library/cl-ibm-blockchain-101-quick-start-guide-for-developers-bluemix-trs/index.html\" target=\"_blank\" rel=\"external\">https://www.ibm.com/developerworks/cloud/library/cl-ibm-blockchain-101-quick-start-guide-for-developers-bluemix-trs/index.html</a></li>\n<li><a href=\"http://blog.codinglabs.org/articles/bitcoin-mechanism-make-easy.html\" target=\"_blank\" rel=\"external\">http://blog.codinglabs.org/articles/bitcoin-mechanism-make-easy.html</a></li>\n<li><a href=\"http://blockchaindev.org/article/introduce_blockchain.html\" target=\"_blank\" rel=\"external\">http://blockchaindev.org/article/introduce_blockchain.html</a></li>\n</ol>\n<h1 id=\"coin\"><a href=\"#coin\" class=\"headerlink\" title=\"coin:\"></a>coin:</h1><p><a href=\"http://www.laiteb.com/\" target=\"_blank\" rel=\"external\">http://www.laiteb.com/</a><br><a href=\"http://ethfans.org/\" target=\"_blank\" rel=\"external\">http://ethfans.org/</a></p>\n<h1 id=\"project\"><a href=\"#project\" class=\"headerlink\" title=\"project\"></a>project</h1><p><a href=\"https://github.com/IBM-Blockchain/fabric-images\" target=\"_blank\" rel=\"external\">https://github.com/IBM-Blockchain/fabric-images</a></p>\n","excerpt":"","more":"<h1 id=\"blockchain\"><a href=\"#blockchain\" class=\"headerlink\" title=\"blockchain\"></a>blockchain</h1><p>zhihu: <a href=\"https://www.zhihu.com/question/37290469\">https://www.zhihu.com/question/37290469</a><br>IBM: <a href=\"https://github.com/IBM-Blockchain\">https://github.com/IBM-Blockchain</a><br>101:</p>\n<ol>\n<li><a href=\"https://www.ibm.com/developerworks/cloud/library/cl-ibm-blockchain-101-quick-start-guide-for-developers-bluemix-trs/index.html\">https://www.ibm.com/developerworks/cloud/library/cl-ibm-blockchain-101-quick-start-guide-for-developers-bluemix-trs/index.html</a></li>\n<li><a href=\"http://blog.codinglabs.org/articles/bitcoin-mechanism-make-easy.html\">http://blog.codinglabs.org/articles/bitcoin-mechanism-make-easy.html</a></li>\n<li><a href=\"http://blockchaindev.org/article/introduce_blockchain.html\">http://blockchaindev.org/article/introduce_blockchain.html</a></li>\n</ol>\n<h1 id=\"coin\"><a href=\"#coin\" class=\"headerlink\" title=\"coin:\"></a>coin:</h1><p><a href=\"http://www.laiteb.com/\">http://www.laiteb.com/</a><br><a href=\"http://ethfans.org/\">http://ethfans.org/</a></p>\n<h1 id=\"project\"><a href=\"#project\" class=\"headerlink\" title=\"project\"></a>project</h1><p><a href=\"https://github.com/IBM-Blockchain/fabric-images\">https://github.com/IBM-Blockchain/fabric-images</a></p>\n"},{"title":"openstack core","_content":"\n# openstack core\n","source":"_posts/cloud-iaas-openstack-core.md","raw":"---\ntitle: openstack core\ncategories:\n- iaas\ntags:\n- core\n- openstack\n---\n\n# openstack core\n","slug":"cloud-iaas-openstack-core","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-04-18T13:14:45.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rbc000bkxsva4a6xg82","content":"<h1 id=\"openstack-core\"><a href=\"#openstack-core\" class=\"headerlink\" title=\"openstack core\"></a>openstack core</h1>","excerpt":"","more":"<h1 id=\"openstack-core\"><a href=\"#openstack-core\" class=\"headerlink\" title=\"openstack core\"></a>openstack core</h1>"},{"title":"paas portal","_content":"\n# paas portal\n","source":"_posts/cloud-paas-portal.md","raw":"---\ntitle: paas portal\ncategories:\n- paas\ntags:\n- portal\n---\n\n# paas portal\n","slug":"cloud-paas-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-04-18T13:15:43.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rbi000ekxsvfqpayfu4","content":"<h1 id=\"paas-portal\"><a href=\"#paas-portal\" class=\"headerlink\" title=\"paas portal\"></a>paas portal</h1>","excerpt":"","more":"<h1 id=\"paas-portal\"><a href=\"#paas-portal\" class=\"headerlink\" title=\"paas portal\"></a>paas portal</h1>"},{"title":"openshift core","_content":"\n# openshift core\n","source":"_posts/cloud-paas-openshift-core.md","raw":"---\ntitle: openshift core\ncategories:\n- paas\ntags:\n- core\n- openshift\n---\n\n# openshift core\n","slug":"cloud-paas-openshift-core","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-04-18T13:15:37.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rbm000hkxsvo27rppn1","content":"<h1 id=\"openshift-core\"><a href=\"#openshift-core\" class=\"headerlink\" title=\"openshift core\"></a>openshift core</h1>","excerpt":"","more":"<h1 id=\"openshift-core\"><a href=\"#openshift-core\" class=\"headerlink\" title=\"openshift core\"></a>openshift core</h1>"},{"title":"saas portal","_content":"\n# saas portal\n","source":"_posts/cloud-saas-portal.md","raw":"---\ntitle: saas portal\ncategories:\n- saas\ntags:\n- portal\n---\n\n# saas portal\n","slug":"cloud-saas-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-04-18T13:16:24.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rbp000lkxsvy9z61t50","content":"<h1 id=\"saas-portal\"><a href=\"#saas-portal\" class=\"headerlink\" title=\"saas portal\"></a>saas portal</h1>","excerpt":"","more":"<h1 id=\"saas-portal\"><a href=\"#saas-portal\" class=\"headerlink\" title=\"saas portal\"></a>saas portal</h1>"},{"title":"community","_content":"\n# awesome\nhttps://github.com/sindresorhus/awesome\n\n# info\nhttps://hackernoon.com/\n\n# coding\nhttp://blog.forecho.com/\n\n# guide\nhttp://manishearth.github.io/blog/2016/01/03/making-your-open-source-project-newcomer-friendly/\n","source":"_posts/community.md","raw":"---\ntitle: community\ncategories:\n- community\ntags:\n- opensource\n---\n\n# awesome\nhttps://github.com/sindresorhus/awesome\n\n# info\nhttps://hackernoon.com/\n\n# coding\nhttp://blog.forecho.com/\n\n# guide\nhttp://manishearth.github.io/blog/2016/01/03/making-your-open-source-project-newcomer-friendly/\n","slug":"community","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-04-18T13:16:29.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rbs000okxsvifi7yrjs","content":"<h1 id=\"awesome\"><a href=\"#awesome\" class=\"headerlink\" title=\"awesome\"></a>awesome</h1><p><a href=\"https://github.com/sindresorhus/awesome\" target=\"_blank\" rel=\"external\">https://github.com/sindresorhus/awesome</a></p>\n<h1 id=\"info\"><a href=\"#info\" class=\"headerlink\" title=\"info\"></a>info</h1><p><a href=\"https://hackernoon.com/\" target=\"_blank\" rel=\"external\">https://hackernoon.com/</a></p>\n<h1 id=\"coding\"><a href=\"#coding\" class=\"headerlink\" title=\"coding\"></a>coding</h1><p><a href=\"http://blog.forecho.com/\" target=\"_blank\" rel=\"external\">http://blog.forecho.com/</a></p>\n<h1 id=\"guide\"><a href=\"#guide\" class=\"headerlink\" title=\"guide\"></a>guide</h1><p><a href=\"http://manishearth.github.io/blog/2016/01/03/making-your-open-source-project-newcomer-friendly/\" target=\"_blank\" rel=\"external\">http://manishearth.github.io/blog/2016/01/03/making-your-open-source-project-newcomer-friendly/</a></p>\n","excerpt":"","more":"<h1 id=\"awesome\"><a href=\"#awesome\" class=\"headerlink\" title=\"awesome\"></a>awesome</h1><p><a href=\"https://github.com/sindresorhus/awesome\">https://github.com/sindresorhus/awesome</a></p>\n<h1 id=\"info\"><a href=\"#info\" class=\"headerlink\" title=\"info\"></a>info</h1><p><a href=\"https://hackernoon.com/\">https://hackernoon.com/</a></p>\n<h1 id=\"coding\"><a href=\"#coding\" class=\"headerlink\" title=\"coding\"></a>coding</h1><p><a href=\"http://blog.forecho.com/\">http://blog.forecho.com/</a></p>\n<h1 id=\"guide\"><a href=\"#guide\" class=\"headerlink\" title=\"guide\"></a>guide</h1><p><a href=\"http://manishearth.github.io/blog/2016/01/03/making-your-open-source-project-newcomer-friendly/\">http://manishearth.github.io/blog/2016/01/03/making-your-open-source-project-newcomer-friendly/</a></p>\n"},{"title":"python portal","_content":"\n# tensorflow\nhttps://www.tensorflow.org/\nhttp://www.tensorfly.cn/\n\n# kylin\nhttp://www.infoq.com/cn/dsghzfdhs\nhttp://kylin.apache.org/cn/\nhttps://www.amazon.cn/%E5%9F%BA%E4%BA%8EApache-Kylin%E6%9E%84%E5%BB%BA%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B9%B3%E5%8F%B0-%E8%92%8B%E5%AE%88%E5%A3%AE/dp/B01N9GO1XC/ref=cm_cr_othr_d_product_top?ie=UTF8\n","source":"_posts/bigdata--portal.md","raw":"---\ntitle: python portal\ncategories:\n- python\ntags:\n- portal\n---\n\n# tensorflow\nhttps://www.tensorflow.org/\nhttp://www.tensorfly.cn/\n\n# kylin\nhttp://www.infoq.com/cn/dsghzfdhs\nhttp://kylin.apache.org/cn/\nhttps://www.amazon.cn/%E5%9F%BA%E4%BA%8EApache-Kylin%E6%9E%84%E5%BB%BA%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B9%B3%E5%8F%B0-%E8%92%8B%E5%AE%88%E5%A3%AE/dp/B01N9GO1XC/ref=cm_cr_othr_d_product_top?ie=UTF8\n","slug":"bigdata--portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-02-09T12:47:15.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rbx000skxsv1p7dq3zk","content":"<h1 id=\"tensorflow\"><a href=\"#tensorflow\" class=\"headerlink\" title=\"tensorflow\"></a>tensorflow</h1><p><a href=\"https://www.tensorflow.org/\" target=\"_blank\" rel=\"external\">https://www.tensorflow.org/</a><br><a href=\"http://www.tensorfly.cn/\" target=\"_blank\" rel=\"external\">http://www.tensorfly.cn/</a></p>\n<h1 id=\"kylin\"><a href=\"#kylin\" class=\"headerlink\" title=\"kylin\"></a>kylin</h1><p><a href=\"http://www.infoq.com/cn/dsghzfdhs\" target=\"_blank\" rel=\"external\">http://www.infoq.com/cn/dsghzfdhs</a><br><a href=\"http://kylin.apache.org/cn/\" target=\"_blank\" rel=\"external\">http://kylin.apache.org/cn/</a><br><a href=\"https://www.amazon.cn/%E5%9F%BA%E4%BA%8EApache-Kylin%E6%9E%84%E5%BB%BA%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B9%B3%E5%8F%B0-%E8%92%8B%E5%AE%88%E5%A3%AE/dp/B01N9GO1XC/ref=cm_cr_othr_d_product_top?ie=UTF8\" target=\"_blank\" rel=\"external\">https://www.amazon.cn/%E5%9F%BA%E4%BA%8EApache-Kylin%E6%9E%84%E5%BB%BA%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B9%B3%E5%8F%B0-%E8%92%8B%E5%AE%88%E5%A3%AE/dp/B01N9GO1XC/ref=cm_cr_othr_d_product_top?ie=UTF8</a></p>\n","excerpt":"","more":"<h1 id=\"tensorflow\"><a href=\"#tensorflow\" class=\"headerlink\" title=\"tensorflow\"></a>tensorflow</h1><p><a href=\"https://www.tensorflow.org/\">https://www.tensorflow.org/</a><br><a href=\"http://www.tensorfly.cn/\">http://www.tensorfly.cn/</a></p>\n<h1 id=\"kylin\"><a href=\"#kylin\" class=\"headerlink\" title=\"kylin\"></a>kylin</h1><p><a href=\"http://www.infoq.com/cn/dsghzfdhs\">http://www.infoq.com/cn/dsghzfdhs</a><br><a href=\"http://kylin.apache.org/cn/\">http://kylin.apache.org/cn/</a><br><a href=\"https://www.amazon.cn/%E5%9F%BA%E4%BA%8EApache-Kylin%E6%9E%84%E5%BB%BA%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B9%B3%E5%8F%B0-%E8%92%8B%E5%AE%88%E5%A3%AE/dp/B01N9GO1XC/ref=cm_cr_othr_d_product_top?ie=UTF8\">https://www.amazon.cn/%E5%9F%BA%E4%BA%8EApache-Kylin%E6%9E%84%E5%BB%BA%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B9%B3%E5%8F%B0-%E8%92%8B%E5%AE%88%E5%A3%AE/dp/B01N9GO1XC/ref=cm_cr_othr_d_product_top?ie=UTF8</a></p>\n"},{"title":"iaas core","_content":"\n# iaas core\n\n# iaas tool\n\ncloudinit\nhttps://cloudinit.readthedocs.io/en/latest/index.html","source":"_posts/cloud-iaas-portal.md","raw":"---\ntitle: iaas core\ncategories:\n- iaas\ntags:\n- portal\n---\n\n# iaas core\n\n# iaas tool\n\ncloudinit\nhttps://cloudinit.readthedocs.io/en/latest/index.html","slug":"cloud-iaas-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-02-18T05:51:25.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rbz000ukxsvyvdar8pr","content":"<h1 id=\"iaas-core\"><a href=\"#iaas-core\" class=\"headerlink\" title=\"iaas core\"></a>iaas core</h1><h1 id=\"iaas-tool\"><a href=\"#iaas-tool\" class=\"headerlink\" title=\"iaas tool\"></a>iaas tool</h1><p>cloudinit<br><a href=\"https://cloudinit.readthedocs.io/en/latest/index.html\" target=\"_blank\" rel=\"external\">https://cloudinit.readthedocs.io/en/latest/index.html</a></p>\n","excerpt":"","more":"<h1 id=\"iaas-core\"><a href=\"#iaas-core\" class=\"headerlink\" title=\"iaas core\"></a>iaas core</h1><h1 id=\"iaas-tool\"><a href=\"#iaas-tool\" class=\"headerlink\" title=\"iaas tool\"></a>iaas tool</h1><p>cloudinit<br><a href=\"https://cloudinit.readthedocs.io/en/latest/index.html\">https://cloudinit.readthedocs.io/en/latest/index.html</a></p>\n"},{"title":"java portal","_content":"\n#  portal\n\nyugong\nhttps://github.com/alibaba/yugong\n\nhttps://github.com/alibaba/AliSQL","source":"_posts/db--portal.md","raw":"---\ntitle: java portal\ncategories:\n- java\ntags:\n- portal\n---\n\n#  portal\n\nyugong\nhttps://github.com/alibaba/yugong\n\nhttps://github.com/alibaba/AliSQL","slug":"db--portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-02-04T08:55:47.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rc4000zkxsvpuw1mnfs","content":"<h1 id=\"portal\"><a href=\"#portal\" class=\"headerlink\" title=\"portal\"></a>portal</h1><p>yugong<br><a href=\"https://github.com/alibaba/yugong\" target=\"_blank\" rel=\"external\">https://github.com/alibaba/yugong</a></p>\n<p><a href=\"https://github.com/alibaba/AliSQL\" target=\"_blank\" rel=\"external\">https://github.com/alibaba/AliSQL</a></p>\n","excerpt":"","more":"<h1 id=\"portal\"><a href=\"#portal\" class=\"headerlink\" title=\"portal\"></a>portal</h1><p>yugong<br><a href=\"https://github.com/alibaba/yugong\">https://github.com/alibaba/yugong</a></p>\n<p><a href=\"https://github.com/alibaba/AliSQL\">https://github.com/alibaba/AliSQL</a></p>\n"},{"title":"mysql portal","_content":"\n# mysql portal\n","source":"_posts/db-mysql-portal.md","raw":"---\ntitle: mysql portal\ncategories:\n- mysql\ntags:\n- portal\n---\n\n# mysql portal\n","slug":"db-mysql-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-04-18T13:16:53.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rcb0011kxsvfqrdlehk","content":"<h1 id=\"mysql-portal\"><a href=\"#mysql-portal\" class=\"headerlink\" title=\"mysql portal\"></a>mysql portal</h1>","excerpt":"","more":"<h1 id=\"mysql-portal\"><a href=\"#mysql-portal\" class=\"headerlink\" title=\"mysql portal\"></a>mysql portal</h1>"},{"title":"nosql portal","_content":"\n# nosql portal\n","source":"_posts/db-nosql-portal.md","raw":"---\ntitle: nosql portal\ncategories:\n- nosql\ntags:\n- portal\n---\n\n# nosql portal\n","slug":"db-nosql-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-04-18T13:17:02.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rce0016kxsv4dkav496","content":"<h1 id=\"nosql-portal\"><a href=\"#nosql-portal\" class=\"headerlink\" title=\"nosql portal\"></a>nosql portal</h1>","excerpt":"","more":"<h1 id=\"nosql-portal\"><a href=\"#nosql-portal\" class=\"headerlink\" title=\"nosql portal\"></a>nosql portal</h1>"},{"title":"devops portal","_content":"\n# devops portal\n\n# awesome\nhttps://github.com/AcalephStorage/awesome-devops\nhttps://github.com/joubertredrat/awesome-devops\n","source":"_posts/devops--portal.md","raw":"---\ntitle: devops portal\ncategories:\n- devops\ntags:\n- portal\n---\n\n# devops portal\n\n# awesome\nhttps://github.com/AcalephStorage/awesome-devops\nhttps://github.com/joubertredrat/awesome-devops\n","slug":"devops--portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-04-18T13:17:10.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rci0018kxsvdfkwhe9j","content":"<h1 id=\"devops-portal\"><a href=\"#devops-portal\" class=\"headerlink\" title=\"devops portal\"></a>devops portal</h1><h1 id=\"awesome\"><a href=\"#awesome\" class=\"headerlink\" title=\"awesome\"></a>awesome</h1><p><a href=\"https://github.com/AcalephStorage/awesome-devops\" target=\"_blank\" rel=\"external\">https://github.com/AcalephStorage/awesome-devops</a><br><a href=\"https://github.com/joubertredrat/awesome-devops\" target=\"_blank\" rel=\"external\">https://github.com/joubertredrat/awesome-devops</a></p>\n","excerpt":"","more":"<h1 id=\"devops-portal\"><a href=\"#devops-portal\" class=\"headerlink\" title=\"devops portal\"></a>devops portal</h1><h1 id=\"awesome\"><a href=\"#awesome\" class=\"headerlink\" title=\"awesome\"></a>awesome</h1><p><a href=\"https://github.com/AcalephStorage/awesome-devops\">https://github.com/AcalephStorage/awesome-devops</a><br><a href=\"https://github.com/joubertredrat/awesome-devops\">https://github.com/joubertredrat/awesome-devops</a></p>\n"},{"title":"docker compose","_content":"\n# about\n\ncode: https://github.com/docker/compose\ndoc: https://docs.docker.com/compose/\nin production: https://docs.docker.com/compose/production/\n\norchestration 官方编排工具, 用于将一个多容器应用编排成一个单一应用\nFig工具的替代品: [fig](http://www.fig.sh/)可以快速搭建开发环境,通过YAML文件管理多个容器;\nFit cmd: add fig.yml; fig up\n\n# install\n\n[link](http://docs.docker.com/compose/install/)\n            \n        docker-compose --version\n\n    64bits Linux or MacOS X:\n\n            curl -L https://github.com/docker/compose/releases/download/1.1.0/docker-compose-`uname -s`-`uname -m` > /usr/local/bin/docker-compose\n            chmod +x /usr/local/bin/docker-compose\n    \n    win and other:\n\n            sudo pip install -U docker-compose\n# command\n\n- docker-compose up -d\n\n- docker exec\n    \n    docker exec -it example_web_1 bash\n\n- docker-compose stop && docker-compose rm --force\n\n- docker-compose build\n    Build or rebuild services\n\n- docker-compose help\n\n- docker-compose kill\n    Kill containers 通过发送 SIGKILL 信号来强制停止服务容器\n    支持通过参数来指定发送的信号\n        docker-compose kill -s SIGINT\n\n- docker-compose logs\n    View output from containers\n\n- docker-compose port\n    Print the public port for a port binding\n\n- docker-compose ps\n    List containers\n\n- docker-compose pull\n    Pulls service images\n\n- docker-compose rm\n    Remove stopped containers\n\n- docker-compose run\n\n    Run a one-off command 在一个服务上执行一个命令\n    docker-compose run ubuntu ping docker.com\n        将会启动一个 ubuntu 服务，执行 ping docker.com 命令\n    默认情况下，所有关联的服务将会自动被启动，除非这些服务已经在运行中。\n    该命令类似启动容器后运行指定的命令，相关卷、链接等等都将会按照期望创建。\n    两个不同点：\n        给定命令将会覆盖原有的自动运行命令；\n        不会自动创建端口，以避免冲突。\n    如果不希望自动启动关联的容器，可以使用 --no-deps 选项\n        docker-compose run --no-deps web python manage.py shell\n        将不会启动 web 容器所关联的其它容器\n\n- docker-compose scale\n\n    Set number of containers for a service 设置同一个服务运行的容器个数\n    service=num\n    docker-compose scale web=2 worker=3\n\n- docker-compose start: Start services\n\n- docker-compose stop: Stop services\n\n- docker-compose restart\n\n    Restart services\n        env variable\n\n            COMPOSE_PROJECT_NAME\n                设置通过 Compose 启动的每一个容器前添加的项目名称，默认是当前工作目录的名字。\n            COMPOSE_FILE\n                设置要使用的 docker-compose.yml 的路径。默认路径是当前工作目录。\n            DOCKER_HOST\n                设置 Docker daemon 的地址。默认使用 unix:///var/run/docker.sock，与 Docker 客户端采用的默认值一致。\n            DOCKER_TLS_VERIFY\n                如果设置不为空，则与 Docker daemon 交互通过 TLS 进行。\n            DOCKER_CERT_PATH\n                配置 TLS 通信所需要的验证（ca.pem、cert.pem 和 key.pem）文件的路径，默认是 ~/.docker 。\n\n- docker-compose up\n\n    Create and start containers\n        $ docker-compose up -d\n\n- docker-compose logs\n\n- docker-compose version\n\n    Show the Docker-Compose version information\n\n- docker-compose unpause\n\n    Unpause services\n\n- docker-compose migrate-to-labels\n\n    Recreate containers to add labels\n   ","source":"_posts/docker-compose-detail.md","raw":"---\ntitle: docker compose\ncategories:\n- docker\ntags:\n- detail\n---\n\n# about\n\ncode: https://github.com/docker/compose\ndoc: https://docs.docker.com/compose/\nin production: https://docs.docker.com/compose/production/\n\norchestration 官方编排工具, 用于将一个多容器应用编排成一个单一应用\nFig工具的替代品: [fig](http://www.fig.sh/)可以快速搭建开发环境,通过YAML文件管理多个容器;\nFit cmd: add fig.yml; fig up\n\n# install\n\n[link](http://docs.docker.com/compose/install/)\n            \n        docker-compose --version\n\n    64bits Linux or MacOS X:\n\n            curl -L https://github.com/docker/compose/releases/download/1.1.0/docker-compose-`uname -s`-`uname -m` > /usr/local/bin/docker-compose\n            chmod +x /usr/local/bin/docker-compose\n    \n    win and other:\n\n            sudo pip install -U docker-compose\n# command\n\n- docker-compose up -d\n\n- docker exec\n    \n    docker exec -it example_web_1 bash\n\n- docker-compose stop && docker-compose rm --force\n\n- docker-compose build\n    Build or rebuild services\n\n- docker-compose help\n\n- docker-compose kill\n    Kill containers 通过发送 SIGKILL 信号来强制停止服务容器\n    支持通过参数来指定发送的信号\n        docker-compose kill -s SIGINT\n\n- docker-compose logs\n    View output from containers\n\n- docker-compose port\n    Print the public port for a port binding\n\n- docker-compose ps\n    List containers\n\n- docker-compose pull\n    Pulls service images\n\n- docker-compose rm\n    Remove stopped containers\n\n- docker-compose run\n\n    Run a one-off command 在一个服务上执行一个命令\n    docker-compose run ubuntu ping docker.com\n        将会启动一个 ubuntu 服务，执行 ping docker.com 命令\n    默认情况下，所有关联的服务将会自动被启动，除非这些服务已经在运行中。\n    该命令类似启动容器后运行指定的命令，相关卷、链接等等都将会按照期望创建。\n    两个不同点：\n        给定命令将会覆盖原有的自动运行命令；\n        不会自动创建端口，以避免冲突。\n    如果不希望自动启动关联的容器，可以使用 --no-deps 选项\n        docker-compose run --no-deps web python manage.py shell\n        将不会启动 web 容器所关联的其它容器\n\n- docker-compose scale\n\n    Set number of containers for a service 设置同一个服务运行的容器个数\n    service=num\n    docker-compose scale web=2 worker=3\n\n- docker-compose start: Start services\n\n- docker-compose stop: Stop services\n\n- docker-compose restart\n\n    Restart services\n        env variable\n\n            COMPOSE_PROJECT_NAME\n                设置通过 Compose 启动的每一个容器前添加的项目名称，默认是当前工作目录的名字。\n            COMPOSE_FILE\n                设置要使用的 docker-compose.yml 的路径。默认路径是当前工作目录。\n            DOCKER_HOST\n                设置 Docker daemon 的地址。默认使用 unix:///var/run/docker.sock，与 Docker 客户端采用的默认值一致。\n            DOCKER_TLS_VERIFY\n                如果设置不为空，则与 Docker daemon 交互通过 TLS 进行。\n            DOCKER_CERT_PATH\n                配置 TLS 通信所需要的验证（ca.pem、cert.pem 和 key.pem）文件的路径，默认是 ~/.docker 。\n\n- docker-compose up\n\n    Create and start containers\n        $ docker-compose up -d\n\n- docker-compose logs\n\n- docker-compose version\n\n    Show the Docker-Compose version information\n\n- docker-compose unpause\n\n    Unpause services\n\n- docker-compose migrate-to-labels\n\n    Recreate containers to add labels\n   ","slug":"docker-compose-detail","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-02-10T15:03:07.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rcl001bkxsvrmvblr5e","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><p>code: <a href=\"https://github.com/docker/compose\" target=\"_blank\" rel=\"external\">https://github.com/docker/compose</a><br>doc: <a href=\"https://docs.docker.com/compose/\" target=\"_blank\" rel=\"external\">https://docs.docker.com/compose/</a><br>in production: <a href=\"https://docs.docker.com/compose/production/\" target=\"_blank\" rel=\"external\">https://docs.docker.com/compose/production/</a></p>\n<p>orchestration 官方编排工具, 用于将一个多容器应用编排成一个单一应用<br>Fig工具的替代品: <a href=\"http://www.fig.sh/\" target=\"_blank\" rel=\"external\">fig</a>可以快速搭建开发环境,通过YAML文件管理多个容器;<br>Fit cmd: add fig.yml; fig up</p>\n<h1 id=\"install\"><a href=\"#install\" class=\"headerlink\" title=\"install\"></a>install</h1><p><a href=\"http://docs.docker.com/compose/install/\" target=\"_blank\" rel=\"external\">link</a></p>\n<pre><code>    docker-compose --version\n\n64bits Linux or MacOS X:\n\n        curl -L https://github.com/docker/compose/releases/download/1.1.0/docker-compose-`uname -s`-`uname -m` &gt; /usr/local/bin/docker-compose\n        chmod +x /usr/local/bin/docker-compose\n\nwin and other:\n\n        sudo pip install -U docker-compose\n</code></pre><h1 id=\"command\"><a href=\"#command\" class=\"headerlink\" title=\"command\"></a>command</h1><ul>\n<li><p>docker-compose up -d</p>\n</li>\n<li><p>docker exec</p>\n<p>  docker exec -it example_web_1 bash</p>\n</li>\n<li><p>docker-compose stop &amp;&amp; docker-compose rm –force</p>\n</li>\n<li><p>docker-compose build<br>  Build or rebuild services</p>\n</li>\n<li><p>docker-compose help</p>\n</li>\n<li><p>docker-compose kill<br>  Kill containers 通过发送 SIGKILL 信号来强制停止服务容器<br>  支持通过参数来指定发送的信号</p>\n<pre><code>docker-compose kill -s SIGINT\n</code></pre></li>\n<li><p>docker-compose logs<br>  View output from containers</p>\n</li>\n<li><p>docker-compose port<br>  Print the public port for a port binding</p>\n</li>\n<li><p>docker-compose ps<br>  List containers</p>\n</li>\n<li><p>docker-compose pull<br>  Pulls service images</p>\n</li>\n<li><p>docker-compose rm<br>  Remove stopped containers</p>\n</li>\n<li><p>docker-compose run</p>\n<p>  Run a one-off command 在一个服务上执行一个命令<br>  docker-compose run ubuntu ping docker.com</p>\n<pre><code>将会启动一个 ubuntu 服务，执行 ping docker.com 命令\n</code></pre><p>  默认情况下，所有关联的服务将会自动被启动，除非这些服务已经在运行中。<br>  该命令类似启动容器后运行指定的命令，相关卷、链接等等都将会按照期望创建。<br>  两个不同点：</p>\n<pre><code>给定命令将会覆盖原有的自动运行命令；\n不会自动创建端口，以避免冲突。\n</code></pre><p>  如果不希望自动启动关联的容器，可以使用 –no-deps 选项</p>\n<pre><code>docker-compose run --no-deps web python manage.py shell\n将不会启动 web 容器所关联的其它容器\n</code></pre></li>\n<li><p>docker-compose scale</p>\n<p>  Set number of containers for a service 设置同一个服务运行的容器个数<br>  service=num<br>  docker-compose scale web=2 worker=3</p>\n</li>\n<li><p>docker-compose start: Start services</p>\n</li>\n<li><p>docker-compose stop: Stop services</p>\n</li>\n<li><p>docker-compose restart</p>\n<p>  Restart services</p>\n<pre><code>env variable\n\n    COMPOSE_PROJECT_NAME\n        设置通过 Compose 启动的每一个容器前添加的项目名称，默认是当前工作目录的名字。\n    COMPOSE_FILE\n        设置要使用的 docker-compose.yml 的路径。默认路径是当前工作目录。\n    DOCKER_HOST\n        设置 Docker daemon 的地址。默认使用 unix:///var/run/docker.sock，与 Docker 客户端采用的默认值一致。\n    DOCKER_TLS_VERIFY\n        如果设置不为空，则与 Docker daemon 交互通过 TLS 进行。\n    DOCKER_CERT_PATH\n        配置 TLS 通信所需要的验证（ca.pem、cert.pem 和 key.pem）文件的路径，默认是 ~/.docker 。\n</code></pre></li>\n<li><p>docker-compose up</p>\n<p>  Create and start containers</p>\n<pre><code>$ docker-compose up -d\n</code></pre></li>\n<li><p>docker-compose logs</p>\n</li>\n<li><p>docker-compose version</p>\n<p>  Show the Docker-Compose version information</p>\n</li>\n<li><p>docker-compose unpause</p>\n<p>  Unpause services</p>\n</li>\n<li><p>docker-compose migrate-to-labels</p>\n<p>  Recreate containers to add labels</p>\n</li>\n</ul>\n","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><p>code: <a href=\"https://github.com/docker/compose\">https://github.com/docker/compose</a><br>doc: <a href=\"https://docs.docker.com/compose/\">https://docs.docker.com/compose/</a><br>in production: <a href=\"https://docs.docker.com/compose/production/\">https://docs.docker.com/compose/production/</a></p>\n<p>orchestration 官方编排工具, 用于将一个多容器应用编排成一个单一应用<br>Fig工具的替代品: <a href=\"http://www.fig.sh/\">fig</a>可以快速搭建开发环境,通过YAML文件管理多个容器;<br>Fit cmd: add fig.yml; fig up</p>\n<h1 id=\"install\"><a href=\"#install\" class=\"headerlink\" title=\"install\"></a>install</h1><p><a href=\"http://docs.docker.com/compose/install/\">link</a></p>\n<pre><code>    docker-compose --version\n\n64bits Linux or MacOS X:\n\n        curl -L https://github.com/docker/compose/releases/download/1.1.0/docker-compose-`uname -s`-`uname -m` &gt; /usr/local/bin/docker-compose\n        chmod +x /usr/local/bin/docker-compose\n\nwin and other:\n\n        sudo pip install -U docker-compose\n</code></pre><h1 id=\"command\"><a href=\"#command\" class=\"headerlink\" title=\"command\"></a>command</h1><ul>\n<li><p>docker-compose up -d</p>\n</li>\n<li><p>docker exec</p>\n<p>  docker exec -it example_web_1 bash</p>\n</li>\n<li><p>docker-compose stop &amp;&amp; docker-compose rm –force</p>\n</li>\n<li><p>docker-compose build<br>  Build or rebuild services</p>\n</li>\n<li><p>docker-compose help</p>\n</li>\n<li><p>docker-compose kill<br>  Kill containers 通过发送 SIGKILL 信号来强制停止服务容器<br>  支持通过参数来指定发送的信号</p>\n<pre><code>docker-compose kill -s SIGINT\n</code></pre></li>\n<li><p>docker-compose logs<br>  View output from containers</p>\n</li>\n<li><p>docker-compose port<br>  Print the public port for a port binding</p>\n</li>\n<li><p>docker-compose ps<br>  List containers</p>\n</li>\n<li><p>docker-compose pull<br>  Pulls service images</p>\n</li>\n<li><p>docker-compose rm<br>  Remove stopped containers</p>\n</li>\n<li><p>docker-compose run</p>\n<p>  Run a one-off command 在一个服务上执行一个命令<br>  docker-compose run ubuntu ping docker.com</p>\n<pre><code>将会启动一个 ubuntu 服务，执行 ping docker.com 命令\n</code></pre><p>  默认情况下，所有关联的服务将会自动被启动，除非这些服务已经在运行中。<br>  该命令类似启动容器后运行指定的命令，相关卷、链接等等都将会按照期望创建。<br>  两个不同点：</p>\n<pre><code>给定命令将会覆盖原有的自动运行命令；\n不会自动创建端口，以避免冲突。\n</code></pre><p>  如果不希望自动启动关联的容器，可以使用 –no-deps 选项</p>\n<pre><code>docker-compose run --no-deps web python manage.py shell\n将不会启动 web 容器所关联的其它容器\n</code></pre></li>\n<li><p>docker-compose scale</p>\n<p>  Set number of containers for a service 设置同一个服务运行的容器个数<br>  service=num<br>  docker-compose scale web=2 worker=3</p>\n</li>\n<li><p>docker-compose start: Start services</p>\n</li>\n<li><p>docker-compose stop: Stop services</p>\n</li>\n<li><p>docker-compose restart</p>\n<p>  Restart services</p>\n<pre><code>env variable\n\n    COMPOSE_PROJECT_NAME\n        设置通过 Compose 启动的每一个容器前添加的项目名称，默认是当前工作目录的名字。\n    COMPOSE_FILE\n        设置要使用的 docker-compose.yml 的路径。默认路径是当前工作目录。\n    DOCKER_HOST\n        设置 Docker daemon 的地址。默认使用 unix:///var/run/docker.sock，与 Docker 客户端采用的默认值一致。\n    DOCKER_TLS_VERIFY\n        如果设置不为空，则与 Docker daemon 交互通过 TLS 进行。\n    DOCKER_CERT_PATH\n        配置 TLS 通信所需要的验证（ca.pem、cert.pem 和 key.pem）文件的路径，默认是 ~/.docker 。\n</code></pre></li>\n<li><p>docker-compose up</p>\n<p>  Create and start containers</p>\n<pre><code>$ docker-compose up -d\n</code></pre></li>\n<li><p>docker-compose logs</p>\n</li>\n<li><p>docker-compose version</p>\n<p>  Show the Docker-Compose version information</p>\n</li>\n<li><p>docker-compose unpause</p>\n<p>  Unpause services</p>\n</li>\n<li><p>docker-compose migrate-to-labels</p>\n<p>  Recreate containers to add labels</p>\n</li>\n</ul>\n"},{"title":"docker cloud detail","_content":"\n# docker cloud detail\n","source":"_posts/docker-cloud-detail.md","raw":"---\ntitle: docker cloud detail\ncategories:\n- docker\ntags:\n- detail\n---\n\n# docker cloud detail\n","slug":"docker-cloud-detail","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-04-18T13:17:47.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rcp001fkxsvk0zd9el4","content":"<h1 id=\"docker-cloud-detail\"><a href=\"#docker-cloud-detail\" class=\"headerlink\" title=\"docker cloud detail\"></a>docker cloud detail</h1>","excerpt":"","more":"<h1 id=\"docker-cloud-detail\"><a href=\"#docker-cloud-detail\" class=\"headerlink\" title=\"docker cloud detail\"></a>docker cloud detail</h1>"},{"title":"docker portal","_content":"\n# resource\n\nofficial:\n[https://www.docker.com/](https://www.docker.com/)\n[https://blog.docker.com/](https://blog.docker.com/)\n[https://linuxcontainers.org/](https://linuxcontainers.org/)\n\nawesome:\n[https://github.com/Friz-zy/awesome-linux-containers](https://github.com/Friz-zy/awesome-linux-containers)\n\ngithub:\n[docker-library](https://github.com/docker-library)\n[dockerfile](https://github.com/tianon/dockerfiles)   \n[docker-cheat-sheet](https://github.com/wsargent/docker-cheat-sheet)\n[Chef Cookbook for Docker](https://github.com/chef-cookbooks/docker)\n\ncommunity:\n[dockerpool](http://dockerpool.com/)\n[dockerone](http://dockerone.com)\n[devops-china](http://devops-china.org)\n[ceph china](http://bbs.ceph.org.cn/)    \ncloud stack(http://www.cloudstack-china.org/)\natcontainer(http://atcontainer.com/)\ngoogle plus(https://plus.google.com/u/0/+DockerIo)\nzhihu(https://www.zhihu.com/topic/19950993)\nreddit\nstackoverflow(http://superuser.com/questions/tagged/docker)\n\norganization:\nopen container project: [link](http://www.opencontainers.org/), [doc](http://blog.docker.com/2015/06/open-container-project-foundation/)\noci:[](https://www.opencontainers.org/), [](https://github.com/opencontainers)\nabout:尽管Docker获得广大公有云厂商的大力支持，但是目前容器技术生态中已经存在许多分支与分歧，如rkt项目。为了解决容器生态中的差异化问题，为了从根本上解决生产环境中运用Docker的风险，Google，Intel，Redhat，Microsoft，EMC，IBM，Amazon，VMware，Oracle，Pivotal，Rancher，HPE，Facebook，Twitter等IT大厂于2015年6月共同宣布成立OCI（Open Container Initiative）组织。OCI组织的目标在于建立通用的容器技术标准。除了保障与延续既有容器服务的生命周期外，还通过不断推出标准的创新的容器解决方案赋能开发者。而OCI成员企业也会秉持开放，安全，弹性等核心价值观来发展容器生态。客观而言，OCI组织的出现确立了容器技术的标准，避免容器技术被单一厂商垄断。统一技术标准后，广大企业不用担心未来新兴的容器技术不兼容Docker。\nCNCF: Cloud Native Computing Foundation\n\nconf\nQConf:http://2016.qconshanghai.com/, http://qconferences.com/\nDockerCon\n    link\n        http://www.slideshare.net/Docker/presentations\n        http://www.dockercon.com/\n    2015\n        guidebook app\n        http://europe-2015.dockercon.com/\n        http://dockerconeu2015.sched.org/\n    2016\n        http://2016.dockercon.com/\n        https://blog.docker.com/2016/06/dockercon-general-session-day-1-and-day-2-videos/\nContainerCon\nContainerCamp\nOperability 1.0\nGoTo Conference\nSoftware Circus\n容器技术大会\n    http://atcontainer.com/\n有容云\n    http://www.bagevent.com/event/176371\n\ndocker vs x\n    docker vs virtual machine\n        link\n            http://stackoverflow.com/questions/16047306/how-is-docker-different-from-a-normal-virtual-machine#\n            http://stackoverflow.com/questions/25444099/why-docker-has-ability-to-run-different-linux-distribution\n            http://stackoverflow.com/questions/18496940/how-to-deal-with-persistent-storage-e-g-databases-in-docker\n\ntraining:\n    http://edu.51cto.com/\n    https://csphere.cn/training/lesson/2\n    http://git.oschina.net/dockerf/docker-practice\n    http://git.oschina.net/dockerf/docker-training\n    mooc:https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS151.x+2T2016/info\n    \nread:\n- infoq: [infoq cn](http://www.infoq.com/cn/dockers/), [infoq en](https://www.infoq.com/docker-2)\n- bot: [yidian](http://www.yidianzixun.com/home?page=channel&keyword=docker), [toutiao](http://toutiao.com/tag85482990/)\n- book\n    Docker Cookbook\n    Docker技术入门与实战:publish,[opensource](http://dockerpool.com/static/books/docker_practice/index.html)\n    Docker源码分析\n    Docker技术详解与实践 lts        \n    [docker入门实战](http://yuedu.baidu.com/ebook/d817967416fc700abb68fca1)\n    [Service discovery with Docker](http://adetante.github.io/articles/service-discovery-with-docker-1/ )\n- tut\n    [](http://www.alauda.cn/tutorial/)\n    [](http://help.daocloud.io/)        \n    [](https://coreos.com/os/docs/latest/quickstart.html   )        \n- course\n    https://training.docker.com/self-paced-training\n    http://www.infoq.com/cn/presentations/antgroup-financial-cloud-paas-docker-practice              \nlanguages:\n- php\n    https://github.com/schmunk42/docker-yii2-app-basic\n    https://github.com/eko/docker-symfony\n    https://github.com/harshjv/docker-laravel\n    https://github.com/tkyk/docker-compose-lamp\n    https://github.com/docker-library/php\n    compose\n    https://github.com/DockerNuts/docker-compose-php-mysql/blob/master/docker-compose.yml\n    https://github.com/tkyk/docker-compose-lamp\n    https://github.com/larryprice/docker-compose-example\n- py\n    https://github.com/dockerfiles/django-uwsgi-nginx\n    https://github.com/mbentley/docker-django-uwsgi-nginx\n    https://github.com/dockerfiles/django-uwsgi-nginx\n    django:https://docs.docker.com/compose/django/\n    flask:dockercook 3.1\n- ruby\n    rails:https://docs.docker.com/compose/rails/\n- js\n    https://github.com/b00giZm/docker-compose-nodejs-examples/tree/master/01-express-nodemon\n- other\n    https://github.com/docker/compose/blob/master/SWARM.md\n    wordpress:https://docs.docker.com/compose/wordpress/\n\n12 factor\n    http://www.the12factorapp.com/\n    https://12factor.net/\n    https://chixq.com/articles/12-factor-app/\n\n\n        ","source":"_posts/docker--portal.md","raw":"---\ntitle: docker portal\ncategories:\n- docker\ntags:\n- portal\n---\n\n# resource\n\nofficial:\n[https://www.docker.com/](https://www.docker.com/)\n[https://blog.docker.com/](https://blog.docker.com/)\n[https://linuxcontainers.org/](https://linuxcontainers.org/)\n\nawesome:\n[https://github.com/Friz-zy/awesome-linux-containers](https://github.com/Friz-zy/awesome-linux-containers)\n\ngithub:\n[docker-library](https://github.com/docker-library)\n[dockerfile](https://github.com/tianon/dockerfiles)   \n[docker-cheat-sheet](https://github.com/wsargent/docker-cheat-sheet)\n[Chef Cookbook for Docker](https://github.com/chef-cookbooks/docker)\n\ncommunity:\n[dockerpool](http://dockerpool.com/)\n[dockerone](http://dockerone.com)\n[devops-china](http://devops-china.org)\n[ceph china](http://bbs.ceph.org.cn/)    \ncloud stack(http://www.cloudstack-china.org/)\natcontainer(http://atcontainer.com/)\ngoogle plus(https://plus.google.com/u/0/+DockerIo)\nzhihu(https://www.zhihu.com/topic/19950993)\nreddit\nstackoverflow(http://superuser.com/questions/tagged/docker)\n\norganization:\nopen container project: [link](http://www.opencontainers.org/), [doc](http://blog.docker.com/2015/06/open-container-project-foundation/)\noci:[](https://www.opencontainers.org/), [](https://github.com/opencontainers)\nabout:尽管Docker获得广大公有云厂商的大力支持，但是目前容器技术生态中已经存在许多分支与分歧，如rkt项目。为了解决容器生态中的差异化问题，为了从根本上解决生产环境中运用Docker的风险，Google，Intel，Redhat，Microsoft，EMC，IBM，Amazon，VMware，Oracle，Pivotal，Rancher，HPE，Facebook，Twitter等IT大厂于2015年6月共同宣布成立OCI（Open Container Initiative）组织。OCI组织的目标在于建立通用的容器技术标准。除了保障与延续既有容器服务的生命周期外，还通过不断推出标准的创新的容器解决方案赋能开发者。而OCI成员企业也会秉持开放，安全，弹性等核心价值观来发展容器生态。客观而言，OCI组织的出现确立了容器技术的标准，避免容器技术被单一厂商垄断。统一技术标准后，广大企业不用担心未来新兴的容器技术不兼容Docker。\nCNCF: Cloud Native Computing Foundation\n\nconf\nQConf:http://2016.qconshanghai.com/, http://qconferences.com/\nDockerCon\n    link\n        http://www.slideshare.net/Docker/presentations\n        http://www.dockercon.com/\n    2015\n        guidebook app\n        http://europe-2015.dockercon.com/\n        http://dockerconeu2015.sched.org/\n    2016\n        http://2016.dockercon.com/\n        https://blog.docker.com/2016/06/dockercon-general-session-day-1-and-day-2-videos/\nContainerCon\nContainerCamp\nOperability 1.0\nGoTo Conference\nSoftware Circus\n容器技术大会\n    http://atcontainer.com/\n有容云\n    http://www.bagevent.com/event/176371\n\ndocker vs x\n    docker vs virtual machine\n        link\n            http://stackoverflow.com/questions/16047306/how-is-docker-different-from-a-normal-virtual-machine#\n            http://stackoverflow.com/questions/25444099/why-docker-has-ability-to-run-different-linux-distribution\n            http://stackoverflow.com/questions/18496940/how-to-deal-with-persistent-storage-e-g-databases-in-docker\n\ntraining:\n    http://edu.51cto.com/\n    https://csphere.cn/training/lesson/2\n    http://git.oschina.net/dockerf/docker-practice\n    http://git.oschina.net/dockerf/docker-training\n    mooc:https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS151.x+2T2016/info\n    \nread:\n- infoq: [infoq cn](http://www.infoq.com/cn/dockers/), [infoq en](https://www.infoq.com/docker-2)\n- bot: [yidian](http://www.yidianzixun.com/home?page=channel&keyword=docker), [toutiao](http://toutiao.com/tag85482990/)\n- book\n    Docker Cookbook\n    Docker技术入门与实战:publish,[opensource](http://dockerpool.com/static/books/docker_practice/index.html)\n    Docker源码分析\n    Docker技术详解与实践 lts        \n    [docker入门实战](http://yuedu.baidu.com/ebook/d817967416fc700abb68fca1)\n    [Service discovery with Docker](http://adetante.github.io/articles/service-discovery-with-docker-1/ )\n- tut\n    [](http://www.alauda.cn/tutorial/)\n    [](http://help.daocloud.io/)        \n    [](https://coreos.com/os/docs/latest/quickstart.html   )        \n- course\n    https://training.docker.com/self-paced-training\n    http://www.infoq.com/cn/presentations/antgroup-financial-cloud-paas-docker-practice              \nlanguages:\n- php\n    https://github.com/schmunk42/docker-yii2-app-basic\n    https://github.com/eko/docker-symfony\n    https://github.com/harshjv/docker-laravel\n    https://github.com/tkyk/docker-compose-lamp\n    https://github.com/docker-library/php\n    compose\n    https://github.com/DockerNuts/docker-compose-php-mysql/blob/master/docker-compose.yml\n    https://github.com/tkyk/docker-compose-lamp\n    https://github.com/larryprice/docker-compose-example\n- py\n    https://github.com/dockerfiles/django-uwsgi-nginx\n    https://github.com/mbentley/docker-django-uwsgi-nginx\n    https://github.com/dockerfiles/django-uwsgi-nginx\n    django:https://docs.docker.com/compose/django/\n    flask:dockercook 3.1\n- ruby\n    rails:https://docs.docker.com/compose/rails/\n- js\n    https://github.com/b00giZm/docker-compose-nodejs-examples/tree/master/01-express-nodemon\n- other\n    https://github.com/docker/compose/blob/master/SWARM.md\n    wordpress:https://docs.docker.com/compose/wordpress/\n\n12 factor\n    http://www.the12factorapp.com/\n    https://12factor.net/\n    https://chixq.com/articles/12-factor-app/\n\n\n        ","slug":"docker--portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-02-10T03:40:59.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rcs001ikxsv7fw0whgr","content":"<h1 id=\"resource\"><a href=\"#resource\" class=\"headerlink\" title=\"resource\"></a>resource</h1><p>official:<br><a href=\"https://www.docker.com/\" target=\"_blank\" rel=\"external\">https://www.docker.com/</a><br><a href=\"https://blog.docker.com/\" target=\"_blank\" rel=\"external\">https://blog.docker.com/</a><br><a href=\"https://linuxcontainers.org/\" target=\"_blank\" rel=\"external\">https://linuxcontainers.org/</a></p>\n<p>awesome:<br><a href=\"https://github.com/Friz-zy/awesome-linux-containers\" target=\"_blank\" rel=\"external\">https://github.com/Friz-zy/awesome-linux-containers</a></p>\n<p>github:<br><a href=\"https://github.com/docker-library\" target=\"_blank\" rel=\"external\">docker-library</a><br><a href=\"https://github.com/tianon/dockerfiles\" target=\"_blank\" rel=\"external\">dockerfile</a><br><a href=\"https://github.com/wsargent/docker-cheat-sheet\" target=\"_blank\" rel=\"external\">docker-cheat-sheet</a><br><a href=\"https://github.com/chef-cookbooks/docker\" target=\"_blank\" rel=\"external\">Chef Cookbook for Docker</a></p>\n<p>community:<br><a href=\"http://dockerpool.com/\" target=\"_blank\" rel=\"external\">dockerpool</a><br><a href=\"http://dockerone.com\" target=\"_blank\" rel=\"external\">dockerone</a><br><a href=\"http://devops-china.org\" target=\"_blank\" rel=\"external\">devops-china</a><br><a href=\"http://bbs.ceph.org.cn/\" target=\"_blank\" rel=\"external\">ceph china</a><br>cloud stack(<a href=\"http://www.cloudstack-china.org/\" target=\"_blank\" rel=\"external\">http://www.cloudstack-china.org/</a>)<br>atcontainer(<a href=\"http://atcontainer.com/\" target=\"_blank\" rel=\"external\">http://atcontainer.com/</a>)<br>google plus(<a href=\"https://plus.google.com/u/0/+DockerIo\" target=\"_blank\" rel=\"external\">https://plus.google.com/u/0/+DockerIo</a>)<br>zhihu(<a href=\"https://www.zhihu.com/topic/19950993\" target=\"_blank\" rel=\"external\">https://www.zhihu.com/topic/19950993</a>)<br>reddit<br>stackoverflow(<a href=\"http://superuser.com/questions/tagged/docker\" target=\"_blank\" rel=\"external\">http://superuser.com/questions/tagged/docker</a>)</p>\n<p>organization:<br>open container project: <a href=\"http://www.opencontainers.org/\" target=\"_blank\" rel=\"external\">link</a>, <a href=\"http://blog.docker.com/2015/06/open-container-project-foundation/\" target=\"_blank\" rel=\"external\">doc</a><br>oci:<a href=\"https://www.opencontainers.org/\" target=\"_blank\" rel=\"external\"></a>, <a href=\"https://github.com/opencontainers\" target=\"_blank\" rel=\"external\"></a><br>about:尽管Docker获得广大公有云厂商的大力支持，但是目前容器技术生态中已经存在许多分支与分歧，如rkt项目。为了解决容器生态中的差异化问题，为了从根本上解决生产环境中运用Docker的风险，Google，Intel，Redhat，Microsoft，EMC，IBM，Amazon，VMware，Oracle，Pivotal，Rancher，HPE，Facebook，Twitter等IT大厂于2015年6月共同宣布成立OCI（Open Container Initiative）组织。OCI组织的目标在于建立通用的容器技术标准。除了保障与延续既有容器服务的生命周期外，还通过不断推出标准的创新的容器解决方案赋能开发者。而OCI成员企业也会秉持开放，安全，弹性等核心价值观来发展容器生态。客观而言，OCI组织的出现确立了容器技术的标准，避免容器技术被单一厂商垄断。统一技术标准后，广大企业不用担心未来新兴的容器技术不兼容Docker。<br>CNCF: Cloud Native Computing Foundation</p>\n<p>conf<br>QConf:<a href=\"http://2016.qconshanghai.com/\" target=\"_blank\" rel=\"external\">http://2016.qconshanghai.com/</a>, <a href=\"http://qconferences.com/\" target=\"_blank\" rel=\"external\">http://qconferences.com/</a><br>DockerCon<br>    link<br>        <a href=\"http://www.slideshare.net/Docker/presentations\" target=\"_blank\" rel=\"external\">http://www.slideshare.net/Docker/presentations</a><br>        <a href=\"http://www.dockercon.com/\" target=\"_blank\" rel=\"external\">http://www.dockercon.com/</a><br>    2015<br>        guidebook app<br>        <a href=\"http://europe-2015.dockercon.com/\" target=\"_blank\" rel=\"external\">http://europe-2015.dockercon.com/</a><br>        <a href=\"http://dockerconeu2015.sched.org/\" target=\"_blank\" rel=\"external\">http://dockerconeu2015.sched.org/</a><br>    2016<br>        <a href=\"http://2016.dockercon.com/\" target=\"_blank\" rel=\"external\">http://2016.dockercon.com/</a><br>        <a href=\"https://blog.docker.com/2016/06/dockercon-general-session-day-1-and-day-2-videos/\" target=\"_blank\" rel=\"external\">https://blog.docker.com/2016/06/dockercon-general-session-day-1-and-day-2-videos/</a><br>ContainerCon<br>ContainerCamp<br>Operability 1.0<br>GoTo Conference<br>Software Circus<br>容器技术大会<br>    <a href=\"http://atcontainer.com/\" target=\"_blank\" rel=\"external\">http://atcontainer.com/</a><br>有容云<br>    <a href=\"http://www.bagevent.com/event/176371\" target=\"_blank\" rel=\"external\">http://www.bagevent.com/event/176371</a></p>\n<p>docker vs x<br>    docker vs virtual machine<br>        link<br>            <a href=\"http://stackoverflow.com/questions/16047306/how-is-docker-different-from-a-normal-virtual-machine#\" target=\"_blank\" rel=\"external\">http://stackoverflow.com/questions/16047306/how-is-docker-different-from-a-normal-virtual-machine#</a><br>            <a href=\"http://stackoverflow.com/questions/25444099/why-docker-has-ability-to-run-different-linux-distribution\" target=\"_blank\" rel=\"external\">http://stackoverflow.com/questions/25444099/why-docker-has-ability-to-run-different-linux-distribution</a><br>            <a href=\"http://stackoverflow.com/questions/18496940/how-to-deal-with-persistent-storage-e-g-databases-in-docker\" target=\"_blank\" rel=\"external\">http://stackoverflow.com/questions/18496940/how-to-deal-with-persistent-storage-e-g-databases-in-docker</a></p>\n<p>training:<br>    <a href=\"http://edu.51cto.com/\" target=\"_blank\" rel=\"external\">http://edu.51cto.com/</a><br>    <a href=\"https://csphere.cn/training/lesson/2\" target=\"_blank\" rel=\"external\">https://csphere.cn/training/lesson/2</a><br>    <a href=\"http://git.oschina.net/dockerf/docker-practice\" target=\"_blank\" rel=\"external\">http://git.oschina.net/dockerf/docker-practice</a><br>    <a href=\"http://git.oschina.net/dockerf/docker-training\" target=\"_blank\" rel=\"external\">http://git.oschina.net/dockerf/docker-training</a><br>    mooc:<a href=\"https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS151.x+2T2016/info\" target=\"_blank\" rel=\"external\">https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS151.x+2T2016/info</a></p>\n<p>read:</p>\n<ul>\n<li>infoq: <a href=\"http://www.infoq.com/cn/dockers/\" target=\"_blank\" rel=\"external\">infoq cn</a>, <a href=\"https://www.infoq.com/docker-2\" target=\"_blank\" rel=\"external\">infoq en</a></li>\n<li>bot: <a href=\"http://www.yidianzixun.com/home?page=channel&amp;keyword=docker\" target=\"_blank\" rel=\"external\">yidian</a>, <a href=\"http://toutiao.com/tag85482990/\" target=\"_blank\" rel=\"external\">toutiao</a></li>\n<li>book<br>  Docker Cookbook<br>  Docker技术入门与实战:publish,<a href=\"http://dockerpool.com/static/books/docker_practice/index.html\" target=\"_blank\" rel=\"external\">opensource</a><br>  Docker源码分析<br>  Docker技术详解与实践 lts<br>  <a href=\"http://yuedu.baidu.com/ebook/d817967416fc700abb68fca1\" target=\"_blank\" rel=\"external\">docker入门实战</a><br>  <a href=\"http://adetante.github.io/articles/service-discovery-with-docker-1/\" target=\"_blank\" rel=\"external\">Service discovery with Docker</a></li>\n<li>tut<br>  <a href=\"http://www.alauda.cn/tutorial/\" target=\"_blank\" rel=\"external\"></a><br>  <a href=\"http://help.daocloud.io/\" target=\"_blank\" rel=\"external\"></a><br>  <a href=\"https://coreos.com/os/docs/latest/quickstart.html\" target=\"_blank\" rel=\"external\"></a>        </li>\n<li>course<br>  <a href=\"https://training.docker.com/self-paced-training\" target=\"_blank\" rel=\"external\">https://training.docker.com/self-paced-training</a><br>  <a href=\"http://www.infoq.com/cn/presentations/antgroup-financial-cloud-paas-docker-practice\" target=\"_blank\" rel=\"external\">http://www.infoq.com/cn/presentations/antgroup-financial-cloud-paas-docker-practice</a><br>languages:</li>\n<li>php<br>  <a href=\"https://github.com/schmunk42/docker-yii2-app-basic\" target=\"_blank\" rel=\"external\">https://github.com/schmunk42/docker-yii2-app-basic</a><br>  <a href=\"https://github.com/eko/docker-symfony\" target=\"_blank\" rel=\"external\">https://github.com/eko/docker-symfony</a><br>  <a href=\"https://github.com/harshjv/docker-laravel\" target=\"_blank\" rel=\"external\">https://github.com/harshjv/docker-laravel</a><br>  <a href=\"https://github.com/tkyk/docker-compose-lamp\" target=\"_blank\" rel=\"external\">https://github.com/tkyk/docker-compose-lamp</a><br>  <a href=\"https://github.com/docker-library/php\" target=\"_blank\" rel=\"external\">https://github.com/docker-library/php</a><br>  compose<br>  <a href=\"https://github.com/DockerNuts/docker-compose-php-mysql/blob/master/docker-compose.yml\" target=\"_blank\" rel=\"external\">https://github.com/DockerNuts/docker-compose-php-mysql/blob/master/docker-compose.yml</a><br>  <a href=\"https://github.com/tkyk/docker-compose-lamp\" target=\"_blank\" rel=\"external\">https://github.com/tkyk/docker-compose-lamp</a><br>  <a href=\"https://github.com/larryprice/docker-compose-example\" target=\"_blank\" rel=\"external\">https://github.com/larryprice/docker-compose-example</a></li>\n<li>py<br>  <a href=\"https://github.com/dockerfiles/django-uwsgi-nginx\" target=\"_blank\" rel=\"external\">https://github.com/dockerfiles/django-uwsgi-nginx</a><br>  <a href=\"https://github.com/mbentley/docker-django-uwsgi-nginx\" target=\"_blank\" rel=\"external\">https://github.com/mbentley/docker-django-uwsgi-nginx</a><br>  <a href=\"https://github.com/dockerfiles/django-uwsgi-nginx\" target=\"_blank\" rel=\"external\">https://github.com/dockerfiles/django-uwsgi-nginx</a><br>  django:<a href=\"https://docs.docker.com/compose/django/\" target=\"_blank\" rel=\"external\">https://docs.docker.com/compose/django/</a><br>  flask:dockercook 3.1</li>\n<li>ruby<br>  rails:<a href=\"https://docs.docker.com/compose/rails/\" target=\"_blank\" rel=\"external\">https://docs.docker.com/compose/rails/</a></li>\n<li>js<br>  <a href=\"https://github.com/b00giZm/docker-compose-nodejs-examples/tree/master/01-express-nodemon\" target=\"_blank\" rel=\"external\">https://github.com/b00giZm/docker-compose-nodejs-examples/tree/master/01-express-nodemon</a></li>\n<li>other<br>  <a href=\"https://github.com/docker/compose/blob/master/SWARM.md\" target=\"_blank\" rel=\"external\">https://github.com/docker/compose/blob/master/SWARM.md</a><br>  wordpress:<a href=\"https://docs.docker.com/compose/wordpress/\" target=\"_blank\" rel=\"external\">https://docs.docker.com/compose/wordpress/</a></li>\n</ul>\n<p>12 factor<br>    <a href=\"http://www.the12factorapp.com/\" target=\"_blank\" rel=\"external\">http://www.the12factorapp.com/</a><br>    <a href=\"https://12factor.net/\" target=\"_blank\" rel=\"external\">https://12factor.net/</a><br>    <a href=\"https://chixq.com/articles/12-factor-app/\" target=\"_blank\" rel=\"external\">https://chixq.com/articles/12-factor-app/</a></p>\n","excerpt":"","more":"<h1 id=\"resource\"><a href=\"#resource\" class=\"headerlink\" title=\"resource\"></a>resource</h1><p>official:<br><a href=\"https://www.docker.com/\">https://www.docker.com/</a><br><a href=\"https://blog.docker.com/\">https://blog.docker.com/</a><br><a href=\"https://linuxcontainers.org/\">https://linuxcontainers.org/</a></p>\n<p>awesome:<br><a href=\"https://github.com/Friz-zy/awesome-linux-containers\">https://github.com/Friz-zy/awesome-linux-containers</a></p>\n<p>github:<br><a href=\"https://github.com/docker-library\">docker-library</a><br><a href=\"https://github.com/tianon/dockerfiles\">dockerfile</a><br><a href=\"https://github.com/wsargent/docker-cheat-sheet\">docker-cheat-sheet</a><br><a href=\"https://github.com/chef-cookbooks/docker\">Chef Cookbook for Docker</a></p>\n<p>community:<br><a href=\"http://dockerpool.com/\">dockerpool</a><br><a href=\"http://dockerone.com\">dockerone</a><br><a href=\"http://devops-china.org\">devops-china</a><br><a href=\"http://bbs.ceph.org.cn/\">ceph china</a><br>cloud stack(<a href=\"http://www.cloudstack-china.org/\">http://www.cloudstack-china.org/</a>)<br>atcontainer(<a href=\"http://atcontainer.com/\">http://atcontainer.com/</a>)<br>google plus(<a href=\"https://plus.google.com/u/0/+DockerIo\">https://plus.google.com/u/0/+DockerIo</a>)<br>zhihu(<a href=\"https://www.zhihu.com/topic/19950993\">https://www.zhihu.com/topic/19950993</a>)<br>reddit<br>stackoverflow(<a href=\"http://superuser.com/questions/tagged/docker\">http://superuser.com/questions/tagged/docker</a>)</p>\n<p>organization:<br>open container project: <a href=\"http://www.opencontainers.org/\">link</a>, <a href=\"http://blog.docker.com/2015/06/open-container-project-foundation/\">doc</a><br>oci:<a href=\"https://www.opencontainers.org/\"></a>, <a href=\"https://github.com/opencontainers\"></a><br>about:尽管Docker获得广大公有云厂商的大力支持，但是目前容器技术生态中已经存在许多分支与分歧，如rkt项目。为了解决容器生态中的差异化问题，为了从根本上解决生产环境中运用Docker的风险，Google，Intel，Redhat，Microsoft，EMC，IBM，Amazon，VMware，Oracle，Pivotal，Rancher，HPE，Facebook，Twitter等IT大厂于2015年6月共同宣布成立OCI（Open Container Initiative）组织。OCI组织的目标在于建立通用的容器技术标准。除了保障与延续既有容器服务的生命周期外，还通过不断推出标准的创新的容器解决方案赋能开发者。而OCI成员企业也会秉持开放，安全，弹性等核心价值观来发展容器生态。客观而言，OCI组织的出现确立了容器技术的标准，避免容器技术被单一厂商垄断。统一技术标准后，广大企业不用担心未来新兴的容器技术不兼容Docker。<br>CNCF: Cloud Native Computing Foundation</p>\n<p>conf<br>QConf:<a href=\"http://2016.qconshanghai.com/\">http://2016.qconshanghai.com/</a>, <a href=\"http://qconferences.com/\">http://qconferences.com/</a><br>DockerCon<br>    link<br>        <a href=\"http://www.slideshare.net/Docker/presentations\">http://www.slideshare.net/Docker/presentations</a><br>        <a href=\"http://www.dockercon.com/\">http://www.dockercon.com/</a><br>    2015<br>        guidebook app<br>        <a href=\"http://europe-2015.dockercon.com/\">http://europe-2015.dockercon.com/</a><br>        <a href=\"http://dockerconeu2015.sched.org/\">http://dockerconeu2015.sched.org/</a><br>    2016<br>        <a href=\"http://2016.dockercon.com/\">http://2016.dockercon.com/</a><br>        <a href=\"https://blog.docker.com/2016/06/dockercon-general-session-day-1-and-day-2-videos/\">https://blog.docker.com/2016/06/dockercon-general-session-day-1-and-day-2-videos/</a><br>ContainerCon<br>ContainerCamp<br>Operability 1.0<br>GoTo Conference<br>Software Circus<br>容器技术大会<br>    <a href=\"http://atcontainer.com/\">http://atcontainer.com/</a><br>有容云<br>    <a href=\"http://www.bagevent.com/event/176371\">http://www.bagevent.com/event/176371</a></p>\n<p>docker vs x<br>    docker vs virtual machine<br>        link<br>            <a href=\"http://stackoverflow.com/questions/16047306/how-is-docker-different-from-a-normal-virtual-machine#\">http://stackoverflow.com/questions/16047306/how-is-docker-different-from-a-normal-virtual-machine#</a><br>            <a href=\"http://stackoverflow.com/questions/25444099/why-docker-has-ability-to-run-different-linux-distribution\">http://stackoverflow.com/questions/25444099/why-docker-has-ability-to-run-different-linux-distribution</a><br>            <a href=\"http://stackoverflow.com/questions/18496940/how-to-deal-with-persistent-storage-e-g-databases-in-docker\">http://stackoverflow.com/questions/18496940/how-to-deal-with-persistent-storage-e-g-databases-in-docker</a></p>\n<p>training:<br>    <a href=\"http://edu.51cto.com/\">http://edu.51cto.com/</a><br>    <a href=\"https://csphere.cn/training/lesson/2\">https://csphere.cn/training/lesson/2</a><br>    <a href=\"http://git.oschina.net/dockerf/docker-practice\">http://git.oschina.net/dockerf/docker-practice</a><br>    <a href=\"http://git.oschina.net/dockerf/docker-training\">http://git.oschina.net/dockerf/docker-training</a><br>    mooc:<a href=\"https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS151.x+2T2016/info\">https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS151.x+2T2016/info</a></p>\n<p>read:</p>\n<ul>\n<li>infoq: <a href=\"http://www.infoq.com/cn/dockers/\">infoq cn</a>, <a href=\"https://www.infoq.com/docker-2\">infoq en</a></li>\n<li>bot: <a href=\"http://www.yidianzixun.com/home?page=channel&amp;keyword=docker\">yidian</a>, <a href=\"http://toutiao.com/tag85482990/\">toutiao</a></li>\n<li>book<br>  Docker Cookbook<br>  Docker技术入门与实战:publish,<a href=\"http://dockerpool.com/static/books/docker_practice/index.html\">opensource</a><br>  Docker源码分析<br>  Docker技术详解与实践 lts<br>  <a href=\"http://yuedu.baidu.com/ebook/d817967416fc700abb68fca1\">docker入门实战</a><br>  <a href=\"http://adetante.github.io/articles/service-discovery-with-docker-1/\">Service discovery with Docker</a></li>\n<li>tut<br>  <a href=\"http://www.alauda.cn/tutorial/\"></a><br>  <a href=\"http://help.daocloud.io/\"></a><br>  <a href=\"https://coreos.com/os/docs/latest/quickstart.html\"></a>        </li>\n<li>course<br>  <a href=\"https://training.docker.com/self-paced-training\">https://training.docker.com/self-paced-training</a><br>  <a href=\"http://www.infoq.com/cn/presentations/antgroup-financial-cloud-paas-docker-practice\">http://www.infoq.com/cn/presentations/antgroup-financial-cloud-paas-docker-practice</a><br>languages:</li>\n<li>php<br>  <a href=\"https://github.com/schmunk42/docker-yii2-app-basic\">https://github.com/schmunk42/docker-yii2-app-basic</a><br>  <a href=\"https://github.com/eko/docker-symfony\">https://github.com/eko/docker-symfony</a><br>  <a href=\"https://github.com/harshjv/docker-laravel\">https://github.com/harshjv/docker-laravel</a><br>  <a href=\"https://github.com/tkyk/docker-compose-lamp\">https://github.com/tkyk/docker-compose-lamp</a><br>  <a href=\"https://github.com/docker-library/php\">https://github.com/docker-library/php</a><br>  compose<br>  <a href=\"https://github.com/DockerNuts/docker-compose-php-mysql/blob/master/docker-compose.yml\">https://github.com/DockerNuts/docker-compose-php-mysql/blob/master/docker-compose.yml</a><br>  <a href=\"https://github.com/tkyk/docker-compose-lamp\">https://github.com/tkyk/docker-compose-lamp</a><br>  <a href=\"https://github.com/larryprice/docker-compose-example\">https://github.com/larryprice/docker-compose-example</a></li>\n<li>py<br>  <a href=\"https://github.com/dockerfiles/django-uwsgi-nginx\">https://github.com/dockerfiles/django-uwsgi-nginx</a><br>  <a href=\"https://github.com/mbentley/docker-django-uwsgi-nginx\">https://github.com/mbentley/docker-django-uwsgi-nginx</a><br>  <a href=\"https://github.com/dockerfiles/django-uwsgi-nginx\">https://github.com/dockerfiles/django-uwsgi-nginx</a><br>  django:<a href=\"https://docs.docker.com/compose/django/\">https://docs.docker.com/compose/django/</a><br>  flask:dockercook 3.1</li>\n<li>ruby<br>  rails:<a href=\"https://docs.docker.com/compose/rails/\">https://docs.docker.com/compose/rails/</a></li>\n<li>js<br>  <a href=\"https://github.com/b00giZm/docker-compose-nodejs-examples/tree/master/01-express-nodemon\">https://github.com/b00giZm/docker-compose-nodejs-examples/tree/master/01-express-nodemon</a></li>\n<li>other<br>  <a href=\"https://github.com/docker/compose/blob/master/SWARM.md\">https://github.com/docker/compose/blob/master/SWARM.md</a><br>  wordpress:<a href=\"https://docs.docker.com/compose/wordpress/\">https://docs.docker.com/compose/wordpress/</a></li>\n</ul>\n<p>12 factor<br>    <a href=\"http://www.the12factorapp.com/\">http://www.the12factorapp.com/</a><br>    <a href=\"https://12factor.net/\">https://12factor.net/</a><br>    <a href=\"https://chixq.com/articles/12-factor-app/\">https://chixq.com/articles/12-factor-app/</a></p>\n"},{"title":"docker compose","_content":"\n# about\n\n- official: https://docs.docker.com/compose/compose-file/\n- django example: https://docs.docker.com/compose/django/\n- rails example: https://docs.docker.com/compose/rails/\n- machine, swarm, compose: https://blog.docker.com/2015/02/orchestrating-docker-with-machine-swarm-and-compose/\n\nThe Compose file is a YAML file defining services, networks and volumes. The default path for a Compose file is ./docker-compose.yml.\n\nA service definition contains configuration which will be applied to each container started for that service, much like passing command-line parameters to docker run. Likewise, network and volume definitions are analogous to docker network create and docker volume create.\n\nAs with docker run, options specified in the Dockerfile (e.g., CMD, EXPOSE, VOLUME, ENV) are respected by default - you don’t need to specify them again in docker-compose.yml.\n\n# service configuration reference\n\n- build\n\n# volume configuration reference\n\n# network configuration reference\n\n# versioning\n\n# variable substitution\n","source":"_posts/docker-compose-file-detail.md","raw":"---\ntitle: docker compose\ncategories:\n- docker\ntags:\n- detail\n- compose\n---\n\n# about\n\n- official: https://docs.docker.com/compose/compose-file/\n- django example: https://docs.docker.com/compose/django/\n- rails example: https://docs.docker.com/compose/rails/\n- machine, swarm, compose: https://blog.docker.com/2015/02/orchestrating-docker-with-machine-swarm-and-compose/\n\nThe Compose file is a YAML file defining services, networks and volumes. The default path for a Compose file is ./docker-compose.yml.\n\nA service definition contains configuration which will be applied to each container started for that service, much like passing command-line parameters to docker run. Likewise, network and volume definitions are analogous to docker network create and docker volume create.\n\nAs with docker run, options specified in the Dockerfile (e.g., CMD, EXPOSE, VOLUME, ENV) are respected by default - you don’t need to specify them again in docker-compose.yml.\n\n# service configuration reference\n\n- build\n\n# volume configuration reference\n\n# network configuration reference\n\n# versioning\n\n# variable substitution\n","slug":"docker-compose-file-detail","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-04-18T13:18:01.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rcv001mkxsv8ov7t3bo","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><ul>\n<li>official: <a href=\"https://docs.docker.com/compose/compose-file/\" target=\"_blank\" rel=\"external\">https://docs.docker.com/compose/compose-file/</a></li>\n<li>django example: <a href=\"https://docs.docker.com/compose/django/\" target=\"_blank\" rel=\"external\">https://docs.docker.com/compose/django/</a></li>\n<li>rails example: <a href=\"https://docs.docker.com/compose/rails/\" target=\"_blank\" rel=\"external\">https://docs.docker.com/compose/rails/</a></li>\n<li>machine, swarm, compose: <a href=\"https://blog.docker.com/2015/02/orchestrating-docker-with-machine-swarm-and-compose/\" target=\"_blank\" rel=\"external\">https://blog.docker.com/2015/02/orchestrating-docker-with-machine-swarm-and-compose/</a></li>\n</ul>\n<p>The Compose file is a YAML file defining services, networks and volumes. The default path for a Compose file is ./docker-compose.yml.</p>\n<p>A service definition contains configuration which will be applied to each container started for that service, much like passing command-line parameters to docker run. Likewise, network and volume definitions are analogous to docker network create and docker volume create.</p>\n<p>As with docker run, options specified in the Dockerfile (e.g., CMD, EXPOSE, VOLUME, ENV) are respected by default - you don’t need to specify them again in docker-compose.yml.</p>\n<h1 id=\"service-configuration-reference\"><a href=\"#service-configuration-reference\" class=\"headerlink\" title=\"service configuration reference\"></a>service configuration reference</h1><ul>\n<li>build</li>\n</ul>\n<h1 id=\"volume-configuration-reference\"><a href=\"#volume-configuration-reference\" class=\"headerlink\" title=\"volume configuration reference\"></a>volume configuration reference</h1><h1 id=\"network-configuration-reference\"><a href=\"#network-configuration-reference\" class=\"headerlink\" title=\"network configuration reference\"></a>network configuration reference</h1><h1 id=\"versioning\"><a href=\"#versioning\" class=\"headerlink\" title=\"versioning\"></a>versioning</h1><h1 id=\"variable-substitution\"><a href=\"#variable-substitution\" class=\"headerlink\" title=\"variable substitution\"></a>variable substitution</h1>","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><ul>\n<li>official: <a href=\"https://docs.docker.com/compose/compose-file/\">https://docs.docker.com/compose/compose-file/</a></li>\n<li>django example: <a href=\"https://docs.docker.com/compose/django/\">https://docs.docker.com/compose/django/</a></li>\n<li>rails example: <a href=\"https://docs.docker.com/compose/rails/\">https://docs.docker.com/compose/rails/</a></li>\n<li>machine, swarm, compose: <a href=\"https://blog.docker.com/2015/02/orchestrating-docker-with-machine-swarm-and-compose/\">https://blog.docker.com/2015/02/orchestrating-docker-with-machine-swarm-and-compose/</a></li>\n</ul>\n<p>The Compose file is a YAML file defining services, networks and volumes. The default path for a Compose file is ./docker-compose.yml.</p>\n<p>A service definition contains configuration which will be applied to each container started for that service, much like passing command-line parameters to docker run. Likewise, network and volume definitions are analogous to docker network create and docker volume create.</p>\n<p>As with docker run, options specified in the Dockerfile (e.g., CMD, EXPOSE, VOLUME, ENV) are respected by default - you don’t need to specify them again in docker-compose.yml.</p>\n<h1 id=\"service-configuration-reference\"><a href=\"#service-configuration-reference\" class=\"headerlink\" title=\"service configuration reference\"></a>service configuration reference</h1><ul>\n<li>build</li>\n</ul>\n<h1 id=\"volume-configuration-reference\"><a href=\"#volume-configuration-reference\" class=\"headerlink\" title=\"volume configuration reference\"></a>volume configuration reference</h1><h1 id=\"network-configuration-reference\"><a href=\"#network-configuration-reference\" class=\"headerlink\" title=\"network configuration reference\"></a>network configuration reference</h1><h1 id=\"versioning\"><a href=\"#versioning\" class=\"headerlink\" title=\"versioning\"></a>versioning</h1><h1 id=\"variable-substitution\"><a href=\"#variable-substitution\" class=\"headerlink\" title=\"variable substitution\"></a>variable substitution</h1>"},{"title":"dockerfile","_content":"\n# about dockerfile\n\n\n# dockerfile command\n\n- point to a Dockerfile anywhere in your file system\n\n        docker build -f /path/to/a/Dockerfile .\n\n- specify a repository and tag at which to save the new image if the build succeeds\n\n        docker build -t shykes/myapp .\n\n# dockerfile keyword\n\n- FROM: base image\n\n- MAINTAINER\n\n        MAINTAINER ag \"allengaller@gmail.com\"\n\n- USER: set user\n\n        USER root\n\n- RUN: run system cmd\n\n        RUN apt-get update\n        RUN [\"apt-get\", \"update\"]\n        RUN apt-get install -y nginx\n        RUN touch test.txt && echo \"abc\" >> abc.txt\n\n- EXPOSE: expose port\n\n- ADD\n\n    The ADD instruction copies new files, directories or remote file URLs from <src> and adds them to the filesystem of the container at the path <dest>.\n\n    pattern: ADD <src>... <dest>; ADD [\"<src>\",... \"<dest>\"]\n    add folder: ADD /webapp /opt/webapp\n    add file: ADD abc.txt /opt/\n    add network file: ADD https://www.baidu.com/img/bd_logo1.png /opt/\n\n- ENV: set env variable\n\n        ENV WEBAPP_PORT = 9090\n\n- WORKDIR: set working directory\n\n    The WORKDIR instruction sets the working directory for any RUN, CMD, ENTRYPOINT, COPY and ADD instructions that follow it in the Dockerfile.The WORKDIR instruction can resolve environment variables previously set using ENV. You can only use environment variables explicitly set in the Dockerfile. For example:\n\n        ENV DIRPATH /path\n        WORKDIR $DIRPATH/$DIRNAME\n        RUN pwd\n\n    The output of the final pwd command in this Dockerfile would be /path/$DIRNAME.It can be used multiple times in the one Dockerfile. If a relative path is provided, it will be relative to the path of the previous WORKDIR instruction. For example:\n\n        WORKDIR /a\n        WORKDIR b\n        WORKDIR c\n        RUN pwd\n\n    The output of the final pwd command in this Dockerfile would be /a/b/c.\n\n        WORKDIR /opt/\n\n- ENTRYPOINT: set boot command, append parameter to boot cmd\n\n    An ENTRYPOINT allows you to configure a container that will run as an executable.For example, the following will start nginx with its default content, listening on port 80:\n\n        docker run -i -t --rm -p 80:80 nginx\n\n        ENTRYPOINT [\"ls\"]\n        ENTRYPOINT [\"ls\"]\n        CMD [\"-l\", \"-a\"]\n\n- CMD: set boot parameter\n\n        CMD [\"ls\", \"-a\", \"-l\"]\n        CMD ls -l -a\n\n- VOLUME: set volume\n\n        VOLUME [\"/data\", \"/var/www\"]\n\n- ONBUILD: trigger for child image\n\n        ONBUILD ADD . /app/src\n        ONBUILD RUN echo \"on build excuted\" >> onbuild.txt\n\n- ARG\n\n- STOPSIGNAL\n\n# best practice [link](https://docs.docker.com/engine/articles/dockerfile_best-practices/)\n\n- Containers should be ephemeral\n\n    The container produced by the image your Dockerfile defines should be as ephemeral as possible.By “ephemeral,” we mean that it can be stopped and destroyed and a new one built and put in place with an absolute minimum of set-up and configuration.\n\n- Use a .dockerignore file\n\n    In most cases, it’s best to put each Dockerfile in an empty directory. Then, add to that directory only the files needed for building the Dockerfile. To increase the build’s performance, you can exclude files and directories by adding a .dockerignore file to that directory as well. This file supports exclusion patterns similar to .gitignore files. For information on creating one, see the .dockerignore file.\n\n- Avoid installing unnecessary packages\n\n    In order to reduce complexity, dependencies, file sizes, and build times, you should avoid installing extra or unnecessary packages just because they might be “nice to have.” For example, you don’t need to include a text editor in a database image.\n\n- Run only one process per container\n\n    In almost all cases, you should only run a single process in a single container. Decoupling applications into multiple containers makes it much easier to scale horizontally and reuse containers. If that service depends on another service, make use of container linking.\n\n- Minimize the number of layers\n\n    You need to find the balance between readability (and thus long-term maintainability) of the Dockerfile and minimizing the number of layers it uses. Be strategic and cautious about the number of layers you use.\n\n- Sort multi-line arguments\n\n    Here’s an example from the buildpack-deps image:\n\n    RUN apt-get update && apt-get install -y \\\n      bzr \\\n      cvs \\\n      git \\\n      mercurial \\\n      subversion\n\n- Build cache\n\n# .dockerignore\n\n    */temp*\n    */*/temp*\n    temp?\n","source":"_posts/docker-dockerfile-detail.md","raw":"---\ntitle: dockerfile\ncategories:\n- docker\ntags:\n- detail\n- dockerfile\n\n---\n\n# about dockerfile\n\n\n# dockerfile command\n\n- point to a Dockerfile anywhere in your file system\n\n        docker build -f /path/to/a/Dockerfile .\n\n- specify a repository and tag at which to save the new image if the build succeeds\n\n        docker build -t shykes/myapp .\n\n# dockerfile keyword\n\n- FROM: base image\n\n- MAINTAINER\n\n        MAINTAINER ag \"allengaller@gmail.com\"\n\n- USER: set user\n\n        USER root\n\n- RUN: run system cmd\n\n        RUN apt-get update\n        RUN [\"apt-get\", \"update\"]\n        RUN apt-get install -y nginx\n        RUN touch test.txt && echo \"abc\" >> abc.txt\n\n- EXPOSE: expose port\n\n- ADD\n\n    The ADD instruction copies new files, directories or remote file URLs from <src> and adds them to the filesystem of the container at the path <dest>.\n\n    pattern: ADD <src>... <dest>; ADD [\"<src>\",... \"<dest>\"]\n    add folder: ADD /webapp /opt/webapp\n    add file: ADD abc.txt /opt/\n    add network file: ADD https://www.baidu.com/img/bd_logo1.png /opt/\n\n- ENV: set env variable\n\n        ENV WEBAPP_PORT = 9090\n\n- WORKDIR: set working directory\n\n    The WORKDIR instruction sets the working directory for any RUN, CMD, ENTRYPOINT, COPY and ADD instructions that follow it in the Dockerfile.The WORKDIR instruction can resolve environment variables previously set using ENV. You can only use environment variables explicitly set in the Dockerfile. For example:\n\n        ENV DIRPATH /path\n        WORKDIR $DIRPATH/$DIRNAME\n        RUN pwd\n\n    The output of the final pwd command in this Dockerfile would be /path/$DIRNAME.It can be used multiple times in the one Dockerfile. If a relative path is provided, it will be relative to the path of the previous WORKDIR instruction. For example:\n\n        WORKDIR /a\n        WORKDIR b\n        WORKDIR c\n        RUN pwd\n\n    The output of the final pwd command in this Dockerfile would be /a/b/c.\n\n        WORKDIR /opt/\n\n- ENTRYPOINT: set boot command, append parameter to boot cmd\n\n    An ENTRYPOINT allows you to configure a container that will run as an executable.For example, the following will start nginx with its default content, listening on port 80:\n\n        docker run -i -t --rm -p 80:80 nginx\n\n        ENTRYPOINT [\"ls\"]\n        ENTRYPOINT [\"ls\"]\n        CMD [\"-l\", \"-a\"]\n\n- CMD: set boot parameter\n\n        CMD [\"ls\", \"-a\", \"-l\"]\n        CMD ls -l -a\n\n- VOLUME: set volume\n\n        VOLUME [\"/data\", \"/var/www\"]\n\n- ONBUILD: trigger for child image\n\n        ONBUILD ADD . /app/src\n        ONBUILD RUN echo \"on build excuted\" >> onbuild.txt\n\n- ARG\n\n- STOPSIGNAL\n\n# best practice [link](https://docs.docker.com/engine/articles/dockerfile_best-practices/)\n\n- Containers should be ephemeral\n\n    The container produced by the image your Dockerfile defines should be as ephemeral as possible.By “ephemeral,” we mean that it can be stopped and destroyed and a new one built and put in place with an absolute minimum of set-up and configuration.\n\n- Use a .dockerignore file\n\n    In most cases, it’s best to put each Dockerfile in an empty directory. Then, add to that directory only the files needed for building the Dockerfile. To increase the build’s performance, you can exclude files and directories by adding a .dockerignore file to that directory as well. This file supports exclusion patterns similar to .gitignore files. For information on creating one, see the .dockerignore file.\n\n- Avoid installing unnecessary packages\n\n    In order to reduce complexity, dependencies, file sizes, and build times, you should avoid installing extra or unnecessary packages just because they might be “nice to have.” For example, you don’t need to include a text editor in a database image.\n\n- Run only one process per container\n\n    In almost all cases, you should only run a single process in a single container. Decoupling applications into multiple containers makes it much easier to scale horizontally and reuse containers. If that service depends on another service, make use of container linking.\n\n- Minimize the number of layers\n\n    You need to find the balance between readability (and thus long-term maintainability) of the Dockerfile and minimizing the number of layers it uses. Be strategic and cautious about the number of layers you use.\n\n- Sort multi-line arguments\n\n    Here’s an example from the buildpack-deps image:\n\n    RUN apt-get update && apt-get install -y \\\n      bzr \\\n      cvs \\\n      git \\\n      mercurial \\\n      subversion\n\n- Build cache\n\n# .dockerignore\n\n    */temp*\n    */*/temp*\n    temp?\n","slug":"docker-dockerfile-detail","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-04-18T13:18:25.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rd0001pkxsvh8wpboqk","content":"<h1 id=\"about-dockerfile\"><a href=\"#about-dockerfile\" class=\"headerlink\" title=\"about dockerfile\"></a>about dockerfile</h1><h1 id=\"dockerfile-command\"><a href=\"#dockerfile-command\" class=\"headerlink\" title=\"dockerfile command\"></a>dockerfile command</h1><ul>\n<li><p>point to a Dockerfile anywhere in your file system</p>\n<pre><code>docker build -f /path/to/a/Dockerfile .\n</code></pre></li>\n<li><p>specify a repository and tag at which to save the new image if the build succeeds</p>\n<pre><code>docker build -t shykes/myapp .\n</code></pre></li>\n</ul>\n<h1 id=\"dockerfile-keyword\"><a href=\"#dockerfile-keyword\" class=\"headerlink\" title=\"dockerfile keyword\"></a>dockerfile keyword</h1><ul>\n<li><p>FROM: base image</p>\n</li>\n<li><p>MAINTAINER</p>\n<pre><code>MAINTAINER ag &quot;allengaller@gmail.com&quot;\n</code></pre></li>\n<li><p>USER: set user</p>\n<pre><code>USER root\n</code></pre></li>\n<li><p>RUN: run system cmd</p>\n<pre><code>RUN apt-get update\nRUN [&quot;apt-get&quot;, &quot;update&quot;]\nRUN apt-get install -y nginx\nRUN touch test.txt &amp;&amp; echo &quot;abc&quot; &gt;&gt; abc.txt\n</code></pre></li>\n<li><p>EXPOSE: expose port</p>\n</li>\n<li><p>ADD</p>\n<p>  The ADD instruction copies new files, directories or remote file URLs from <src> and adds them to the filesystem of the container at the path <dest>.</dest></src></p>\n<p>  pattern: ADD <src>… <dest>; ADD [“<src>“,… “<dest>“]<br>  add folder: ADD /webapp /opt/webapp<br>  add file: ADD abc.txt /opt/<br>  add network file: ADD <a href=\"https://www.baidu.com/img/bd_logo1.png\" target=\"_blank\" rel=\"external\">https://www.baidu.com/img/bd_logo1.png</a> /opt/</dest></src></dest></src></p>\n</li>\n<li><p>ENV: set env variable</p>\n<pre><code>ENV WEBAPP_PORT = 9090\n</code></pre></li>\n<li><p>WORKDIR: set working directory</p>\n<p>  The WORKDIR instruction sets the working directory for any RUN, CMD, ENTRYPOINT, COPY and ADD instructions that follow it in the Dockerfile.The WORKDIR instruction can resolve environment variables previously set using ENV. You can only use environment variables explicitly set in the Dockerfile. For example:</p>\n<pre><code>ENV DIRPATH /path\nWORKDIR $DIRPATH/$DIRNAME\nRUN pwd\n</code></pre><p>  The output of the final pwd command in this Dockerfile would be /path/$DIRNAME.It can be used multiple times in the one Dockerfile. If a relative path is provided, it will be relative to the path of the previous WORKDIR instruction. For example:</p>\n<pre><code>WORKDIR /a\nWORKDIR b\nWORKDIR c\nRUN pwd\n</code></pre><p>  The output of the final pwd command in this Dockerfile would be /a/b/c.</p>\n<pre><code>WORKDIR /opt/\n</code></pre></li>\n<li><p>ENTRYPOINT: set boot command, append parameter to boot cmd</p>\n<p>  An ENTRYPOINT allows you to configure a container that will run as an executable.For example, the following will start nginx with its default content, listening on port 80:</p>\n<pre><code>docker run -i -t --rm -p 80:80 nginx\n\nENTRYPOINT [&quot;ls&quot;]\nENTRYPOINT [&quot;ls&quot;]\nCMD [&quot;-l&quot;, &quot;-a&quot;]\n</code></pre></li>\n<li><p>CMD: set boot parameter</p>\n<pre><code>CMD [&quot;ls&quot;, &quot;-a&quot;, &quot;-l&quot;]\nCMD ls -l -a\n</code></pre></li>\n<li><p>VOLUME: set volume</p>\n<pre><code>VOLUME [&quot;/data&quot;, &quot;/var/www&quot;]\n</code></pre></li>\n<li><p>ONBUILD: trigger for child image</p>\n<pre><code>ONBUILD ADD . /app/src\nONBUILD RUN echo &quot;on build excuted&quot; &gt;&gt; onbuild.txt\n</code></pre></li>\n<li><p>ARG</p>\n</li>\n<li><p>STOPSIGNAL</p>\n</li>\n</ul>\n<h1 id=\"best-practice-link\"><a href=\"#best-practice-link\" class=\"headerlink\" title=\"best practice link\"></a>best practice <a href=\"https://docs.docker.com/engine/articles/dockerfile_best-practices/\" target=\"_blank\" rel=\"external\">link</a></h1><ul>\n<li><p>Containers should be ephemeral</p>\n<p>  The container produced by the image your Dockerfile defines should be as ephemeral as possible.By “ephemeral,” we mean that it can be stopped and destroyed and a new one built and put in place with an absolute minimum of set-up and configuration.</p>\n</li>\n<li><p>Use a .dockerignore file</p>\n<p>  In most cases, it’s best to put each Dockerfile in an empty directory. Then, add to that directory only the files needed for building the Dockerfile. To increase the build’s performance, you can exclude files and directories by adding a .dockerignore file to that directory as well. This file supports exclusion patterns similar to .gitignore files. For information on creating one, see the .dockerignore file.</p>\n</li>\n<li><p>Avoid installing unnecessary packages</p>\n<p>  In order to reduce complexity, dependencies, file sizes, and build times, you should avoid installing extra or unnecessary packages just because they might be “nice to have.” For example, you don’t need to include a text editor in a database image.</p>\n</li>\n<li><p>Run only one process per container</p>\n<p>  In almost all cases, you should only run a single process in a single container. Decoupling applications into multiple containers makes it much easier to scale horizontally and reuse containers. If that service depends on another service, make use of container linking.</p>\n</li>\n<li><p>Minimize the number of layers</p>\n<p>  You need to find the balance between readability (and thus long-term maintainability) of the Dockerfile and minimizing the number of layers it uses. Be strategic and cautious about the number of layers you use.</p>\n</li>\n<li><p>Sort multi-line arguments</p>\n<p>  Here’s an example from the buildpack-deps image:</p>\n<p>  RUN apt-get update &amp;&amp; apt-get install -y \\</p>\n<pre><code>bzr \\\ncvs \\\ngit \\\nmercurial \\\nsubversion\n</code></pre></li>\n<li><p>Build cache</p>\n</li>\n</ul>\n<h1 id=\"dockerignore\"><a href=\"#dockerignore\" class=\"headerlink\" title=\".dockerignore\"></a>.dockerignore</h1><pre><code>*/temp*\n*/*/temp*\ntemp?\n</code></pre>","excerpt":"","more":"<h1 id=\"about-dockerfile\"><a href=\"#about-dockerfile\" class=\"headerlink\" title=\"about dockerfile\"></a>about dockerfile</h1><h1 id=\"dockerfile-command\"><a href=\"#dockerfile-command\" class=\"headerlink\" title=\"dockerfile command\"></a>dockerfile command</h1><ul>\n<li><p>point to a Dockerfile anywhere in your file system</p>\n<pre><code>docker build -f /path/to/a/Dockerfile .\n</code></pre></li>\n<li><p>specify a repository and tag at which to save the new image if the build succeeds</p>\n<pre><code>docker build -t shykes/myapp .\n</code></pre></li>\n</ul>\n<h1 id=\"dockerfile-keyword\"><a href=\"#dockerfile-keyword\" class=\"headerlink\" title=\"dockerfile keyword\"></a>dockerfile keyword</h1><ul>\n<li><p>FROM: base image</p>\n</li>\n<li><p>MAINTAINER</p>\n<pre><code>MAINTAINER ag &quot;allengaller@gmail.com&quot;\n</code></pre></li>\n<li><p>USER: set user</p>\n<pre><code>USER root\n</code></pre></li>\n<li><p>RUN: run system cmd</p>\n<pre><code>RUN apt-get update\nRUN [&quot;apt-get&quot;, &quot;update&quot;]\nRUN apt-get install -y nginx\nRUN touch test.txt &amp;&amp; echo &quot;abc&quot; &gt;&gt; abc.txt\n</code></pre></li>\n<li><p>EXPOSE: expose port</p>\n</li>\n<li><p>ADD</p>\n<p>  The ADD instruction copies new files, directories or remote file URLs from <src> and adds them to the filesystem of the container at the path <dest>.</p>\n<p>  pattern: ADD <src>… <dest>; ADD [“<src>“,… “<dest>“]<br>  add folder: ADD /webapp /opt/webapp<br>  add file: ADD abc.txt /opt/<br>  add network file: ADD <a href=\"https://www.baidu.com/img/bd_logo1.png\">https://www.baidu.com/img/bd_logo1.png</a> /opt/</p>\n</li>\n<li><p>ENV: set env variable</p>\n<pre><code>ENV WEBAPP_PORT = 9090\n</code></pre></li>\n<li><p>WORKDIR: set working directory</p>\n<p>  The WORKDIR instruction sets the working directory for any RUN, CMD, ENTRYPOINT, COPY and ADD instructions that follow it in the Dockerfile.The WORKDIR instruction can resolve environment variables previously set using ENV. You can only use environment variables explicitly set in the Dockerfile. For example:</p>\n<pre><code>ENV DIRPATH /path\nWORKDIR $DIRPATH/$DIRNAME\nRUN pwd\n</code></pre><p>  The output of the final pwd command in this Dockerfile would be /path/$DIRNAME.It can be used multiple times in the one Dockerfile. If a relative path is provided, it will be relative to the path of the previous WORKDIR instruction. For example:</p>\n<pre><code>WORKDIR /a\nWORKDIR b\nWORKDIR c\nRUN pwd\n</code></pre><p>  The output of the final pwd command in this Dockerfile would be /a/b/c.</p>\n<pre><code>WORKDIR /opt/\n</code></pre></li>\n<li><p>ENTRYPOINT: set boot command, append parameter to boot cmd</p>\n<p>  An ENTRYPOINT allows you to configure a container that will run as an executable.For example, the following will start nginx with its default content, listening on port 80:</p>\n<pre><code>docker run -i -t --rm -p 80:80 nginx\n\nENTRYPOINT [&quot;ls&quot;]\nENTRYPOINT [&quot;ls&quot;]\nCMD [&quot;-l&quot;, &quot;-a&quot;]\n</code></pre></li>\n<li><p>CMD: set boot parameter</p>\n<pre><code>CMD [&quot;ls&quot;, &quot;-a&quot;, &quot;-l&quot;]\nCMD ls -l -a\n</code></pre></li>\n<li><p>VOLUME: set volume</p>\n<pre><code>VOLUME [&quot;/data&quot;, &quot;/var/www&quot;]\n</code></pre></li>\n<li><p>ONBUILD: trigger for child image</p>\n<pre><code>ONBUILD ADD . /app/src\nONBUILD RUN echo &quot;on build excuted&quot; &gt;&gt; onbuild.txt\n</code></pre></li>\n<li><p>ARG</p>\n</li>\n<li><p>STOPSIGNAL</p>\n</li>\n</ul>\n<h1 id=\"best-practice-link\"><a href=\"#best-practice-link\" class=\"headerlink\" title=\"best practice link\"></a>best practice <a href=\"https://docs.docker.com/engine/articles/dockerfile_best-practices/\">link</a></h1><ul>\n<li><p>Containers should be ephemeral</p>\n<p>  The container produced by the image your Dockerfile defines should be as ephemeral as possible.By “ephemeral,” we mean that it can be stopped and destroyed and a new one built and put in place with an absolute minimum of set-up and configuration.</p>\n</li>\n<li><p>Use a .dockerignore file</p>\n<p>  In most cases, it’s best to put each Dockerfile in an empty directory. Then, add to that directory only the files needed for building the Dockerfile. To increase the build’s performance, you can exclude files and directories by adding a .dockerignore file to that directory as well. This file supports exclusion patterns similar to .gitignore files. For information on creating one, see the .dockerignore file.</p>\n</li>\n<li><p>Avoid installing unnecessary packages</p>\n<p>  In order to reduce complexity, dependencies, file sizes, and build times, you should avoid installing extra or unnecessary packages just because they might be “nice to have.” For example, you don’t need to include a text editor in a database image.</p>\n</li>\n<li><p>Run only one process per container</p>\n<p>  In almost all cases, you should only run a single process in a single container. Decoupling applications into multiple containers makes it much easier to scale horizontally and reuse containers. If that service depends on another service, make use of container linking.</p>\n</li>\n<li><p>Minimize the number of layers</p>\n<p>  You need to find the balance between readability (and thus long-term maintainability) of the Dockerfile and minimizing the number of layers it uses. Be strategic and cautious about the number of layers you use.</p>\n</li>\n<li><p>Sort multi-line arguments</p>\n<p>  Here’s an example from the buildpack-deps image:</p>\n<p>  RUN apt-get update &amp;&amp; apt-get install -y \\</p>\n<pre><code>bzr \\\ncvs \\\ngit \\\nmercurial \\\nsubversion\n</code></pre></li>\n<li><p>Build cache</p>\n</li>\n</ul>\n<h1 id=\"dockerignore\"><a href=\"#dockerignore\" class=\"headerlink\" title=\".dockerignore\"></a>.dockerignore</h1><pre><code>*/temp*\n*/*/temp*\ntemp?\n</code></pre>"},{"title":"dockerhub","_content":"\n# dockerhub\n\n## about dockerhub [link](https://hub.docker.com)\n\n- types:\n\n        official image\n        user image\n\n## command\n\n- docker login\n\n        comfig: cat ~/.dockercfg\n\n- build: Automated Build/Trusted Build\n    \n- registry\n\n        docker pull registry\n        docker run -p 5000:5000 -d -i -t registry\n        docker commit 3ie9djk 127.0.0.1:5000/my_image:v1\n            [registry_host: registry_port\\image_name:image_tag]\n        docker push 127.0.0.1:5000/my_image:v1","source":"_posts/docker-dockerhub-detail.md","raw":"---\ntitle: dockerhub\ncategories:\n- docker\ntags:\n- detail\n---\n\n# dockerhub\n\n## about dockerhub [link](https://hub.docker.com)\n\n- types:\n\n        official image\n        user image\n\n## command\n\n- docker login\n\n        comfig: cat ~/.dockercfg\n\n- build: Automated Build/Trusted Build\n    \n- registry\n\n        docker pull registry\n        docker run -p 5000:5000 -d -i -t registry\n        docker commit 3ie9djk 127.0.0.1:5000/my_image:v1\n            [registry_host: registry_port\\image_name:image_tag]\n        docker push 127.0.0.1:5000/my_image:v1","slug":"docker-dockerhub-detail","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-13T11:46:34.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rd6001tkxsv2lkkphnm","content":"<h1 id=\"dockerhub\"><a href=\"#dockerhub\" class=\"headerlink\" title=\"dockerhub\"></a>dockerhub</h1><h2 id=\"about-dockerhub-link\"><a href=\"#about-dockerhub-link\" class=\"headerlink\" title=\"about dockerhub link\"></a>about dockerhub <a href=\"https://hub.docker.com\" target=\"_blank\" rel=\"external\">link</a></h2><ul>\n<li><p>types:</p>\n<pre><code>official image\nuser image\n</code></pre></li>\n</ul>\n<h2 id=\"command\"><a href=\"#command\" class=\"headerlink\" title=\"command\"></a>command</h2><ul>\n<li><p>docker login</p>\n<pre><code>comfig: cat ~/.dockercfg\n</code></pre></li>\n<li><p>build: Automated Build/Trusted Build</p>\n</li>\n<li><p>registry</p>\n<pre><code>docker pull registry\ndocker run -p 5000:5000 -d -i -t registry\ndocker commit 3ie9djk 127.0.0.1:5000/my_image:v1\n    [registry_host: registry_port\\image_name:image_tag]\ndocker push 127.0.0.1:5000/my_image:v1\n</code></pre></li>\n</ul>\n","excerpt":"","more":"<h1 id=\"dockerhub\"><a href=\"#dockerhub\" class=\"headerlink\" title=\"dockerhub\"></a>dockerhub</h1><h2 id=\"about-dockerhub-link\"><a href=\"#about-dockerhub-link\" class=\"headerlink\" title=\"about dockerhub link\"></a>about dockerhub <a href=\"https://hub.docker.com\">link</a></h2><ul>\n<li><p>types:</p>\n<pre><code>official image\nuser image\n</code></pre></li>\n</ul>\n<h2 id=\"command\"><a href=\"#command\" class=\"headerlink\" title=\"command\"></a>command</h2><ul>\n<li><p>docker login</p>\n<pre><code>comfig: cat ~/.dockercfg\n</code></pre></li>\n<li><p>build: Automated Build/Trusted Build</p>\n</li>\n<li><p>registry</p>\n<pre><code>docker pull registry\ndocker run -p 5000:5000 -d -i -t registry\ndocker commit 3ie9djk 127.0.0.1:5000/my_image:v1\n    [registry_host: registry_port\\image_name:image_tag]\ndocker push 127.0.0.1:5000/my_image:v1\n</code></pre></li>\n</ul>\n"},{"title":"docker engine","_content":"\n## about\nsource code: https://github.com/docker/docker\ninstallation: https://www.docker.com/products/overview\n\n## docker engine command\n\n- tips\n\n    Delete all containers:\n        \n        docker rm $(docker ps -a -q)\n\n    Delete all images:\n        \n        docker rmi $(docker images -q)\n\n- env\n\n    - info\n    \n    - version\n\n- life-cycle\n\n    - create:  (ini: stop)\n\n            --restart: check for exit code then restart container; always or on-failure; on-failure:5 restart 5 times max\n\n            sudo docker run --restart=always --name docker_restart -d ubuntu /bin/sh -c \"while true;do echo hello world;sleep 1;done\"\n\n    - exec: exec cmd insid container\n\n            sudo docker exec -d daemon_dave touch /etc/new_config_file\n            sudo docker exec -t -i daemon_dave /bin/bash\n\n    - kill: send SIGKILL signal to container process\n    \n    - pause\n\n    - restart\n\n    - rm: cannot remove a running container; docker stop or kill first or docker rm -f bad_ubuntu\n        \n        -q: list only container ids;\n        delete all container at once:\n            \n            docker rm `docker ps -a -q`\n\n    - run: [reference](https://docs.docker.com/engine/reference/run/);(ini: run); \n\n        equals: docker create & docker start\n\n        2 types of container\n        - interactive\n            -i: STDIN\n            -t: open terminal\n            exit?: docker stop or kill;exit\n\n                sudo docker run -i -t --name=inspect_shell ubuntu /bin/bash\n                inspect_shell: container name\n                base image: ubuntu\n                command: /bin/bash\n                file system: image+writable layer\n                network: virtual network interface bridge to host & set a IP\n        \n        - daemon: -d\n            exit?: docker stop or kill\n            \n                sudo docker run --name daemon_while -d ubuntu /bin/sh -c \"while true; do echo hello world; sleep 1; done\"\n            \n                return token\n            \n                docker ps\n\n    - start: start existing container\n            \n            sudo docker start inspect_shell or cid\n\n    - stop: works for both interactive and daemon container;send SIGTERM signal to container process\n\n            sudo docker stop daemon_while\n            sudo docker stop s39c938dj34489d\n        \n    - unpause\n\n- registry\n\n    - login\n    \n    - logout\n    \n    - pull\n    \n    - push\n    \n    - search\n\n- image\n\n    - build\n    \n    - images\n    \n    - import:             \n\n            cat my_container.rar | sudo docker import - imported:container\n            repository: imported, tag: container\n            docker import url res:tag\n\n    - load\n    \n    - rmi\n    \n    - save\n    \n    - tag\n    \n    - commit\n\n- container\n\n    - attach: attach terminal to interactive container\n    \n    - export\n\n            sudo docker run -i -t --nam=inspect_import ubuntu /bin/bash\n            #... do something\n            sudo docker export inspect_import > my_container.tar\n\n    - inspect: check out the configuration\n\n            sudo docker inspect daemon_dave\n\n        -f or --format:\n\n            sudo docker inspect --format='{{ .State.Running }}' daemon_dave\n    \n    - port\n    \n    - ps: checkout existing container\n\n        -a: all\n            Exited(0): exit\n        -l: latest container\n        -n=x: latest x container\n\n    - rename\n    \n    - stats\n    \n    - top: check out UID PID PPID...\n        \n            sudo docker run -d --name=\"daemon_top\" ubuntu /bin/bash -c 'while true;do sleep 1;done'\n        \n        2 process:\n        \n            sudo docker top daemon_top\n\n    - wait\n    \n    - cp\n    \n    - diff\n\n- sys log\n\n    - events\n    \n    - history\n    \n    - logs\n\n        -f: realtime\n        --tail=x: last x line\n                \n            sudo docker logs -f --tail=5 -t daemon_logs\n\n- other\n\n    - docker daemon: [link](https://docs.docker.com/engine/reference/commandline/daemon/), A self-sufficient runtime for linux containers.\n            \n        The Docker daemon can listen for Docker Remote API requests via three different types of Socket: unix, tcp, and fd.\n        By default, a unix domain socket (or IPC socket) is created at /var/run/docker.sock, requiring either root permission, or docker group membership.","source":"_posts/docker-engine-detail.md","raw":"---\ntitle: docker engine\ncategories:\n- docker\ntags:\n- detail\n---\n\n## about\nsource code: https://github.com/docker/docker\ninstallation: https://www.docker.com/products/overview\n\n## docker engine command\n\n- tips\n\n    Delete all containers:\n        \n        docker rm $(docker ps -a -q)\n\n    Delete all images:\n        \n        docker rmi $(docker images -q)\n\n- env\n\n    - info\n    \n    - version\n\n- life-cycle\n\n    - create:  (ini: stop)\n\n            --restart: check for exit code then restart container; always or on-failure; on-failure:5 restart 5 times max\n\n            sudo docker run --restart=always --name docker_restart -d ubuntu /bin/sh -c \"while true;do echo hello world;sleep 1;done\"\n\n    - exec: exec cmd insid container\n\n            sudo docker exec -d daemon_dave touch /etc/new_config_file\n            sudo docker exec -t -i daemon_dave /bin/bash\n\n    - kill: send SIGKILL signal to container process\n    \n    - pause\n\n    - restart\n\n    - rm: cannot remove a running container; docker stop or kill first or docker rm -f bad_ubuntu\n        \n        -q: list only container ids;\n        delete all container at once:\n            \n            docker rm `docker ps -a -q`\n\n    - run: [reference](https://docs.docker.com/engine/reference/run/);(ini: run); \n\n        equals: docker create & docker start\n\n        2 types of container\n        - interactive\n            -i: STDIN\n            -t: open terminal\n            exit?: docker stop or kill;exit\n\n                sudo docker run -i -t --name=inspect_shell ubuntu /bin/bash\n                inspect_shell: container name\n                base image: ubuntu\n                command: /bin/bash\n                file system: image+writable layer\n                network: virtual network interface bridge to host & set a IP\n        \n        - daemon: -d\n            exit?: docker stop or kill\n            \n                sudo docker run --name daemon_while -d ubuntu /bin/sh -c \"while true; do echo hello world; sleep 1; done\"\n            \n                return token\n            \n                docker ps\n\n    - start: start existing container\n            \n            sudo docker start inspect_shell or cid\n\n    - stop: works for both interactive and daemon container;send SIGTERM signal to container process\n\n            sudo docker stop daemon_while\n            sudo docker stop s39c938dj34489d\n        \n    - unpause\n\n- registry\n\n    - login\n    \n    - logout\n    \n    - pull\n    \n    - push\n    \n    - search\n\n- image\n\n    - build\n    \n    - images\n    \n    - import:             \n\n            cat my_container.rar | sudo docker import - imported:container\n            repository: imported, tag: container\n            docker import url res:tag\n\n    - load\n    \n    - rmi\n    \n    - save\n    \n    - tag\n    \n    - commit\n\n- container\n\n    - attach: attach terminal to interactive container\n    \n    - export\n\n            sudo docker run -i -t --nam=inspect_import ubuntu /bin/bash\n            #... do something\n            sudo docker export inspect_import > my_container.tar\n\n    - inspect: check out the configuration\n\n            sudo docker inspect daemon_dave\n\n        -f or --format:\n\n            sudo docker inspect --format='{{ .State.Running }}' daemon_dave\n    \n    - port\n    \n    - ps: checkout existing container\n\n        -a: all\n            Exited(0): exit\n        -l: latest container\n        -n=x: latest x container\n\n    - rename\n    \n    - stats\n    \n    - top: check out UID PID PPID...\n        \n            sudo docker run -d --name=\"daemon_top\" ubuntu /bin/bash -c 'while true;do sleep 1;done'\n        \n        2 process:\n        \n            sudo docker top daemon_top\n\n    - wait\n    \n    - cp\n    \n    - diff\n\n- sys log\n\n    - events\n    \n    - history\n    \n    - logs\n\n        -f: realtime\n        --tail=x: last x line\n                \n            sudo docker logs -f --tail=5 -t daemon_logs\n\n- other\n\n    - docker daemon: [link](https://docs.docker.com/engine/reference/commandline/daemon/), A self-sufficient runtime for linux containers.\n            \n        The Docker daemon can listen for Docker Remote API requests via three different types of Socket: unix, tcp, and fd.\n        By default, a unix domain socket (or IPC socket) is created at /var/run/docker.sock, requiring either root permission, or docker group membership.","slug":"docker-engine-detail","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-02-10T05:34:03.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rd9001wkxsv37rjqnk0","content":"<h2 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h2><p>source code: <a href=\"https://github.com/docker/docker\" target=\"_blank\" rel=\"external\">https://github.com/docker/docker</a><br>installation: <a href=\"https://www.docker.com/products/overview\" target=\"_blank\" rel=\"external\">https://www.docker.com/products/overview</a></p>\n<h2 id=\"docker-engine-command\"><a href=\"#docker-engine-command\" class=\"headerlink\" title=\"docker engine command\"></a>docker engine command</h2><ul>\n<li><p>tips</p>\n<p>  Delete all containers:</p>\n<pre><code>docker rm $(docker ps -a -q)\n</code></pre><p>  Delete all images:</p>\n<pre><code>docker rmi $(docker images -q)\n</code></pre></li>\n<li><p>env</p>\n<ul>\n<li><p>info</p>\n</li>\n<li><p>version</p>\n</li>\n</ul>\n</li>\n<li><p>life-cycle</p>\n<ul>\n<li><p>create:  (ini: stop)</p>\n<pre><code>--restart: check for exit code then restart container; always or on-failure; on-failure:5 restart 5 times max\n\nsudo docker run --restart=always --name docker_restart -d ubuntu /bin/sh -c &quot;while true;do echo hello world;sleep 1;done&quot;\n</code></pre></li>\n<li><p>exec: exec cmd insid container</p>\n<pre><code>sudo docker exec -d daemon_dave touch /etc/new_config_file\nsudo docker exec -t -i daemon_dave /bin/bash\n</code></pre></li>\n<li><p>kill: send SIGKILL signal to container process</p>\n</li>\n<li><p>pause</p>\n</li>\n<li><p>restart</p>\n</li>\n<li><p>rm: cannot remove a running container; docker stop or kill first or docker rm -f bad_ubuntu</p>\n<p>  -q: list only container ids;<br>  delete all container at once:</p>\n<pre><code>docker rm `docker ps -a -q`\n</code></pre></li>\n<li><p>run: <a href=\"https://docs.docker.com/engine/reference/run/\" target=\"_blank\" rel=\"external\">reference</a>;(ini: run); </p>\n<p>  equals: docker create &amp; docker start</p>\n<p>  2 types of container</p>\n<ul>\n<li><p>interactive<br>  -i: STDIN<br>  -t: open terminal<br>  exit?: docker stop or kill;exit</p>\n<pre><code>sudo docker run -i -t --name=inspect_shell ubuntu /bin/bash\ninspect_shell: container name\nbase image: ubuntu\ncommand: /bin/bash\nfile system: image+writable layer\nnetwork: virtual network interface bridge to host &amp; set a IP\n</code></pre></li>\n<li><p>daemon: -d<br>  exit?: docker stop or kill</p>\n<pre><code>sudo docker run --name daemon_while -d ubuntu /bin/sh -c &quot;while true; do echo hello world; sleep 1; done&quot;\n\nreturn token\n\ndocker ps\n</code></pre></li>\n</ul>\n</li>\n<li><p>start: start existing container</p>\n<pre><code>sudo docker start inspect_shell or cid\n</code></pre></li>\n<li><p>stop: works for both interactive and daemon container;send SIGTERM signal to container process</p>\n<pre><code>sudo docker stop daemon_while\nsudo docker stop s39c938dj34489d\n</code></pre></li>\n<li><p>unpause</p>\n</li>\n</ul>\n</li>\n<li><p>registry</p>\n<ul>\n<li><p>login</p>\n</li>\n<li><p>logout</p>\n</li>\n<li><p>pull</p>\n</li>\n<li><p>push</p>\n</li>\n<li><p>search</p>\n</li>\n</ul>\n</li>\n<li><p>image</p>\n<ul>\n<li><p>build</p>\n</li>\n<li><p>images</p>\n</li>\n<li><p>import:             </p>\n<pre><code>cat my_container.rar | sudo docker import - imported:container\nrepository: imported, tag: container\ndocker import url res:tag\n</code></pre></li>\n<li><p>load</p>\n</li>\n<li><p>rmi</p>\n</li>\n<li><p>save</p>\n</li>\n<li><p>tag</p>\n</li>\n<li><p>commit</p>\n</li>\n</ul>\n</li>\n<li><p>container</p>\n<ul>\n<li><p>attach: attach terminal to interactive container</p>\n</li>\n<li><p>export</p>\n<pre><code>sudo docker run -i -t --nam=inspect_import ubuntu /bin/bash\n#... do something\nsudo docker export inspect_import &gt; my_container.tar\n</code></pre></li>\n<li><p>inspect: check out the configuration</p>\n<pre><code>sudo docker inspect daemon_dave\n</code></pre><p>  -f or –format:</p>\n<pre><code>sudo docker inspect --format=&apos;{{ .State.Running }}&apos; daemon_dave\n</code></pre></li>\n<li><p>port</p>\n</li>\n<li><p>ps: checkout existing container</p>\n<p>  -a: all</p>\n<pre><code>Exited(0): exit\n</code></pre><p>  -l: latest container<br>  -n=x: latest x container</p>\n</li>\n<li><p>rename</p>\n</li>\n<li><p>stats</p>\n</li>\n<li><p>top: check out UID PID PPID…</p>\n<pre><code>sudo docker run -d --name=&quot;daemon_top&quot; ubuntu /bin/bash -c &apos;while true;do sleep 1;done&apos;\n</code></pre><p>  2 process:</p>\n<pre><code>sudo docker top daemon_top\n</code></pre></li>\n<li><p>wait</p>\n</li>\n<li><p>cp</p>\n</li>\n<li><p>diff</p>\n</li>\n</ul>\n</li>\n<li><p>sys log</p>\n<ul>\n<li><p>events</p>\n</li>\n<li><p>history</p>\n</li>\n<li><p>logs</p>\n<p>  -f: realtime<br>  –tail=x: last x line</p>\n<pre><code>sudo docker logs -f --tail=5 -t daemon_logs\n</code></pre></li>\n</ul>\n</li>\n<li><p>other</p>\n<ul>\n<li><p>docker daemon: <a href=\"https://docs.docker.com/engine/reference/commandline/daemon/\" target=\"_blank\" rel=\"external\">link</a>, A self-sufficient runtime for linux containers.</p>\n<p>  The Docker daemon can listen for Docker Remote API requests via three different types of Socket: unix, tcp, and fd.<br>  By default, a unix domain socket (or IPC socket) is created at /var/run/docker.sock, requiring either root permission, or docker group membership.</p>\n</li>\n</ul>\n</li>\n</ul>\n","excerpt":"","more":"<h2 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h2><p>source code: <a href=\"https://github.com/docker/docker\">https://github.com/docker/docker</a><br>installation: <a href=\"https://www.docker.com/products/overview\">https://www.docker.com/products/overview</a></p>\n<h2 id=\"docker-engine-command\"><a href=\"#docker-engine-command\" class=\"headerlink\" title=\"docker engine command\"></a>docker engine command</h2><ul>\n<li><p>tips</p>\n<p>  Delete all containers:</p>\n<pre><code>docker rm $(docker ps -a -q)\n</code></pre><p>  Delete all images:</p>\n<pre><code>docker rmi $(docker images -q)\n</code></pre></li>\n<li><p>env</p>\n<ul>\n<li><p>info</p>\n</li>\n<li><p>version</p>\n</li>\n</ul>\n</li>\n<li><p>life-cycle</p>\n<ul>\n<li><p>create:  (ini: stop)</p>\n<pre><code>--restart: check for exit code then restart container; always or on-failure; on-failure:5 restart 5 times max\n\nsudo docker run --restart=always --name docker_restart -d ubuntu /bin/sh -c &quot;while true;do echo hello world;sleep 1;done&quot;\n</code></pre></li>\n<li><p>exec: exec cmd insid container</p>\n<pre><code>sudo docker exec -d daemon_dave touch /etc/new_config_file\nsudo docker exec -t -i daemon_dave /bin/bash\n</code></pre></li>\n<li><p>kill: send SIGKILL signal to container process</p>\n</li>\n<li><p>pause</p>\n</li>\n<li><p>restart</p>\n</li>\n<li><p>rm: cannot remove a running container; docker stop or kill first or docker rm -f bad_ubuntu</p>\n<p>  -q: list only container ids;<br>  delete all container at once:</p>\n<pre><code>docker rm `docker ps -a -q`\n</code></pre></li>\n<li><p>run: <a href=\"https://docs.docker.com/engine/reference/run/\">reference</a>;(ini: run); </p>\n<p>  equals: docker create &amp; docker start</p>\n<p>  2 types of container</p>\n<ul>\n<li><p>interactive<br>  -i: STDIN<br>  -t: open terminal<br>  exit?: docker stop or kill;exit</p>\n<pre><code>sudo docker run -i -t --name=inspect_shell ubuntu /bin/bash\ninspect_shell: container name\nbase image: ubuntu\ncommand: /bin/bash\nfile system: image+writable layer\nnetwork: virtual network interface bridge to host &amp; set a IP\n</code></pre></li>\n<li><p>daemon: -d<br>  exit?: docker stop or kill</p>\n<pre><code>sudo docker run --name daemon_while -d ubuntu /bin/sh -c &quot;while true; do echo hello world; sleep 1; done&quot;\n\nreturn token\n\ndocker ps\n</code></pre></li>\n</ul>\n</li>\n<li><p>start: start existing container</p>\n<pre><code>sudo docker start inspect_shell or cid\n</code></pre></li>\n<li><p>stop: works for both interactive and daemon container;send SIGTERM signal to container process</p>\n<pre><code>sudo docker stop daemon_while\nsudo docker stop s39c938dj34489d\n</code></pre></li>\n<li><p>unpause</p>\n</li>\n</ul>\n</li>\n<li><p>registry</p>\n<ul>\n<li><p>login</p>\n</li>\n<li><p>logout</p>\n</li>\n<li><p>pull</p>\n</li>\n<li><p>push</p>\n</li>\n<li><p>search</p>\n</li>\n</ul>\n</li>\n<li><p>image</p>\n<ul>\n<li><p>build</p>\n</li>\n<li><p>images</p>\n</li>\n<li><p>import:             </p>\n<pre><code>cat my_container.rar | sudo docker import - imported:container\nrepository: imported, tag: container\ndocker import url res:tag\n</code></pre></li>\n<li><p>load</p>\n</li>\n<li><p>rmi</p>\n</li>\n<li><p>save</p>\n</li>\n<li><p>tag</p>\n</li>\n<li><p>commit</p>\n</li>\n</ul>\n</li>\n<li><p>container</p>\n<ul>\n<li><p>attach: attach terminal to interactive container</p>\n</li>\n<li><p>export</p>\n<pre><code>sudo docker run -i -t --nam=inspect_import ubuntu /bin/bash\n#... do something\nsudo docker export inspect_import &gt; my_container.tar\n</code></pre></li>\n<li><p>inspect: check out the configuration</p>\n<pre><code>sudo docker inspect daemon_dave\n</code></pre><p>  -f or –format:</p>\n<pre><code>sudo docker inspect --format=&apos;{{ .State.Running }}&apos; daemon_dave\n</code></pre></li>\n<li><p>port</p>\n</li>\n<li><p>ps: checkout existing container</p>\n<p>  -a: all</p>\n<pre><code>Exited(0): exit\n</code></pre><p>  -l: latest container<br>  -n=x: latest x container</p>\n</li>\n<li><p>rename</p>\n</li>\n<li><p>stats</p>\n</li>\n<li><p>top: check out UID PID PPID…</p>\n<pre><code>sudo docker run -d --name=&quot;daemon_top&quot; ubuntu /bin/bash -c &apos;while true;do sleep 1;done&apos;\n</code></pre><p>  2 process:</p>\n<pre><code>sudo docker top daemon_top\n</code></pre></li>\n<li><p>wait</p>\n</li>\n<li><p>cp</p>\n</li>\n<li><p>diff</p>\n</li>\n</ul>\n</li>\n<li><p>sys log</p>\n<ul>\n<li><p>events</p>\n</li>\n<li><p>history</p>\n</li>\n<li><p>logs</p>\n<p>  -f: realtime<br>  –tail=x: last x line</p>\n<pre><code>sudo docker logs -f --tail=5 -t daemon_logs\n</code></pre></li>\n</ul>\n</li>\n<li><p>other</p>\n<ul>\n<li><p>docker daemon: <a href=\"https://docs.docker.com/engine/reference/commandline/daemon/\">link</a>, A self-sufficient runtime for linux containers.</p>\n<p>  The Docker daemon can listen for Docker Remote API requests via three different types of Socket: unix, tcp, and fd.<br>  By default, a unix domain socket (or IPC socket) is created at /var/run/docker.sock, requiring either root permission, or docker group membership.</p>\n</li>\n</ul>\n</li>\n</ul>\n"},{"title":"docker cookbook","_content":"\n# docker cookbook","source":"_posts/docker-cookbook.md","raw":"---\ntitle: docker cookbook\ncategories:\n- docker\ntags:\n- cookbook\n---\n\n# docker cookbook","slug":"docker-cookbook","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-13T10:51:59.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rdc0020kxsva8eeadnz","content":"<h1 id=\"docker-cookbook\"><a href=\"#docker-cookbook\" class=\"headerlink\" title=\"docker cookbook\"></a>docker cookbook</h1>","excerpt":"","more":"<h1 id=\"docker-cookbook\"><a href=\"#docker-cookbook\" class=\"headerlink\" title=\"docker cookbook\"></a>docker cookbook</h1>"},{"title":"docker core","_content":"\n# about docker\n\n\n# basic concept\n\n- [dockerfile]()\n- [image]()\n- [yaml]()\n- [file system]()\n\n- [dockerhub]()\n\n# basic component\n\n- [docker engine]()\n- [docker compose]()\n- [docker swarm]()\n\n# basic topic\n\n- [storage]()\n- [network]()\n- [security]()","source":"_posts/docker-core.md","raw":"---\ntitle: docker core\ncategories:\n- docker\ntags:\n- core\n---\n\n# about docker\n\n\n# basic concept\n\n- [dockerfile]()\n- [image]()\n- [yaml]()\n- [file system]()\n\n- [dockerhub]()\n\n# basic component\n\n- [docker engine]()\n- [docker compose]()\n- [docker swarm]()\n\n# basic topic\n\n- [storage]()\n- [network]()\n- [security]()","slug":"docker-core","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-15T06:28:13.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rdf0022kxsvx69sj1cg","content":"<h1 id=\"about-docker\"><a href=\"#about-docker\" class=\"headerlink\" title=\"about docker\"></a>about docker</h1><h1 id=\"basic-concept\"><a href=\"#basic-concept\" class=\"headerlink\" title=\"basic concept\"></a>basic concept</h1><ul>\n<li><a href=\"\">dockerfile</a></li>\n<li><a href=\"\">image</a></li>\n<li><a href=\"\">yaml</a></li>\n<li><p><a href=\"\">file system</a></p>\n</li>\n<li><p><a href=\"\">dockerhub</a></p>\n</li>\n</ul>\n<h1 id=\"basic-component\"><a href=\"#basic-component\" class=\"headerlink\" title=\"basic component\"></a>basic component</h1><ul>\n<li><a href=\"\">docker engine</a></li>\n<li><a href=\"\">docker compose</a></li>\n<li><a href=\"\">docker swarm</a></li>\n</ul>\n<h1 id=\"basic-topic\"><a href=\"#basic-topic\" class=\"headerlink\" title=\"basic topic\"></a>basic topic</h1><ul>\n<li><a href=\"\">storage</a></li>\n<li><a href=\"\">network</a></li>\n<li><a href=\"\">security</a></li>\n</ul>\n","excerpt":"","more":"<h1 id=\"about-docker\"><a href=\"#about-docker\" class=\"headerlink\" title=\"about docker\"></a>about docker</h1><h1 id=\"basic-concept\"><a href=\"#basic-concept\" class=\"headerlink\" title=\"basic concept\"></a>basic concept</h1><ul>\n<li><a href=\"\">dockerfile</a></li>\n<li><a href=\"\">image</a></li>\n<li><a href=\"\">yaml</a></li>\n<li><p><a href=\"\">file system</a></p>\n</li>\n<li><p><a href=\"\">dockerhub</a></p>\n</li>\n</ul>\n<h1 id=\"basic-component\"><a href=\"#basic-component\" class=\"headerlink\" title=\"basic component\"></a>basic component</h1><ul>\n<li><a href=\"\">docker engine</a></li>\n<li><a href=\"\">docker compose</a></li>\n<li><a href=\"\">docker swarm</a></li>\n</ul>\n<h1 id=\"basic-topic\"><a href=\"#basic-topic\" class=\"headerlink\" title=\"basic topic\"></a>basic topic</h1><ul>\n<li><a href=\"\">storage</a></li>\n<li><a href=\"\">network</a></li>\n<li><a href=\"\">security</a></li>\n</ul>\n"},{"title":"docker filesystem","_content":"\n# about\n\n\n# AuFS\n\n- about\n    \n    http://aufs.sourceforge.net/\n    \n    layered file system\n    \n    AuFS is a layered file system, so you can have a read only part, and a write part, and merge those together. So you could have the common parts of the operating system as read only, which are shared amongst all of your containers, and then give each container its own mount for writing.So let's say you have a container image that is 1GB in size. If you wanted to use a Full VM, you would need to have 1GB times x number of VMs you want. With LXC and AuFS you can share the bulk of the 1GB and if you have 1000 containers you still might only have a little over 1GB of space for the containers OS, assuming they are all running the same OS image.\n\n# ceph\n\n- about\n\n    一个 Linux PB 级分布式文件系统\n    Ceph is a distributed object store and file system designed to provide excellent performance, reliability and scalability.\n\n- feature\n\n    - 以时间复杂度为O(1)的方式提供消息持久化能力，即使对TB级以上数据也能保证常数时间复杂度的访问性能。\n    - 高吞吐率。即使在非常廉价的商用机器上也能做到单机支持每秒100K条以上消息的传输。\n    - 支持Kafka Server间的消息分区，及分布式消费，同时保证每个Partition内的消息顺序传输。\n    - 同时支持离线数据处理和实时数据处理。\n    - Scale out：支持在线水平扩展。\n\n- resource\n\n    link： http://ceph.com/\n    doc： http://docs.openfans.org/ceph\n\n- core\n\n- components\n    \n    cluster monitors\n    clients\n    metadata server cluster\n    object storage cluster\n\n# overlayfs\n\n- link： https://www.kernel.org/doc/Documentation/filesystems/overlayfs.txt","source":"_posts/docker-filesystem-core.md","raw":"---\ntitle: docker filesystem\ncategories:\n- docker\ntags:\n- core\n- filesystem\n---\n\n# about\n\n\n# AuFS\n\n- about\n    \n    http://aufs.sourceforge.net/\n    \n    layered file system\n    \n    AuFS is a layered file system, so you can have a read only part, and a write part, and merge those together. So you could have the common parts of the operating system as read only, which are shared amongst all of your containers, and then give each container its own mount for writing.So let's say you have a container image that is 1GB in size. If you wanted to use a Full VM, you would need to have 1GB times x number of VMs you want. With LXC and AuFS you can share the bulk of the 1GB and if you have 1000 containers you still might only have a little over 1GB of space for the containers OS, assuming they are all running the same OS image.\n\n# ceph\n\n- about\n\n    一个 Linux PB 级分布式文件系统\n    Ceph is a distributed object store and file system designed to provide excellent performance, reliability and scalability.\n\n- feature\n\n    - 以时间复杂度为O(1)的方式提供消息持久化能力，即使对TB级以上数据也能保证常数时间复杂度的访问性能。\n    - 高吞吐率。即使在非常廉价的商用机器上也能做到单机支持每秒100K条以上消息的传输。\n    - 支持Kafka Server间的消息分区，及分布式消费，同时保证每个Partition内的消息顺序传输。\n    - 同时支持离线数据处理和实时数据处理。\n    - Scale out：支持在线水平扩展。\n\n- resource\n\n    link： http://ceph.com/\n    doc： http://docs.openfans.org/ceph\n\n- core\n\n- components\n    \n    cluster monitors\n    clients\n    metadata server cluster\n    object storage cluster\n\n# overlayfs\n\n- link： https://www.kernel.org/doc/Documentation/filesystems/overlayfs.txt","slug":"docker-filesystem-core","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-15T07:30:20.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rdj0027kxsvav51iztu","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><h1 id=\"AuFS\"><a href=\"#AuFS\" class=\"headerlink\" title=\"AuFS\"></a>AuFS</h1><ul>\n<li><p>about</p>\n<p>  <a href=\"http://aufs.sourceforge.net/\" target=\"_blank\" rel=\"external\">http://aufs.sourceforge.net/</a></p>\n<p>  layered file system</p>\n<p>  AuFS is a layered file system, so you can have a read only part, and a write part, and merge those together. So you could have the common parts of the operating system as read only, which are shared amongst all of your containers, and then give each container its own mount for writing.So let’s say you have a container image that is 1GB in size. If you wanted to use a Full VM, you would need to have 1GB times x number of VMs you want. With LXC and AuFS you can share the bulk of the 1GB and if you have 1000 containers you still might only have a little over 1GB of space for the containers OS, assuming they are all running the same OS image.</p>\n</li>\n</ul>\n<h1 id=\"ceph\"><a href=\"#ceph\" class=\"headerlink\" title=\"ceph\"></a>ceph</h1><ul>\n<li><p>about</p>\n<p>  一个 Linux PB 级分布式文件系统<br>  Ceph is a distributed object store and file system designed to provide excellent performance, reliability and scalability.</p>\n</li>\n<li><p>feature</p>\n<ul>\n<li>以时间复杂度为O(1)的方式提供消息持久化能力，即使对TB级以上数据也能保证常数时间复杂度的访问性能。</li>\n<li>高吞吐率。即使在非常廉价的商用机器上也能做到单机支持每秒100K条以上消息的传输。</li>\n<li>支持Kafka Server间的消息分区，及分布式消费，同时保证每个Partition内的消息顺序传输。</li>\n<li>同时支持离线数据处理和实时数据处理。</li>\n<li>Scale out：支持在线水平扩展。</li>\n</ul>\n</li>\n<li><p>resource</p>\n<p>  link： <a href=\"http://ceph.com/\" target=\"_blank\" rel=\"external\">http://ceph.com/</a><br>  doc： <a href=\"http://docs.openfans.org/ceph\" target=\"_blank\" rel=\"external\">http://docs.openfans.org/ceph</a></p>\n</li>\n<li><p>core</p>\n</li>\n<li><p>components</p>\n<p>  cluster monitors<br>  clients<br>  metadata server cluster<br>  object storage cluster</p>\n</li>\n</ul>\n<h1 id=\"overlayfs\"><a href=\"#overlayfs\" class=\"headerlink\" title=\"overlayfs\"></a>overlayfs</h1><ul>\n<li>link： <a href=\"https://www.kernel.org/doc/Documentation/filesystems/overlayfs.txt\" target=\"_blank\" rel=\"external\">https://www.kernel.org/doc/Documentation/filesystems/overlayfs.txt</a></li>\n</ul>\n","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><h1 id=\"AuFS\"><a href=\"#AuFS\" class=\"headerlink\" title=\"AuFS\"></a>AuFS</h1><ul>\n<li><p>about</p>\n<p>  <a href=\"http://aufs.sourceforge.net/\">http://aufs.sourceforge.net/</a></p>\n<p>  layered file system</p>\n<p>  AuFS is a layered file system, so you can have a read only part, and a write part, and merge those together. So you could have the common parts of the operating system as read only, which are shared amongst all of your containers, and then give each container its own mount for writing.So let’s say you have a container image that is 1GB in size. If you wanted to use a Full VM, you would need to have 1GB times x number of VMs you want. With LXC and AuFS you can share the bulk of the 1GB and if you have 1000 containers you still might only have a little over 1GB of space for the containers OS, assuming they are all running the same OS image.</p>\n</li>\n</ul>\n<h1 id=\"ceph\"><a href=\"#ceph\" class=\"headerlink\" title=\"ceph\"></a>ceph</h1><ul>\n<li><p>about</p>\n<p>  一个 Linux PB 级分布式文件系统<br>  Ceph is a distributed object store and file system designed to provide excellent performance, reliability and scalability.</p>\n</li>\n<li><p>feature</p>\n<ul>\n<li>以时间复杂度为O(1)的方式提供消息持久化能力，即使对TB级以上数据也能保证常数时间复杂度的访问性能。</li>\n<li>高吞吐率。即使在非常廉价的商用机器上也能做到单机支持每秒100K条以上消息的传输。</li>\n<li>支持Kafka Server间的消息分区，及分布式消费，同时保证每个Partition内的消息顺序传输。</li>\n<li>同时支持离线数据处理和实时数据处理。</li>\n<li>Scale out：支持在线水平扩展。</li>\n</ul>\n</li>\n<li><p>resource</p>\n<p>  link： <a href=\"http://ceph.com/\">http://ceph.com/</a><br>  doc： <a href=\"http://docs.openfans.org/ceph\">http://docs.openfans.org/ceph</a></p>\n</li>\n<li><p>core</p>\n</li>\n<li><p>components</p>\n<p>  cluster monitors<br>  clients<br>  metadata server cluster<br>  object storage cluster</p>\n</li>\n</ul>\n<h1 id=\"overlayfs\"><a href=\"#overlayfs\" class=\"headerlink\" title=\"overlayfs\"></a>overlayfs</h1><ul>\n<li>link： <a href=\"https://www.kernel.org/doc/Documentation/filesystems/overlayfs.txt\">https://www.kernel.org/doc/Documentation/filesystems/overlayfs.txt</a></li>\n</ul>\n"},{"title":"k8s core","_content":"\n# k8s core\nhttps://kubeup.com/\nhttps://quay.io\n","source":"_posts/docker-k8s-core.md","raw":"---\ntitle: k8s core\ncategories:\n- docker\ntags:\n- core\n- k8s\n---\n\n# k8s core\nhttps://kubeup.com/\nhttps://quay.io\n","slug":"docker-k8s-core","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-04-18T13:18:42.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rdm0029kxsv979zukli","content":"<h1 id=\"k8s-core\"><a href=\"#k8s-core\" class=\"headerlink\" title=\"k8s core\"></a>k8s core</h1><p><a href=\"https://kubeup.com/\" target=\"_blank\" rel=\"external\">https://kubeup.com/</a><br><a href=\"https://quay.io\" target=\"_blank\" rel=\"external\">https://quay.io</a></p>\n","excerpt":"","more":"<h1 id=\"k8s-core\"><a href=\"#k8s-core\" class=\"headerlink\" title=\"k8s core\"></a>k8s core</h1><p><a href=\"https://kubeup.com/\">https://kubeup.com/</a><br><a href=\"https://quay.io\">https://quay.io</a></p>\n"},{"title":"docker image","_content":"\n# about docker image\n\n- standard: [Docker Image Specification](https://github.com/docker/docker/blob/master/image/spec/v1.md)\n\n- layer\n\n        r & w layer-container\n        add nginx-image2\n        add nginx-image1\n        ubuntu-base image\n        kernel-bootfs\n\n- duplication while writing 写时复制机制\n\n# docker image command\n        \n- docker pull\n\n- docker run\n\n- docker images: check out\n    \n        docker images ububtu\n\n- docker inspect\n\n        docker inspect ubuntu\n\n- docker search: AUTOMATED-automatic build\n\n- docker rmi: delete image\n\n        docker rmi c03k349dfjn2\n    \n    -f if some container depends on this image:\n\n        docker rmi -f ubuntu\n    \n    delete all:\n        \n        docker rm $(docker ps -a -q)\n\n- docker commit: one way to create local image, the other way is dockerfile\n    commit changes to user image\n    \n        sudo docker run -t -i ubuntu\n        apt-get install sqlite3\n        echo 'test docker commit' >> hellodocker\n        exit\n        sudo docker commit -m=\"message\" --author=\"ag\" CONTAINERID ag/sqlite3:v1\n        sudo docker run -t -i ag/sqlite3:v1\n        cat hellodocker\n        sqlite3 -version\n\n- docker build: build image with dockerfile\n    \n    -rm=false: do not delete the tmp image while building\n\n    -t: set namespace, repo name, tag\n\n        sudo docker build -t ag/test:v1\n\n- docker tag\n\n        sudo docker tag ag/test:v1 ag/test:v2\n        (v1 and v2 will have the same image id)\n    \n    build with github:\n    \n        sudo docker build -t ag/test:v1 git://github.com/ag/dockerfile.git\n\n- docker save\n\n- docker load\n\n- docker diff\n    \n        docker diff container","source":"_posts/docker-image-detail.md","raw":"---\ntitle: docker image\ncategories:\n- docker\ntags:\n- detail\n---\n\n# about docker image\n\n- standard: [Docker Image Specification](https://github.com/docker/docker/blob/master/image/spec/v1.md)\n\n- layer\n\n        r & w layer-container\n        add nginx-image2\n        add nginx-image1\n        ubuntu-base image\n        kernel-bootfs\n\n- duplication while writing 写时复制机制\n\n# docker image command\n        \n- docker pull\n\n- docker run\n\n- docker images: check out\n    \n        docker images ububtu\n\n- docker inspect\n\n        docker inspect ubuntu\n\n- docker search: AUTOMATED-automatic build\n\n- docker rmi: delete image\n\n        docker rmi c03k349dfjn2\n    \n    -f if some container depends on this image:\n\n        docker rmi -f ubuntu\n    \n    delete all:\n        \n        docker rm $(docker ps -a -q)\n\n- docker commit: one way to create local image, the other way is dockerfile\n    commit changes to user image\n    \n        sudo docker run -t -i ubuntu\n        apt-get install sqlite3\n        echo 'test docker commit' >> hellodocker\n        exit\n        sudo docker commit -m=\"message\" --author=\"ag\" CONTAINERID ag/sqlite3:v1\n        sudo docker run -t -i ag/sqlite3:v1\n        cat hellodocker\n        sqlite3 -version\n\n- docker build: build image with dockerfile\n    \n    -rm=false: do not delete the tmp image while building\n\n    -t: set namespace, repo name, tag\n\n        sudo docker build -t ag/test:v1\n\n- docker tag\n\n        sudo docker tag ag/test:v1 ag/test:v2\n        (v1 and v2 will have the same image id)\n    \n    build with github:\n    \n        sudo docker build -t ag/test:v1 git://github.com/ag/dockerfile.git\n\n- docker save\n\n- docker load\n\n- docker diff\n    \n        docker diff container","slug":"docker-image-detail","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-13T11:50:32.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rdq002dkxsvxnhy6rnm","content":"<h1 id=\"about-docker-image\"><a href=\"#about-docker-image\" class=\"headerlink\" title=\"about docker image\"></a>about docker image</h1><ul>\n<li><p>standard: <a href=\"https://github.com/docker/docker/blob/master/image/spec/v1.md\" target=\"_blank\" rel=\"external\">Docker Image Specification</a></p>\n</li>\n<li><p>layer</p>\n<pre><code>r &amp; w layer-container\nadd nginx-image2\nadd nginx-image1\nubuntu-base image\nkernel-bootfs\n</code></pre></li>\n<li><p>duplication while writing 写时复制机制</p>\n</li>\n</ul>\n<h1 id=\"docker-image-command\"><a href=\"#docker-image-command\" class=\"headerlink\" title=\"docker image command\"></a>docker image command</h1><ul>\n<li><p>docker pull</p>\n</li>\n<li><p>docker run</p>\n</li>\n<li><p>docker images: check out</p>\n<pre><code>docker images ububtu\n</code></pre></li>\n<li><p>docker inspect</p>\n<pre><code>docker inspect ubuntu\n</code></pre></li>\n<li><p>docker search: AUTOMATED-automatic build</p>\n</li>\n<li><p>docker rmi: delete image</p>\n<pre><code>docker rmi c03k349dfjn2\n</code></pre><p>  -f if some container depends on this image:</p>\n<pre><code>docker rmi -f ubuntu\n</code></pre><p>  delete all:</p>\n<pre><code>docker rm $(docker ps -a -q)\n</code></pre></li>\n<li><p>docker commit: one way to create local image, the other way is dockerfile<br>  commit changes to user image</p>\n<pre><code>sudo docker run -t -i ubuntu\napt-get install sqlite3\necho &apos;test docker commit&apos; &gt;&gt; hellodocker\nexit\nsudo docker commit -m=&quot;message&quot; --author=&quot;ag&quot; CONTAINERID ag/sqlite3:v1\nsudo docker run -t -i ag/sqlite3:v1\ncat hellodocker\nsqlite3 -version\n</code></pre></li>\n<li><p>docker build: build image with dockerfile</p>\n<p>  -rm=false: do not delete the tmp image while building</p>\n<p>  -t: set namespace, repo name, tag</p>\n<pre><code>sudo docker build -t ag/test:v1\n</code></pre></li>\n<li><p>docker tag</p>\n<pre><code>sudo docker tag ag/test:v1 ag/test:v2\n(v1 and v2 will have the same image id)\n</code></pre><p>  build with github:</p>\n<pre><code>sudo docker build -t ag/test:v1 git://github.com/ag/dockerfile.git\n</code></pre></li>\n<li><p>docker save</p>\n</li>\n<li><p>docker load</p>\n</li>\n<li><p>docker diff</p>\n<pre><code>docker diff container\n</code></pre></li>\n</ul>\n","excerpt":"","more":"<h1 id=\"about-docker-image\"><a href=\"#about-docker-image\" class=\"headerlink\" title=\"about docker image\"></a>about docker image</h1><ul>\n<li><p>standard: <a href=\"https://github.com/docker/docker/blob/master/image/spec/v1.md\">Docker Image Specification</a></p>\n</li>\n<li><p>layer</p>\n<pre><code>r &amp; w layer-container\nadd nginx-image2\nadd nginx-image1\nubuntu-base image\nkernel-bootfs\n</code></pre></li>\n<li><p>duplication while writing 写时复制机制</p>\n</li>\n</ul>\n<h1 id=\"docker-image-command\"><a href=\"#docker-image-command\" class=\"headerlink\" title=\"docker image command\"></a>docker image command</h1><ul>\n<li><p>docker pull</p>\n</li>\n<li><p>docker run</p>\n</li>\n<li><p>docker images: check out</p>\n<pre><code>docker images ububtu\n</code></pre></li>\n<li><p>docker inspect</p>\n<pre><code>docker inspect ubuntu\n</code></pre></li>\n<li><p>docker search: AUTOMATED-automatic build</p>\n</li>\n<li><p>docker rmi: delete image</p>\n<pre><code>docker rmi c03k349dfjn2\n</code></pre><p>  -f if some container depends on this image:</p>\n<pre><code>docker rmi -f ubuntu\n</code></pre><p>  delete all:</p>\n<pre><code>docker rm $(docker ps -a -q)\n</code></pre></li>\n<li><p>docker commit: one way to create local image, the other way is dockerfile<br>  commit changes to user image</p>\n<pre><code>sudo docker run -t -i ubuntu\napt-get install sqlite3\necho &apos;test docker commit&apos; &gt;&gt; hellodocker\nexit\nsudo docker commit -m=&quot;message&quot; --author=&quot;ag&quot; CONTAINERID ag/sqlite3:v1\nsudo docker run -t -i ag/sqlite3:v1\ncat hellodocker\nsqlite3 -version\n</code></pre></li>\n<li><p>docker build: build image with dockerfile</p>\n<p>  -rm=false: do not delete the tmp image while building</p>\n<p>  -t: set namespace, repo name, tag</p>\n<pre><code>sudo docker build -t ag/test:v1\n</code></pre></li>\n<li><p>docker tag</p>\n<pre><code>sudo docker tag ag/test:v1 ag/test:v2\n(v1 and v2 will have the same image id)\n</code></pre><p>  build with github:</p>\n<pre><code>sudo docker build -t ag/test:v1 git://github.com/ag/dockerfile.git\n</code></pre></li>\n<li><p>docker save</p>\n</li>\n<li><p>docker load</p>\n</li>\n<li><p>docker diff</p>\n<pre><code>docker diff container\n</code></pre></li>\n</ul>\n"},{"title":"k8s core","_content":"\n#  Alibaba Cloud\n\nguide: https://yq.aliyun.com/articles/68921?spm=5176.100240.searchblog.20.kVq4L7\n\n## prepare\n\n1. create vpc\nvpc, vswitch,\n\n2. create 2 ecs\nCentOS 7.2-x64, Ubuntu 16.04 x64\ncreate security group;\n\n3. connect ecs 1 as master\n$ssh root@112.74.175.225\n$hostname # ecs instance name\nexample: \n    $curl -L 'http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh' | bash -s nice --node-type master --key-id $ACCESS_KEY_ID --key-secret $ACCESS_KEY_SECRET --region $REGION --discovery token://\n\ninstall:\n    $curl -L 'http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh' | bash -s nice --node-type master --key-id pFR3aVQnXvjZzwKn --key-secret Xil10SqrVumfSfd7ESNjSIcNlsrm1V --region cn-shenzhen --discovery token://\nuninstall:\n    $curl -L 'http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh' | bash -s nice --node-type down\nlog：\n    $ journalctl -u kubelet -f\n\nkubeadm join --discovery token://3f3793:697efc725dcda9e8@192.168.1.237:9898\n\n4. connect ecs 2 3 as node\ninstall: \n    $ curl -L 'http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh' | bash -s nice --node-type node --key-id pFR3aVQnXvjZzwKn --key-secret Xil10SqrVumfSfd7ESNjSIcNlsrm1V --region cn-shenzhen --discovery token://3f3793:697efc725dcda9e8@192.168.1.237:9898\n\n# kubectl get nodes\nNAME                      STATUS         AGE       VERSION\nizwz92didsxigwodbj5mztz   Ready          27m       v1.6.0-alpha.0.2229+88fbc68ad99479-dirty\nizwz99k8xfd8rnweyyp1y2z   Ready          25m       v1.6.0-alpha.0.2229+88fbc68ad99479-dirty\nizwz99o2kucvflwkkpf9d2z   Ready,master   40m       v1.6.0-alpha.0.2229+88fbc68ad99479-dirty\n\n5. network setting\ngoto master:\n# curl -sSL http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/conf/flannel-vpc.yml -o flannel-vpc.yml\n# vi flannel-vpc.yml \n# kubectl apply -f flannel-vpc.yml\nconfigmap \"kube-flannel-cfg\" created\ndaemonset \"kube-flannel-ds\" created\n\n# kubectl --namespace=kube-system get ds\nNAME              DESIRED   CURRENT   READY     NODE-SELECTOR                   AGE\nkube-flannel-ds   3         3         3         beta.kubernetes.io/arch=amd64   1m\nkube-proxy        3         3         3         <none>                          43m\n\n6. create application\ngoto master:\n# kubectl run nginx --image=registry.cn-hangzhou.aliyuncs.com/spacexnice/nginx:latest --replicas=2 --labels run=nginx\ndeployment \"nginx\" created\n\n# kubectl get po\nNAME                     READY     STATUS    RESTARTS   AGE\nnginx-3579028506-rglm7   1/1       Running   0          38s\nnginx-3579028506-rtgsw   1/1       Running   0          38s\n\n# kubectl expose deployment nginx --port=80 --target-port=80 --type=LoadBalancer\nservice \"nginx\" exposed\n\n# kubectl get svc\nNAME         CLUSTER-IP     EXTERNAL-IP     PORT(S)        AGE\nkubernetes   172.19.0.1     <none>          443/TCP        46m\nnginx        172.19.3.137   120.77.128.24   80:32392/TCP   17s\n\nNOTICE：\n1. kubectl 命令的配置目前只放在了master上，但这并不意味着创建的应用都在Master上， 这是集群范围的\n如果想其他地方执行Kubectl命令的话，把/etc/kubernetes/admin.conf copy过去就可以了\n\n2. dockerfile\nFrom ubuntu:16.04\nRUN  apt update && apt install nginx\nENTRYPOINT  nginx\nCMD [\"-g\",\"daemon off;\"]\n\n","source":"_posts/docker-k8s-install.md","raw":"---\ntitle: k8s core\ncategories:\n- docker\ntags:\n- core\n---\n\n#  Alibaba Cloud\n\nguide: https://yq.aliyun.com/articles/68921?spm=5176.100240.searchblog.20.kVq4L7\n\n## prepare\n\n1. create vpc\nvpc, vswitch,\n\n2. create 2 ecs\nCentOS 7.2-x64, Ubuntu 16.04 x64\ncreate security group;\n\n3. connect ecs 1 as master\n$ssh root@112.74.175.225\n$hostname # ecs instance name\nexample: \n    $curl -L 'http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh' | bash -s nice --node-type master --key-id $ACCESS_KEY_ID --key-secret $ACCESS_KEY_SECRET --region $REGION --discovery token://\n\ninstall:\n    $curl -L 'http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh' | bash -s nice --node-type master --key-id pFR3aVQnXvjZzwKn --key-secret Xil10SqrVumfSfd7ESNjSIcNlsrm1V --region cn-shenzhen --discovery token://\nuninstall:\n    $curl -L 'http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh' | bash -s nice --node-type down\nlog：\n    $ journalctl -u kubelet -f\n\nkubeadm join --discovery token://3f3793:697efc725dcda9e8@192.168.1.237:9898\n\n4. connect ecs 2 3 as node\ninstall: \n    $ curl -L 'http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh' | bash -s nice --node-type node --key-id pFR3aVQnXvjZzwKn --key-secret Xil10SqrVumfSfd7ESNjSIcNlsrm1V --region cn-shenzhen --discovery token://3f3793:697efc725dcda9e8@192.168.1.237:9898\n\n# kubectl get nodes\nNAME                      STATUS         AGE       VERSION\nizwz92didsxigwodbj5mztz   Ready          27m       v1.6.0-alpha.0.2229+88fbc68ad99479-dirty\nizwz99k8xfd8rnweyyp1y2z   Ready          25m       v1.6.0-alpha.0.2229+88fbc68ad99479-dirty\nizwz99o2kucvflwkkpf9d2z   Ready,master   40m       v1.6.0-alpha.0.2229+88fbc68ad99479-dirty\n\n5. network setting\ngoto master:\n# curl -sSL http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/conf/flannel-vpc.yml -o flannel-vpc.yml\n# vi flannel-vpc.yml \n# kubectl apply -f flannel-vpc.yml\nconfigmap \"kube-flannel-cfg\" created\ndaemonset \"kube-flannel-ds\" created\n\n# kubectl --namespace=kube-system get ds\nNAME              DESIRED   CURRENT   READY     NODE-SELECTOR                   AGE\nkube-flannel-ds   3         3         3         beta.kubernetes.io/arch=amd64   1m\nkube-proxy        3         3         3         <none>                          43m\n\n6. create application\ngoto master:\n# kubectl run nginx --image=registry.cn-hangzhou.aliyuncs.com/spacexnice/nginx:latest --replicas=2 --labels run=nginx\ndeployment \"nginx\" created\n\n# kubectl get po\nNAME                     READY     STATUS    RESTARTS   AGE\nnginx-3579028506-rglm7   1/1       Running   0          38s\nnginx-3579028506-rtgsw   1/1       Running   0          38s\n\n# kubectl expose deployment nginx --port=80 --target-port=80 --type=LoadBalancer\nservice \"nginx\" exposed\n\n# kubectl get svc\nNAME         CLUSTER-IP     EXTERNAL-IP     PORT(S)        AGE\nkubernetes   172.19.0.1     <none>          443/TCP        46m\nnginx        172.19.3.137   120.77.128.24   80:32392/TCP   17s\n\nNOTICE：\n1. kubectl 命令的配置目前只放在了master上，但这并不意味着创建的应用都在Master上， 这是集群范围的\n如果想其他地方执行Kubectl命令的话，把/etc/kubernetes/admin.conf copy过去就可以了\n\n2. dockerfile\nFrom ubuntu:16.04\nRUN  apt update && apt install nginx\nENTRYPOINT  nginx\nCMD [\"-g\",\"daemon off;\"]\n\n","slug":"docker-k8s-install","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-02-18T10:36:42.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rdw002gkxsvobjrkkl9","content":"<h1 id=\"Alibaba-Cloud\"><a href=\"#Alibaba-Cloud\" class=\"headerlink\" title=\"Alibaba Cloud\"></a>Alibaba Cloud</h1><p>guide: <a href=\"https://yq.aliyun.com/articles/68921?spm=5176.100240.searchblog.20.kVq4L7\" target=\"_blank\" rel=\"external\">https://yq.aliyun.com/articles/68921?spm=5176.100240.searchblog.20.kVq4L7</a></p>\n<h2 id=\"prepare\"><a href=\"#prepare\" class=\"headerlink\" title=\"prepare\"></a>prepare</h2><ol>\n<li><p>create vpc<br>vpc, vswitch,</p>\n</li>\n<li><p>create 2 ecs<br>CentOS 7.2-x64, Ubuntu 16.04 x64<br>create security group;</p>\n</li>\n<li><p>connect ecs 1 as master<br>$ssh root@112.74.175.225<br>$hostname # ecs instance name<br>example:<br> $curl -L ‘<a href=\"http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh\" target=\"_blank\" rel=\"external\">http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh</a>‘ | bash -s nice –node-type master –key-id $ACCESS_KEY_ID –key-secret $ACCESS_KEY_SECRET –region $REGION –discovery token://</p>\n</li>\n</ol>\n<p>install:<br>    $curl -L ‘<a href=\"http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh\" target=\"_blank\" rel=\"external\">http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh</a>‘ | bash -s nice –node-type master –key-id pFR3aVQnXvjZzwKn –key-secret Xil10SqrVumfSfd7ESNjSIcNlsrm1V –region cn-shenzhen –discovery token://<br>uninstall:<br>    $curl -L ‘<a href=\"http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh\" target=\"_blank\" rel=\"external\">http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh</a>‘ | bash -s nice –node-type down<br>log：<br>    $ journalctl -u kubelet -f</p>\n<p>kubeadm join –discovery token://3f3793:697efc725dcda9e8@192.168.1.237:9898</p>\n<ol>\n<li>connect ecs 2 3 as node<br>install:<br> $ curl -L ‘<a href=\"http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh\" target=\"_blank\" rel=\"external\">http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh</a>‘ | bash -s nice –node-type node –key-id pFR3aVQnXvjZzwKn –key-secret Xil10SqrVumfSfd7ESNjSIcNlsrm1V –region cn-shenzhen –discovery token://3f3793:697efc725dcda9e8@192.168.1.237:9898</li>\n</ol>\n<h1 id=\"kubectl-get-nodes\"><a href=\"#kubectl-get-nodes\" class=\"headerlink\" title=\"kubectl get nodes\"></a>kubectl get nodes</h1><p>NAME                      STATUS         AGE       VERSION<br>izwz92didsxigwodbj5mztz   Ready          27m       v1.6.0-alpha.0.2229+88fbc68ad99479-dirty<br>izwz99k8xfd8rnweyyp1y2z   Ready          25m       v1.6.0-alpha.0.2229+88fbc68ad99479-dirty<br>izwz99o2kucvflwkkpf9d2z   Ready,master   40m       v1.6.0-alpha.0.2229+88fbc68ad99479-dirty</p>\n<ol>\n<li>network setting<br>goto master:<h1 id=\"curl-sSL-http-aliacs-k8s-oss-cn-hangzhou-aliyuncs-com-conf-flannel-vpc-yml-o-flannel-vpc-yml\"><a href=\"#curl-sSL-http-aliacs-k8s-oss-cn-hangzhou-aliyuncs-com-conf-flannel-vpc-yml-o-flannel-vpc-yml\" class=\"headerlink\" title=\"curl -sSL http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/conf/flannel-vpc.yml -o flannel-vpc.yml\"></a>curl -sSL <a href=\"http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/conf/flannel-vpc.yml\" target=\"_blank\" rel=\"external\">http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/conf/flannel-vpc.yml</a> -o flannel-vpc.yml</h1><h1 id=\"vi-flannel-vpc-yml\"><a href=\"#vi-flannel-vpc-yml\" class=\"headerlink\" title=\"vi flannel-vpc.yml\"></a>vi flannel-vpc.yml</h1><h1 id=\"kubectl-apply-f-flannel-vpc-yml\"><a href=\"#kubectl-apply-f-flannel-vpc-yml\" class=\"headerlink\" title=\"kubectl apply -f flannel-vpc.yml\"></a>kubectl apply -f flannel-vpc.yml</h1>configmap “kube-flannel-cfg” created<br>daemonset “kube-flannel-ds” created</li>\n</ol>\n<h1 id=\"kubectl-–namespace-kube-system-get-ds\"><a href=\"#kubectl-–namespace-kube-system-get-ds\" class=\"headerlink\" title=\"kubectl –namespace=kube-system get ds\"></a>kubectl –namespace=kube-system get ds</h1><p>NAME              DESIRED   CURRENT   READY     NODE-SELECTOR                   AGE<br>kube-flannel-ds   3         3         3         beta.kubernetes.io/arch=amd64   1m<br>kube-proxy        3         3         3         <none>                          43m</none></p>\n<ol>\n<li>create application<br>goto master:<h1 id=\"kubectl-run-nginx-–image-registry-cn-hangzhou-aliyuncs-com-spacexnice-nginx-latest-–replicas-2-–labels-run-nginx\"><a href=\"#kubectl-run-nginx-–image-registry-cn-hangzhou-aliyuncs-com-spacexnice-nginx-latest-–replicas-2-–labels-run-nginx\" class=\"headerlink\" title=\"kubectl run nginx –image=registry.cn-hangzhou.aliyuncs.com/spacexnice/nginx:latest –replicas=2 –labels run=nginx\"></a>kubectl run nginx –image=registry.cn-hangzhou.aliyuncs.com/spacexnice/nginx:latest –replicas=2 –labels run=nginx</h1>deployment “nginx” created</li>\n</ol>\n<h1 id=\"kubectl-get-po\"><a href=\"#kubectl-get-po\" class=\"headerlink\" title=\"kubectl get po\"></a>kubectl get po</h1><p>NAME                     READY     STATUS    RESTARTS   AGE<br>nginx-3579028506-rglm7   1/1       Running   0          38s<br>nginx-3579028506-rtgsw   1/1       Running   0          38s</p>\n<h1 id=\"kubectl-expose-deployment-nginx-–port-80-–target-port-80-–type-LoadBalancer\"><a href=\"#kubectl-expose-deployment-nginx-–port-80-–target-port-80-–type-LoadBalancer\" class=\"headerlink\" title=\"kubectl expose deployment nginx –port=80 –target-port=80 –type=LoadBalancer\"></a>kubectl expose deployment nginx –port=80 –target-port=80 –type=LoadBalancer</h1><p>service “nginx” exposed</p>\n<h1 id=\"kubectl-get-svc\"><a href=\"#kubectl-get-svc\" class=\"headerlink\" title=\"kubectl get svc\"></a>kubectl get svc</h1><p>NAME         CLUSTER-IP     EXTERNAL-IP     PORT(S)        AGE<br>kubernetes   172.19.0.1     <none>          443/TCP        46m<br>nginx        172.19.3.137   120.77.128.24   80:32392/TCP   17s</none></p>\n<p>NOTICE：</p>\n<ol>\n<li><p>kubectl 命令的配置目前只放在了master上，但这并不意味着创建的应用都在Master上， 这是集群范围的<br>如果想其他地方执行Kubectl命令的话，把/etc/kubernetes/admin.conf copy过去就可以了</p>\n</li>\n<li><p>dockerfile<br>From ubuntu:16.04<br>RUN  apt update &amp;&amp; apt install nginx<br>ENTRYPOINT  nginx<br>CMD [“-g”,”daemon off;”]</p>\n</li>\n</ol>\n","excerpt":"","more":"<h1 id=\"Alibaba-Cloud\"><a href=\"#Alibaba-Cloud\" class=\"headerlink\" title=\"Alibaba Cloud\"></a>Alibaba Cloud</h1><p>guide: <a href=\"https://yq.aliyun.com/articles/68921?spm=5176.100240.searchblog.20.kVq4L7\">https://yq.aliyun.com/articles/68921?spm=5176.100240.searchblog.20.kVq4L7</a></p>\n<h2 id=\"prepare\"><a href=\"#prepare\" class=\"headerlink\" title=\"prepare\"></a>prepare</h2><ol>\n<li><p>create vpc<br>vpc, vswitch,</p>\n</li>\n<li><p>create 2 ecs<br>CentOS 7.2-x64, Ubuntu 16.04 x64<br>create security group;</p>\n</li>\n<li><p>connect ecs 1 as master<br>$ssh root@112.74.175.225<br>$hostname # ecs instance name<br>example:<br> $curl -L ‘<a href=\"http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh\">http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh</a>‘ | bash -s nice –node-type master –key-id $ACCESS_KEY_ID –key-secret $ACCESS_KEY_SECRET –region $REGION –discovery token://</p>\n</li>\n</ol>\n<p>install:<br>    $curl -L ‘<a href=\"http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh\">http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh</a>‘ | bash -s nice –node-type master –key-id pFR3aVQnXvjZzwKn –key-secret Xil10SqrVumfSfd7ESNjSIcNlsrm1V –region cn-shenzhen –discovery token://<br>uninstall:<br>    $curl -L ‘<a href=\"http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh\">http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh</a>‘ | bash -s nice –node-type down<br>log：<br>    $ journalctl -u kubelet -f</p>\n<p>kubeadm join –discovery token://3f3793:697efc725dcda9e8@192.168.1.237:9898</p>\n<ol>\n<li>connect ecs 2 3 as node<br>install:<br> $ curl -L ‘<a href=\"http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh\">http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh</a>‘ | bash -s nice –node-type node –key-id pFR3aVQnXvjZzwKn –key-secret Xil10SqrVumfSfd7ESNjSIcNlsrm1V –region cn-shenzhen –discovery token://3f3793:697efc725dcda9e8@192.168.1.237:9898</li>\n</ol>\n<h1 id=\"kubectl-get-nodes\"><a href=\"#kubectl-get-nodes\" class=\"headerlink\" title=\"kubectl get nodes\"></a>kubectl get nodes</h1><p>NAME                      STATUS         AGE       VERSION<br>izwz92didsxigwodbj5mztz   Ready          27m       v1.6.0-alpha.0.2229+88fbc68ad99479-dirty<br>izwz99k8xfd8rnweyyp1y2z   Ready          25m       v1.6.0-alpha.0.2229+88fbc68ad99479-dirty<br>izwz99o2kucvflwkkpf9d2z   Ready,master   40m       v1.6.0-alpha.0.2229+88fbc68ad99479-dirty</p>\n<ol>\n<li>network setting<br>goto master:<h1 id=\"curl-sSL-http-aliacs-k8s-oss-cn-hangzhou-aliyuncs-com-conf-flannel-vpc-yml-o-flannel-vpc-yml\"><a href=\"#curl-sSL-http-aliacs-k8s-oss-cn-hangzhou-aliyuncs-com-conf-flannel-vpc-yml-o-flannel-vpc-yml\" class=\"headerlink\" title=\"curl -sSL http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/conf/flannel-vpc.yml -o flannel-vpc.yml\"></a>curl -sSL <a href=\"http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/conf/flannel-vpc.yml\">http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/conf/flannel-vpc.yml</a> -o flannel-vpc.yml</h1><h1 id=\"vi-flannel-vpc-yml\"><a href=\"#vi-flannel-vpc-yml\" class=\"headerlink\" title=\"vi flannel-vpc.yml\"></a>vi flannel-vpc.yml</h1><h1 id=\"kubectl-apply-f-flannel-vpc-yml\"><a href=\"#kubectl-apply-f-flannel-vpc-yml\" class=\"headerlink\" title=\"kubectl apply -f flannel-vpc.yml\"></a>kubectl apply -f flannel-vpc.yml</h1>configmap “kube-flannel-cfg” created<br>daemonset “kube-flannel-ds” created</li>\n</ol>\n<h1 id=\"kubectl-–namespace-kube-system-get-ds\"><a href=\"#kubectl-–namespace-kube-system-get-ds\" class=\"headerlink\" title=\"kubectl –namespace=kube-system get ds\"></a>kubectl –namespace=kube-system get ds</h1><p>NAME              DESIRED   CURRENT   READY     NODE-SELECTOR                   AGE<br>kube-flannel-ds   3         3         3         beta.kubernetes.io/arch=amd64   1m<br>kube-proxy        3         3         3         <none>                          43m</p>\n<ol>\n<li>create application<br>goto master:<h1 id=\"kubectl-run-nginx-–image-registry-cn-hangzhou-aliyuncs-com-spacexnice-nginx-latest-–replicas-2-–labels-run-nginx\"><a href=\"#kubectl-run-nginx-–image-registry-cn-hangzhou-aliyuncs-com-spacexnice-nginx-latest-–replicas-2-–labels-run-nginx\" class=\"headerlink\" title=\"kubectl run nginx –image=registry.cn-hangzhou.aliyuncs.com/spacexnice/nginx:latest –replicas=2 –labels run=nginx\"></a>kubectl run nginx –image=registry.cn-hangzhou.aliyuncs.com/spacexnice/nginx:latest –replicas=2 –labels run=nginx</h1>deployment “nginx” created</li>\n</ol>\n<h1 id=\"kubectl-get-po\"><a href=\"#kubectl-get-po\" class=\"headerlink\" title=\"kubectl get po\"></a>kubectl get po</h1><p>NAME                     READY     STATUS    RESTARTS   AGE<br>nginx-3579028506-rglm7   1/1       Running   0          38s<br>nginx-3579028506-rtgsw   1/1       Running   0          38s</p>\n<h1 id=\"kubectl-expose-deployment-nginx-–port-80-–target-port-80-–type-LoadBalancer\"><a href=\"#kubectl-expose-deployment-nginx-–port-80-–target-port-80-–type-LoadBalancer\" class=\"headerlink\" title=\"kubectl expose deployment nginx –port=80 –target-port=80 –type=LoadBalancer\"></a>kubectl expose deployment nginx –port=80 –target-port=80 –type=LoadBalancer</h1><p>service “nginx” exposed</p>\n<h1 id=\"kubectl-get-svc\"><a href=\"#kubectl-get-svc\" class=\"headerlink\" title=\"kubectl get svc\"></a>kubectl get svc</h1><p>NAME         CLUSTER-IP     EXTERNAL-IP     PORT(S)        AGE<br>kubernetes   172.19.0.1     <none>          443/TCP        46m<br>nginx        172.19.3.137   120.77.128.24   80:32392/TCP   17s</p>\n<p>NOTICE：</p>\n<ol>\n<li><p>kubectl 命令的配置目前只放在了master上，但这并不意味着创建的应用都在Master上， 这是集群范围的<br>如果想其他地方执行Kubectl命令的话，把/etc/kubernetes/admin.conf copy过去就可以了</p>\n</li>\n<li><p>dockerfile<br>From ubuntu:16.04<br>RUN  apt update &amp;&amp; apt install nginx<br>ENTRYPOINT  nginx<br>CMD [“-g”,”daemon off;”]</p>\n</li>\n</ol>\n"},{"title":"docker machine","_content":"\n# about\ncode: https://github.com/docker/machine\ndoc: https://docs.docker.com/machine/overview/\n\n# faq\n\n- What’s the difference between Docker Engine and Docker Machine?\n\nWhen people say “Docker” they typically mean Docker Engine, the client-server application made up of the Docker daemon, a REST API that specifies interfaces for interacting with the daemon, and a command line interface (CLI) client that talks to the daemon (through the REST API wrapper). Docker Engine accepts docker commands from the CLI, such as docker run <image>, docker ps to list running containers, docker images to list images, and so on.","source":"_posts/docker-machine-detail.md","raw":"---\ntitle: docker machine\ncategories:\n- docker\ntags:\n- detail\n---\n\n# about\ncode: https://github.com/docker/machine\ndoc: https://docs.docker.com/machine/overview/\n\n# faq\n\n- What’s the difference between Docker Engine and Docker Machine?\n\nWhen people say “Docker” they typically mean Docker Engine, the client-server application made up of the Docker daemon, a REST API that specifies interfaces for interacting with the daemon, and a command line interface (CLI) client that talks to the daemon (through the REST API wrapper). Docker Engine accepts docker commands from the CLI, such as docker run <image>, docker ps to list running containers, docker images to list images, and so on.","slug":"docker-machine-detail","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-02-10T05:48:40.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rdy002kkxsv03m04ti0","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><p>code: <a href=\"https://github.com/docker/machine\" target=\"_blank\" rel=\"external\">https://github.com/docker/machine</a><br>doc: <a href=\"https://docs.docker.com/machine/overview/\" target=\"_blank\" rel=\"external\">https://docs.docker.com/machine/overview/</a></p>\n<h1 id=\"faq\"><a href=\"#faq\" class=\"headerlink\" title=\"faq\"></a>faq</h1><ul>\n<li>What’s the difference between Docker Engine and Docker Machine?</li>\n</ul>\n<p>When people say “Docker” they typically mean Docker Engine, the client-server application made up of the Docker daemon, a REST API that specifies interfaces for interacting with the daemon, and a command line interface (CLI) client that talks to the daemon (through the REST API wrapper). Docker Engine accepts docker commands from the CLI, such as docker run <image>, docker ps to list running containers, docker images to list images, and so on.</image></p>\n","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><p>code: <a href=\"https://github.com/docker/machine\">https://github.com/docker/machine</a><br>doc: <a href=\"https://docs.docker.com/machine/overview/\">https://docs.docker.com/machine/overview/</a></p>\n<h1 id=\"faq\"><a href=\"#faq\" class=\"headerlink\" title=\"faq\"></a>faq</h1><ul>\n<li>What’s the difference between Docker Engine and Docker Machine?</li>\n</ul>\n<p>When people say “Docker” they typically mean Docker Engine, the client-server application made up of the Docker daemon, a REST API that specifies interfaces for interacting with the daemon, and a command line interface (CLI) client that talks to the daemon (through the REST API wrapper). Docker Engine accepts docker commands from the CLI, such as docker run <image>, docker ps to list running containers, docker images to list images, and so on.</p>\n"},{"title":"docker network core","_content":"\n# about\n\n- docker daemon ini process(docker -d)\n\n    [/var/lib/docker|116d5cd4] +job init_networkdriver()\n    [/var/lib/docker|116d5cd4.init_networkdriver()] creating new bridge for docker0\n    [/var/lib/docker|116d5cd4.init_networkdriver()] getting iface addr\n    [/var/lib/docker|116d5cd4] -job init_networkdriver() = OK (0)\n\n- default mode\n    bridge\n        docker0\n\n- expose port\n\n    -P: randomly expose a port between 49000-49900\n            sudo docker run -d -P traning/webapp python app.py\n    -p:\n            ip:hostPort:containerPost | ip::containerPort | hostPort:containerPort\n\n- check out network setting\n\n        sudo docker inspect --format '{{.NetworkSettings}}' CID\n\n- container link\n\n    about: docker0 bridge; iptables\n\n    --link name:alias\n\n            sudo docker -d --name dbdata training/postgres\n            sudo docker run -d -P --name web --link dbdata:db training/webapp python app.py\n            sudo docker inspect web\n\n        Links: /dbdata:/web/db\n\n        how web container use dbdata:\n\n            - env variable\n                sudo docker run --rm --name web2 --link dbdata:webdb training/webapp env\n                <name>_PORT_<port>_<protocol>_ADDR/PORT/PROTO\n\n            - /etc/hosts\n\n    - ambassador\n        about: 代理连接\n        connect redis client and server via 2 ambassador\n                sudo docker run -d --name redis ag/redis\n                sudo docker run -d --name redis ag/redis\n\n# docker network design\n\nhttps://blog.docker.com/2016/03/docker-networking-design-philosophy/\n\n# cnm design\n\nhttps://github.com/docker/libnetwork/blob/master/docs/design.md\n\n# docker network command\n\n- docker network ls\n\n- docker network create\n\n- docker network connect\n\n- docker network disconnect\n\n- docker network inspect\n\n- docker network rm\n\n# docker network api\n\n- network driver api\n\n- IPAM api\n","source":"_posts/docker-network-core.md","raw":"---\ntitle: docker network core\ncategories:\n- docker\ntags:\n- core\n- network\n---\n\n# about\n\n- docker daemon ini process(docker -d)\n\n    [/var/lib/docker|116d5cd4] +job init_networkdriver()\n    [/var/lib/docker|116d5cd4.init_networkdriver()] creating new bridge for docker0\n    [/var/lib/docker|116d5cd4.init_networkdriver()] getting iface addr\n    [/var/lib/docker|116d5cd4] -job init_networkdriver() = OK (0)\n\n- default mode\n    bridge\n        docker0\n\n- expose port\n\n    -P: randomly expose a port between 49000-49900\n            sudo docker run -d -P traning/webapp python app.py\n    -p:\n            ip:hostPort:containerPost | ip::containerPort | hostPort:containerPort\n\n- check out network setting\n\n        sudo docker inspect --format '{{.NetworkSettings}}' CID\n\n- container link\n\n    about: docker0 bridge; iptables\n\n    --link name:alias\n\n            sudo docker -d --name dbdata training/postgres\n            sudo docker run -d -P --name web --link dbdata:db training/webapp python app.py\n            sudo docker inspect web\n\n        Links: /dbdata:/web/db\n\n        how web container use dbdata:\n\n            - env variable\n                sudo docker run --rm --name web2 --link dbdata:webdb training/webapp env\n                <name>_PORT_<port>_<protocol>_ADDR/PORT/PROTO\n\n            - /etc/hosts\n\n    - ambassador\n        about: 代理连接\n        connect redis client and server via 2 ambassador\n                sudo docker run -d --name redis ag/redis\n                sudo docker run -d --name redis ag/redis\n\n# docker network design\n\nhttps://blog.docker.com/2016/03/docker-networking-design-philosophy/\n\n# cnm design\n\nhttps://github.com/docker/libnetwork/blob/master/docs/design.md\n\n# docker network command\n\n- docker network ls\n\n- docker network create\n\n- docker network connect\n\n- docker network disconnect\n\n- docker network inspect\n\n- docker network rm\n\n# docker network api\n\n- network driver api\n\n- IPAM api\n","slug":"docker-network-core","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-04-18T13:21:17.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4re2002nkxsvy32e1jsl","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><ul>\n<li><p>docker daemon ini process(docker -d)</p>\n<p>  [/var/lib/docker|116d5cd4] +job init_networkdriver()<br>  [/var/lib/docker|116d5cd4.init_networkdriver()] creating new bridge for docker0<br>  [/var/lib/docker|116d5cd4.init_networkdriver()] getting iface addr<br>  [/var/lib/docker|116d5cd4] -job init_networkdriver() = OK (0)</p>\n</li>\n<li><p>default mode<br>  bridge</p>\n<pre><code>docker0\n</code></pre></li>\n<li><p>expose port</p>\n<p>  -P: randomly expose a port between 49000-49900</p>\n<pre><code>sudo docker run -d -P traning/webapp python app.py\n</code></pre><p>  -p:</p>\n<pre><code>ip:hostPort:containerPost | ip::containerPort | hostPort:containerPort\n</code></pre></li>\n<li><p>check out network setting</p>\n<pre><code>sudo docker inspect --format &apos;{{.NetworkSettings}}&apos; CID\n</code></pre></li>\n<li><p>container link</p>\n<p>  about: docker0 bridge; iptables</p>\n<p>  –link name:alias</p>\n<pre><code>    sudo docker -d --name dbdata training/postgres\n    sudo docker run -d -P --name web --link dbdata:db training/webapp python app.py\n    sudo docker inspect web\n\nLinks: /dbdata:/web/db\n\nhow web container use dbdata:\n\n    - env variable\n        sudo docker run --rm --name web2 --link dbdata:webdb training/webapp env\n        &lt;name&gt;_PORT_&lt;port&gt;_&lt;protocol&gt;_ADDR/PORT/PROTO\n\n    - /etc/hosts\n</code></pre><ul>\n<li>ambassador<br>  about: 代理连接<br>  connect redis client and server via 2 ambassador<pre><code>sudo docker run -d --name redis ag/redis\nsudo docker run -d --name redis ag/redis\n</code></pre></li>\n</ul>\n</li>\n</ul>\n<h1 id=\"docker-network-design\"><a href=\"#docker-network-design\" class=\"headerlink\" title=\"docker network design\"></a>docker network design</h1><p><a href=\"https://blog.docker.com/2016/03/docker-networking-design-philosophy/\" target=\"_blank\" rel=\"external\">https://blog.docker.com/2016/03/docker-networking-design-philosophy/</a></p>\n<h1 id=\"cnm-design\"><a href=\"#cnm-design\" class=\"headerlink\" title=\"cnm design\"></a>cnm design</h1><p><a href=\"https://github.com/docker/libnetwork/blob/master/docs/design.md\" target=\"_blank\" rel=\"external\">https://github.com/docker/libnetwork/blob/master/docs/design.md</a></p>\n<h1 id=\"docker-network-command\"><a href=\"#docker-network-command\" class=\"headerlink\" title=\"docker network command\"></a>docker network command</h1><ul>\n<li><p>docker network ls</p>\n</li>\n<li><p>docker network create</p>\n</li>\n<li><p>docker network connect</p>\n</li>\n<li><p>docker network disconnect</p>\n</li>\n<li><p>docker network inspect</p>\n</li>\n<li><p>docker network rm</p>\n</li>\n</ul>\n<h1 id=\"docker-network-api\"><a href=\"#docker-network-api\" class=\"headerlink\" title=\"docker network api\"></a>docker network api</h1><ul>\n<li><p>network driver api</p>\n</li>\n<li><p>IPAM api</p>\n</li>\n</ul>\n","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><ul>\n<li><p>docker daemon ini process(docker -d)</p>\n<p>  [/var/lib/docker|116d5cd4] +job init_networkdriver()<br>  [/var/lib/docker|116d5cd4.init_networkdriver()] creating new bridge for docker0<br>  [/var/lib/docker|116d5cd4.init_networkdriver()] getting iface addr<br>  [/var/lib/docker|116d5cd4] -job init_networkdriver() = OK (0)</p>\n</li>\n<li><p>default mode<br>  bridge</p>\n<pre><code>docker0\n</code></pre></li>\n<li><p>expose port</p>\n<p>  -P: randomly expose a port between 49000-49900</p>\n<pre><code>sudo docker run -d -P traning/webapp python app.py\n</code></pre><p>  -p:</p>\n<pre><code>ip:hostPort:containerPost | ip::containerPort | hostPort:containerPort\n</code></pre></li>\n<li><p>check out network setting</p>\n<pre><code>sudo docker inspect --format &apos;{{.NetworkSettings}}&apos; CID\n</code></pre></li>\n<li><p>container link</p>\n<p>  about: docker0 bridge; iptables</p>\n<p>  –link name:alias</p>\n<pre><code>    sudo docker -d --name dbdata training/postgres\n    sudo docker run -d -P --name web --link dbdata:db training/webapp python app.py\n    sudo docker inspect web\n\nLinks: /dbdata:/web/db\n\nhow web container use dbdata:\n\n    - env variable\n        sudo docker run --rm --name web2 --link dbdata:webdb training/webapp env\n        &lt;name&gt;_PORT_&lt;port&gt;_&lt;protocol&gt;_ADDR/PORT/PROTO\n\n    - /etc/hosts\n</code></pre><ul>\n<li>ambassador<br>  about: 代理连接<br>  connect redis client and server via 2 ambassador<pre><code>sudo docker run -d --name redis ag/redis\nsudo docker run -d --name redis ag/redis\n</code></pre></li>\n</ul>\n</li>\n</ul>\n<h1 id=\"docker-network-design\"><a href=\"#docker-network-design\" class=\"headerlink\" title=\"docker network design\"></a>docker network design</h1><p><a href=\"https://blog.docker.com/2016/03/docker-networking-design-philosophy/\">https://blog.docker.com/2016/03/docker-networking-design-philosophy/</a></p>\n<h1 id=\"cnm-design\"><a href=\"#cnm-design\" class=\"headerlink\" title=\"cnm design\"></a>cnm design</h1><p><a href=\"https://github.com/docker/libnetwork/blob/master/docs/design.md\">https://github.com/docker/libnetwork/blob/master/docs/design.md</a></p>\n<h1 id=\"docker-network-command\"><a href=\"#docker-network-command\" class=\"headerlink\" title=\"docker network command\"></a>docker network command</h1><ul>\n<li><p>docker network ls</p>\n</li>\n<li><p>docker network create</p>\n</li>\n<li><p>docker network connect</p>\n</li>\n<li><p>docker network disconnect</p>\n</li>\n<li><p>docker network inspect</p>\n</li>\n<li><p>docker network rm</p>\n</li>\n</ul>\n<h1 id=\"docker-network-api\"><a href=\"#docker-network-api\" class=\"headerlink\" title=\"docker network api\"></a>docker network api</h1><ul>\n<li><p>network driver api</p>\n</li>\n<li><p>IPAM api</p>\n</li>\n</ul>\n"},{"title":"docker security core","_content":"\n# docker security core\n\n# cgroups\n\n# capability\n\n# notary\nhttps://github.com/docker/notary\n","source":"_posts/docker-security-core.md","raw":"---\ntitle: docker security core\ncategories:\n- docker\ntags:\n- core\n- security\n---\n\n# docker security core\n\n# cgroups\n\n# capability\n\n# notary\nhttps://github.com/docker/notary\n","slug":"docker-security-core","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-04-18T13:21:09.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4reb002qkxsvrcfagwk5","content":"<h1 id=\"docker-security-core\"><a href=\"#docker-security-core\" class=\"headerlink\" title=\"docker security core\"></a>docker security core</h1><h1 id=\"cgroups\"><a href=\"#cgroups\" class=\"headerlink\" title=\"cgroups\"></a>cgroups</h1><h1 id=\"capability\"><a href=\"#capability\" class=\"headerlink\" title=\"capability\"></a>capability</h1><h1 id=\"notary\"><a href=\"#notary\" class=\"headerlink\" title=\"notary\"></a>notary</h1><p><a href=\"https://github.com/docker/notary\" target=\"_blank\" rel=\"external\">https://github.com/docker/notary</a></p>\n","excerpt":"","more":"<h1 id=\"docker-security-core\"><a href=\"#docker-security-core\" class=\"headerlink\" title=\"docker security core\"></a>docker security core</h1><h1 id=\"cgroups\"><a href=\"#cgroups\" class=\"headerlink\" title=\"cgroups\"></a>cgroups</h1><h1 id=\"capability\"><a href=\"#capability\" class=\"headerlink\" title=\"capability\"></a>capability</h1><h1 id=\"notary\"><a href=\"#notary\" class=\"headerlink\" title=\"notary\"></a>notary</h1><p><a href=\"https://github.com/docker/notary\">https://github.com/docker/notary</a></p>\n"},{"title":"docker store detail","_content":"\n# docker store detail\n","source":"_posts/docker-store-detail.md","raw":"---\ntitle: docker store detail\ncategories:\n- docker\ntags:\n- detail\n---\n\n# docker store detail\n","slug":"docker-store-detail","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-04-18T13:20:52.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4reh002tkxsvn2nx8m5o","content":"<h1 id=\"docker-store-detail\"><a href=\"#docker-store-detail\" class=\"headerlink\" title=\"docker store detail\"></a>docker store detail</h1>","excerpt":"","more":"<h1 id=\"docker-store-detail\"><a href=\"#docker-store-detail\" class=\"headerlink\" title=\"docker store detail\"></a>docker store detail</h1>"},{"title":"docker storage core","_content":"\n# about\n\n- data volumes 数据卷\n\n    - create\n\n        using dockerfile:\n\n                VOLUME /var/lib/postgresql\n\n        docker run -v:\n\n                docker run -d -P -v /webapp training/webapp python app.py\n                docker inspect my_data\n                docker inspect --format {{.Volums}} my_data\n\n    - mount file\n\n            $ sudo docker run --rm -it -v ~/.bash_history:/.bash_history ubuntu /bin/bash\n\n    - mount folder\n\n            $ sudo docker run -d -P --name web -v /src/webapp:/opt/webapp training/webapp python app.py\n\n    - mount local directory\n\n            sudo docker run -d -P --name webapp -v `pwd`:/webapp:ro training/webapp python app.py\n\n- data volume containers 数据卷容器\n\n    - tips:\n\n            $ sudo docker run -it -v /dbdata --name dbdata training/postgres\n            sudo docker run -d --volumes-from=dbdata --name db1 training/postgres\n            sudo docker run -d --name db2 --volumes-from=dbdata training/postgres\n            sudo docker run -d --name db2 --volumes-from=db1 training/postgres\n            docker rm -v db3\n\n    - migration\n\n        backup:\n\n                $ sudo docker run --volumes-from dbdata -v $(pwd):/backup --name worker ubuntu tar cvf /backup/backup.tar /dbdata\n                should use sudo\n\n        restore:\n\n                $ sudo docker run -v /dbdata --name dbdata2 ubuntu /bin/bash\n\n# link\n[Manage data in containers](https://docs.docker.com/engine/userguide/containers/dockervolumes/)\n\n# docker storage command\n\n- docker volume create\n\n- docker volume inspect\n\n- docker volume ls\n\n- docker volume rm\n","source":"_posts/docker-storage-core.md","raw":"---\ntitle: docker storage core\ncategories:\n- docker\ntags:\n- core\n- storage\n---\n\n# about\n\n- data volumes 数据卷\n\n    - create\n\n        using dockerfile:\n\n                VOLUME /var/lib/postgresql\n\n        docker run -v:\n\n                docker run -d -P -v /webapp training/webapp python app.py\n                docker inspect my_data\n                docker inspect --format {{.Volums}} my_data\n\n    - mount file\n\n            $ sudo docker run --rm -it -v ~/.bash_history:/.bash_history ubuntu /bin/bash\n\n    - mount folder\n\n            $ sudo docker run -d -P --name web -v /src/webapp:/opt/webapp training/webapp python app.py\n\n    - mount local directory\n\n            sudo docker run -d -P --name webapp -v `pwd`:/webapp:ro training/webapp python app.py\n\n- data volume containers 数据卷容器\n\n    - tips:\n\n            $ sudo docker run -it -v /dbdata --name dbdata training/postgres\n            sudo docker run -d --volumes-from=dbdata --name db1 training/postgres\n            sudo docker run -d --name db2 --volumes-from=dbdata training/postgres\n            sudo docker run -d --name db2 --volumes-from=db1 training/postgres\n            docker rm -v db3\n\n    - migration\n\n        backup:\n\n                $ sudo docker run --volumes-from dbdata -v $(pwd):/backup --name worker ubuntu tar cvf /backup/backup.tar /dbdata\n                should use sudo\n\n        restore:\n\n                $ sudo docker run -v /dbdata --name dbdata2 ubuntu /bin/bash\n\n# link\n[Manage data in containers](https://docs.docker.com/engine/userguide/containers/dockervolumes/)\n\n# docker storage command\n\n- docker volume create\n\n- docker volume inspect\n\n- docker volume ls\n\n- docker volume rm\n","slug":"docker-storage-core","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-04-18T13:21:13.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rek002wkxsvtpp5poiw","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><ul>\n<li><p>data volumes 数据卷</p>\n<ul>\n<li><p>create</p>\n<p>  using dockerfile:</p>\n<pre><code>VOLUME /var/lib/postgresql\n</code></pre><p>  docker run -v:</p>\n<pre><code>docker run -d -P -v /webapp training/webapp python app.py\ndocker inspect my_data\ndocker inspect --format {{.Volums}} my_data\n</code></pre></li>\n<li><p>mount file</p>\n<pre><code>$ sudo docker run --rm -it -v ~/.bash_history:/.bash_history ubuntu /bin/bash\n</code></pre></li>\n<li><p>mount folder</p>\n<pre><code>$ sudo docker run -d -P --name web -v /src/webapp:/opt/webapp training/webapp python app.py\n</code></pre></li>\n<li><p>mount local directory</p>\n<pre><code>sudo docker run -d -P --name webapp -v `pwd`:/webapp:ro training/webapp python app.py\n</code></pre></li>\n</ul>\n</li>\n<li><p>data volume containers 数据卷容器</p>\n<ul>\n<li><p>tips:</p>\n<pre><code>$ sudo docker run -it -v /dbdata --name dbdata training/postgres\nsudo docker run -d --volumes-from=dbdata --name db1 training/postgres\nsudo docker run -d --name db2 --volumes-from=dbdata training/postgres\nsudo docker run -d --name db2 --volumes-from=db1 training/postgres\ndocker rm -v db3\n</code></pre></li>\n<li><p>migration</p>\n<p>  backup:</p>\n<pre><code>$ sudo docker run --volumes-from dbdata -v $(pwd):/backup --name worker ubuntu tar cvf /backup/backup.tar /dbdata\nshould use sudo\n</code></pre><p>  restore:</p>\n<pre><code>$ sudo docker run -v /dbdata --name dbdata2 ubuntu /bin/bash\n</code></pre></li>\n</ul>\n</li>\n</ul>\n<h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><p><a href=\"https://docs.docker.com/engine/userguide/containers/dockervolumes/\" target=\"_blank\" rel=\"external\">Manage data in containers</a></p>\n<h1 id=\"docker-storage-command\"><a href=\"#docker-storage-command\" class=\"headerlink\" title=\"docker storage command\"></a>docker storage command</h1><ul>\n<li><p>docker volume create</p>\n</li>\n<li><p>docker volume inspect</p>\n</li>\n<li><p>docker volume ls</p>\n</li>\n<li><p>docker volume rm</p>\n</li>\n</ul>\n","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><ul>\n<li><p>data volumes 数据卷</p>\n<ul>\n<li><p>create</p>\n<p>  using dockerfile:</p>\n<pre><code>VOLUME /var/lib/postgresql\n</code></pre><p>  docker run -v:</p>\n<pre><code>docker run -d -P -v /webapp training/webapp python app.py\ndocker inspect my_data\ndocker inspect --format {{.Volums}} my_data\n</code></pre></li>\n<li><p>mount file</p>\n<pre><code>$ sudo docker run --rm -it -v ~/.bash_history:/.bash_history ubuntu /bin/bash\n</code></pre></li>\n<li><p>mount folder</p>\n<pre><code>$ sudo docker run -d -P --name web -v /src/webapp:/opt/webapp training/webapp python app.py\n</code></pre></li>\n<li><p>mount local directory</p>\n<pre><code>sudo docker run -d -P --name webapp -v `pwd`:/webapp:ro training/webapp python app.py\n</code></pre></li>\n</ul>\n</li>\n<li><p>data volume containers 数据卷容器</p>\n<ul>\n<li><p>tips:</p>\n<pre><code>$ sudo docker run -it -v /dbdata --name dbdata training/postgres\nsudo docker run -d --volumes-from=dbdata --name db1 training/postgres\nsudo docker run -d --name db2 --volumes-from=dbdata training/postgres\nsudo docker run -d --name db2 --volumes-from=db1 training/postgres\ndocker rm -v db3\n</code></pre></li>\n<li><p>migration</p>\n<p>  backup:</p>\n<pre><code>$ sudo docker run --volumes-from dbdata -v $(pwd):/backup --name worker ubuntu tar cvf /backup/backup.tar /dbdata\nshould use sudo\n</code></pre><p>  restore:</p>\n<pre><code>$ sudo docker run -v /dbdata --name dbdata2 ubuntu /bin/bash\n</code></pre></li>\n</ul>\n</li>\n</ul>\n<h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><p><a href=\"https://docs.docker.com/engine/userguide/containers/dockervolumes/\">Manage data in containers</a></p>\n<h1 id=\"docker-storage-command\"><a href=\"#docker-storage-command\" class=\"headerlink\" title=\"docker storage command\"></a>docker storage command</h1><ul>\n<li><p>docker volume create</p>\n</li>\n<li><p>docker volume inspect</p>\n</li>\n<li><p>docker volume ls</p>\n</li>\n<li><p>docker volume rm</p>\n</li>\n</ul>\n"},{"title":"docker swarm detail","_content":"\n# about\n\n\n\n# docker swarm command\n\n- docker-machine ls\n\n- docker-machine create -d virtualbox local\n\n- $(docker-machine env local)\n\n- docker run swarm create\n\n        $  docker-machine create -d virtualbox --swarm --swarm-master --swarm-discovery token://63e7a1adb607ce4db056a29b1f5d30cf swarm-master\n\n        $(docker-machine env --swarm swarm-master)\n\n- docker-machine ls\n","source":"_posts/docker-swarm-detail.md","raw":"---\ntitle: docker swarm detail\ncategories:\n- docker\ntags:\n- detail\n---\n\n# about\n\n\n\n# docker swarm command\n\n- docker-machine ls\n\n- docker-machine create -d virtualbox local\n\n- $(docker-machine env local)\n\n- docker run swarm create\n\n        $  docker-machine create -d virtualbox --swarm --swarm-master --swarm-discovery token://63e7a1adb607ce4db056a29b1f5d30cf swarm-master\n\n        $(docker-machine env --swarm swarm-master)\n\n- docker-machine ls\n","slug":"docker-swarm-detail","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-04-18T13:21:21.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4reo0030kxsv60te7vm8","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><h1 id=\"docker-swarm-command\"><a href=\"#docker-swarm-command\" class=\"headerlink\" title=\"docker swarm command\"></a>docker swarm command</h1><ul>\n<li><p>docker-machine ls</p>\n</li>\n<li><p>docker-machine create -d virtualbox local</p>\n</li>\n<li><p>$(docker-machine env local)</p>\n</li>\n<li><p>docker run swarm create</p>\n<pre><code>$  docker-machine create -d virtualbox --swarm --swarm-master --swarm-discovery token://63e7a1adb607ce4db056a29b1f5d30cf swarm-master\n\n$(docker-machine env --swarm swarm-master)\n</code></pre></li>\n<li><p>docker-machine ls</p>\n</li>\n</ul>\n","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><h1 id=\"docker-swarm-command\"><a href=\"#docker-swarm-command\" class=\"headerlink\" title=\"docker swarm command\"></a>docker swarm command</h1><ul>\n<li><p>docker-machine ls</p>\n</li>\n<li><p>docker-machine create -d virtualbox local</p>\n</li>\n<li><p>$(docker-machine env local)</p>\n</li>\n<li><p>docker run swarm create</p>\n<pre><code>$  docker-machine create -d virtualbox --swarm --swarm-master --swarm-discovery token://63e7a1adb607ce4db056a29b1f5d30cf swarm-master\n\n$(docker-machine env --swarm swarm-master)\n</code></pre></li>\n<li><p>docker-machine ls</p>\n</li>\n</ul>\n"},{"title":"docker swarmkit detail","_content":"\n# about\n\n- link: https://github.com/docker/swarmkit\n\n#\n","source":"_posts/docker-swarmkit-detail.md","raw":"---\ntitle: docker swarmkit detail\ncategories:\n- iaas\ntags:\n- detail\n- docker\n- swarmkit\n---\n\n# about\n\n- link: https://github.com/docker/swarmkit\n\n#\n","slug":"docker-swarmkit-detail","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-04-18T13:21:48.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4req0032kxsv9mfll6v4","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><ul>\n<li>link: <a href=\"https://github.com/docker/swarmkit\" target=\"_blank\" rel=\"external\">https://github.com/docker/swarmkit</a></li>\n</ul>\n<p>#</p>\n","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><ul>\n<li>link: <a href=\"https://github.com/docker/swarmkit\">https://github.com/docker/swarmkit</a></li>\n</ul>\n<p>#</p>\n"},{"title":"mesos portal","_content":"\n# mesos portal\n","source":"_posts/docker-mesos-portal.md","raw":"---\ntitle: mesos portal\ncategories:\n- docker\ntags:\n- core\n- mesos\n---\n\n# mesos portal\n","slug":"docker-mesos-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-04-18T13:19:18.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rew0037kxsv4mj8yqp5","content":"<h1 id=\"mesos-portal\"><a href=\"#mesos-portal\" class=\"headerlink\" title=\"mesos portal\"></a>mesos portal</h1>","excerpt":"","more":"<h1 id=\"mesos-portal\"><a href=\"#mesos-portal\" class=\"headerlink\" title=\"mesos portal\"></a>mesos portal</h1>"},{"title":"docker yaml file detail","_content":"\n# about\n\n# link\n\nhttp://www.yaml.org/\nhttps://en.wikipedia.org/wiki/YAML\n","source":"_posts/docker-yaml-detail.md","raw":"---\ntitle: docker yaml file detail\ncategories:\n- docker\ntags:\n- detail\n- yaml\n---\n\n# about\n\n# link\n\nhttp://www.yaml.org/\nhttps://en.wikipedia.org/wiki/YAML\n","slug":"docker-yaml-detail","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-04-18T13:22:18.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rf00039kxsvn0a835sy","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><p><a href=\"http://www.yaml.org/\" target=\"_blank\" rel=\"external\">http://www.yaml.org/</a><br><a href=\"https://en.wikipedia.org/wiki/YAML\" target=\"_blank\" rel=\"external\">https://en.wikipedia.org/wiki/YAML</a></p>\n","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><p><a href=\"http://www.yaml.org/\">http://www.yaml.org/</a><br><a href=\"https://en.wikipedia.org/wiki/YAML\">https://en.wikipedia.org/wiki/YAML</a></p>\n"},{"title":"mesos core","_content":"\n#  core\n\nhttps://github.com/Dataman-Cloud/Mesos-CN/blob/master/SUMMARY.md","source":"_posts/docker-mesos-core.md","raw":"---\ntitle: mesos core\ncategories:\n- docker\ntags:\n- core\n---\n\n#  core\n\nhttps://github.com/Dataman-Cloud/Mesos-CN/blob/master/SUMMARY.md","slug":"docker-mesos-core","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-02-10T06:16:56.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rf3003dkxsv48kfkoaq","content":"<h1 id=\"core\"><a href=\"#core\" class=\"headerlink\" title=\"core\"></a>core</h1><p><a href=\"https://github.com/Dataman-Cloud/Mesos-CN/blob/master/SUMMARY.md\" target=\"_blank\" rel=\"external\">https://github.com/Dataman-Cloud/Mesos-CN/blob/master/SUMMARY.md</a></p>\n","excerpt":"","more":"<h1 id=\"core\"><a href=\"#core\" class=\"headerlink\" title=\"core\"></a>core</h1><p><a href=\"https://github.com/Dataman-Cloud/Mesos-CN/blob/master/SUMMARY.md\">https://github.com/Dataman-Cloud/Mesos-CN/blob/master/SUMMARY.md</a></p>\n"},{"title":"iot portal","_content":"\n# core\n","source":"_posts/iot-portal.md","raw":"---\ntitle: iot portal\ncategories:\n- iot\ntags:\n- portal\n- iot\n---\n\n# core\n","slug":"iot-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-04-18T13:22:41.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rf6003gkxsvyjy2mrjq","content":"<h1 id=\"core\"><a href=\"#core\" class=\"headerlink\" title=\"core\"></a>core</h1>","excerpt":"","more":"<h1 id=\"core\"><a href=\"#core\" class=\"headerlink\" title=\"core\"></a>core</h1>"},{"title":"programming language portal","_content":"","source":"_posts/lang--portal.md","raw":"---\ntitle: programming language portal\ncategories:\n- programming language\ntags:\n- portal\n- programming language\n---\n","slug":"lang--portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-04-18T13:23:17.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rf9003kkxsvtlouqbco","content":"","excerpt":"","more":""},{"title":"go programming language portal","_content":"\n# about\n\n# link\ncommunity: http://golangtc.com/\ndoc: http://godoc.golangtc.com/\nblog: https://blog.golang.org/\n\n# env\nhttp://golangtc.com/t/587edd57b09ecc2e18000169\n\n# doc\nhttp://godoc.golangtc.com/doc/effective_go.html\n","source":"_posts/lang-go-portal.md","raw":"---\ntitle: go programming language portal\ncategories:\n- programming language\ntags:\n- portal\n- programming language\n---\n\n# about\n\n# link\ncommunity: http://golangtc.com/\ndoc: http://godoc.golangtc.com/\nblog: https://blog.golang.org/\n\n# env\nhttp://golangtc.com/t/587edd57b09ecc2e18000169\n\n# doc\nhttp://godoc.golangtc.com/doc/effective_go.html\n","slug":"lang-go-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-04-18T13:23:53.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rfb003mkxsvedf5bx9f","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><p>community: <a href=\"http://golangtc.com/\" target=\"_blank\" rel=\"external\">http://golangtc.com/</a><br>doc: <a href=\"http://godoc.golangtc.com/\" target=\"_blank\" rel=\"external\">http://godoc.golangtc.com/</a><br>blog: <a href=\"https://blog.golang.org/\" target=\"_blank\" rel=\"external\">https://blog.golang.org/</a></p>\n<h1 id=\"env\"><a href=\"#env\" class=\"headerlink\" title=\"env\"></a>env</h1><p><a href=\"http://golangtc.com/t/587edd57b09ecc2e18000169\" target=\"_blank\" rel=\"external\">http://golangtc.com/t/587edd57b09ecc2e18000169</a></p>\n<h1 id=\"doc\"><a href=\"#doc\" class=\"headerlink\" title=\"doc\"></a>doc</h1><p><a href=\"http://godoc.golangtc.com/doc/effective_go.html\" target=\"_blank\" rel=\"external\">http://godoc.golangtc.com/doc/effective_go.html</a></p>\n","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><p>community: <a href=\"http://golangtc.com/\">http://golangtc.com/</a><br>doc: <a href=\"http://godoc.golangtc.com/\">http://godoc.golangtc.com/</a><br>blog: <a href=\"https://blog.golang.org/\">https://blog.golang.org/</a></p>\n<h1 id=\"env\"><a href=\"#env\" class=\"headerlink\" title=\"env\"></a>env</h1><p><a href=\"http://golangtc.com/t/587edd57b09ecc2e18000169\">http://golangtc.com/t/587edd57b09ecc2e18000169</a></p>\n<h1 id=\"doc\"><a href=\"#doc\" class=\"headerlink\" title=\"doc\"></a>doc</h1><p><a href=\"http://godoc.golangtc.com/doc/effective_go.html\">http://godoc.golangtc.com/doc/effective_go.html</a></p>\n"},{"title":"java programming language portal","_content":"\n#  portal\n\njava one:https://www.oracle.com/javaone\n\n# spring boot\nhttp://www.infoq.com/cn/articles/microframeworks1-spring-boot\n","source":"_posts/lang-java-portal.md","raw":"---\ntitle: java programming language portal\ncategories:\n- programming language\ntags:\n- portal\n- programming language\n---\n\n#  portal\n\njava one:https://www.oracle.com/javaone\n\n# spring boot\nhttp://www.infoq.com/cn/articles/microframeworks1-spring-boot\n","slug":"lang-java-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-04-18T13:23:49.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rfg003okxsvahdj6s25","content":"<h1 id=\"portal\"><a href=\"#portal\" class=\"headerlink\" title=\"portal\"></a>portal</h1><p>java one:<a href=\"https://www.oracle.com/javaone\" target=\"_blank\" rel=\"external\">https://www.oracle.com/javaone</a></p>\n<h1 id=\"spring-boot\"><a href=\"#spring-boot\" class=\"headerlink\" title=\"spring boot\"></a>spring boot</h1><p><a href=\"http://www.infoq.com/cn/articles/microframeworks1-spring-boot\" target=\"_blank\" rel=\"external\">http://www.infoq.com/cn/articles/microframeworks1-spring-boot</a></p>\n","excerpt":"","more":"<h1 id=\"portal\"><a href=\"#portal\" class=\"headerlink\" title=\"portal\"></a>portal</h1><p>java one:<a href=\"https://www.oracle.com/javaone\">https://www.oracle.com/javaone</a></p>\n<h1 id=\"spring-boot\"><a href=\"#spring-boot\" class=\"headerlink\" title=\"spring boot\"></a>spring boot</h1><p><a href=\"http://www.infoq.com/cn/articles/microframeworks1-spring-boot\">http://www.infoq.com/cn/articles/microframeworks1-spring-boot</a></p>\n"},{"title":"nodejs core","_content":"","source":"_posts/lang-js-nodejs-core.md","raw":"---\ntitle: nodejs core\ncategories:\n- nodejs\ntags:\n- core\n---\n","slug":"lang-js-nodejs-core","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-04-18T13:24:08.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rfn003skxsvsn2vbv97","content":"","excerpt":"","more":""},{"title":"js programming language portal","_content":"\n# about\nvs: https://da-14.com/blog/reactjs-vs-angular-comparison-which-better\nhttps://hackernoon.com/5-best-javascript-frameworks-in-2017-7a63b3870282?gi=47ffee68edc6\n\nhttps://colorlib.com/wp/javascript-frameworks/\nhttps://medium.com/@ZombieCodeKill/choosing-a-javascript-framework-535745d0ab90#.bj206ygou\n\n#jquery\nhttp://jquery.com/\n","source":"_posts/lang-js-portal.md","raw":"---\ntitle: js programming language portal\ncategories:\n- programming language\ntags:\n- portal\n- programming language\n---\n\n# about\nvs: https://da-14.com/blog/reactjs-vs-angular-comparison-which-better\nhttps://hackernoon.com/5-best-javascript-frameworks-in-2017-7a63b3870282?gi=47ffee68edc6\n\nhttps://colorlib.com/wp/javascript-frameworks/\nhttps://medium.com/@ZombieCodeKill/choosing-a-javascript-framework-535745d0ab90#.bj206ygou\n\n#jquery\nhttp://jquery.com/\n","slug":"lang-js-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-04-18T13:24:24.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rfq003vkxsvfu6sj2e1","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><p>vs: <a href=\"https://da-14.com/blog/reactjs-vs-angular-comparison-which-better\" target=\"_blank\" rel=\"external\">https://da-14.com/blog/reactjs-vs-angular-comparison-which-better</a><br><a href=\"https://hackernoon.com/5-best-javascript-frameworks-in-2017-7a63b3870282?gi=47ffee68edc6\" target=\"_blank\" rel=\"external\">https://hackernoon.com/5-best-javascript-frameworks-in-2017-7a63b3870282?gi=47ffee68edc6</a></p>\n<p><a href=\"https://colorlib.com/wp/javascript-frameworks/\" target=\"_blank\" rel=\"external\">https://colorlib.com/wp/javascript-frameworks/</a><br><a href=\"https://medium.com/@ZombieCodeKill/choosing-a-javascript-framework-535745d0ab90#.bj206ygou\" target=\"_blank\" rel=\"external\">https://medium.com/@ZombieCodeKill/choosing-a-javascript-framework-535745d0ab90#.bj206ygou</a></p>\n<p>#jquery<br><a href=\"http://jquery.com/\" target=\"_blank\" rel=\"external\">http://jquery.com/</a></p>\n","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><p>vs: <a href=\"https://da-14.com/blog/reactjs-vs-angular-comparison-which-better\">https://da-14.com/blog/reactjs-vs-angular-comparison-which-better</a><br><a href=\"https://hackernoon.com/5-best-javascript-frameworks-in-2017-7a63b3870282?gi=47ffee68edc6\">https://hackernoon.com/5-best-javascript-frameworks-in-2017-7a63b3870282?gi=47ffee68edc6</a></p>\n<p><a href=\"https://colorlib.com/wp/javascript-frameworks/\">https://colorlib.com/wp/javascript-frameworks/</a><br><a href=\"https://medium.com/@ZombieCodeKill/choosing-a-javascript-framework-535745d0ab90#.bj206ygou\">https://medium.com/@ZombieCodeKill/choosing-a-javascript-framework-535745d0ab90#.bj206ygou</a></p>\n<p>#jquery<br><a href=\"http://jquery.com/\">http://jquery.com/</a></p>\n"},{"title":"php programming language portal","_content":"\n# framework\nyii\nlaravel: https://laravel.com/\ncakephp\nthinkphp: https://github.com/top-think/think\n\n# yii\nhttp://www.yiiframework.com/\nhttp://www.yiichina.com/\n\nperformance： yii+workerman、yii+swoole\napi：https://github.com/bolasblack/http-api-guide\n\n\n\n# laravel\nhttps://laravel-china.org/\ndocker: http://www.jianshu.com/p/2ad17b1df2c0\nvs:http://www.danbp.org/p/node/79\n\n# cakephp\nhttps://cakephp.org/\nhttps://book.cakephp.org/3.0/en/index.html\n","source":"_posts/lang-php-portal.md","raw":"---\ntitle: php programming language portal\ncategories:\n- php\ntags:\n- portal\n- programming language\n---\n\n# framework\nyii\nlaravel: https://laravel.com/\ncakephp\nthinkphp: https://github.com/top-think/think\n\n# yii\nhttp://www.yiiframework.com/\nhttp://www.yiichina.com/\n\nperformance： yii+workerman、yii+swoole\napi：https://github.com/bolasblack/http-api-guide\n\n\n\n# laravel\nhttps://laravel-china.org/\ndocker: http://www.jianshu.com/p/2ad17b1df2c0\nvs:http://www.danbp.org/p/node/79\n\n# cakephp\nhttps://cakephp.org/\nhttps://book.cakephp.org/3.0/en/index.html\n","slug":"lang-php-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-04-18T13:24:38.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rfu003zkxsvqf61sj9w","content":"<h1 id=\"framework\"><a href=\"#framework\" class=\"headerlink\" title=\"framework\"></a>framework</h1><p>yii<br>laravel: <a href=\"https://laravel.com/\" target=\"_blank\" rel=\"external\">https://laravel.com/</a><br>cakephp<br>thinkphp: <a href=\"https://github.com/top-think/think\" target=\"_blank\" rel=\"external\">https://github.com/top-think/think</a></p>\n<h1 id=\"yii\"><a href=\"#yii\" class=\"headerlink\" title=\"yii\"></a>yii</h1><p><a href=\"http://www.yiiframework.com/\" target=\"_blank\" rel=\"external\">http://www.yiiframework.com/</a><br><a href=\"http://www.yiichina.com/\" target=\"_blank\" rel=\"external\">http://www.yiichina.com/</a></p>\n<p>performance： yii+workerman、yii+swoole<br>api：<a href=\"https://github.com/bolasblack/http-api-guide\" target=\"_blank\" rel=\"external\">https://github.com/bolasblack/http-api-guide</a></p>\n<h1 id=\"laravel\"><a href=\"#laravel\" class=\"headerlink\" title=\"laravel\"></a>laravel</h1><p><a href=\"https://laravel-china.org/\" target=\"_blank\" rel=\"external\">https://laravel-china.org/</a><br>docker: <a href=\"http://www.jianshu.com/p/2ad17b1df2c0\" target=\"_blank\" rel=\"external\">http://www.jianshu.com/p/2ad17b1df2c0</a><br>vs:<a href=\"http://www.danbp.org/p/node/79\" target=\"_blank\" rel=\"external\">http://www.danbp.org/p/node/79</a></p>\n<h1 id=\"cakephp\"><a href=\"#cakephp\" class=\"headerlink\" title=\"cakephp\"></a>cakephp</h1><p><a href=\"https://cakephp.org/\" target=\"_blank\" rel=\"external\">https://cakephp.org/</a><br><a href=\"https://book.cakephp.org/3.0/en/index.html\" target=\"_blank\" rel=\"external\">https://book.cakephp.org/3.0/en/index.html</a></p>\n","excerpt":"","more":"<h1 id=\"framework\"><a href=\"#framework\" class=\"headerlink\" title=\"framework\"></a>framework</h1><p>yii<br>laravel: <a href=\"https://laravel.com/\">https://laravel.com/</a><br>cakephp<br>thinkphp: <a href=\"https://github.com/top-think/think\">https://github.com/top-think/think</a></p>\n<h1 id=\"yii\"><a href=\"#yii\" class=\"headerlink\" title=\"yii\"></a>yii</h1><p><a href=\"http://www.yiiframework.com/\">http://www.yiiframework.com/</a><br><a href=\"http://www.yiichina.com/\">http://www.yiichina.com/</a></p>\n<p>performance： yii+workerman、yii+swoole<br>api：<a href=\"https://github.com/bolasblack/http-api-guide\">https://github.com/bolasblack/http-api-guide</a></p>\n<h1 id=\"laravel\"><a href=\"#laravel\" class=\"headerlink\" title=\"laravel\"></a>laravel</h1><p><a href=\"https://laravel-china.org/\">https://laravel-china.org/</a><br>docker: <a href=\"http://www.jianshu.com/p/2ad17b1df2c0\">http://www.jianshu.com/p/2ad17b1df2c0</a><br>vs:<a href=\"http://www.danbp.org/p/node/79\">http://www.danbp.org/p/node/79</a></p>\n<h1 id=\"cakephp\"><a href=\"#cakephp\" class=\"headerlink\" title=\"cakephp\"></a>cakephp</h1><p><a href=\"https://cakephp.org/\">https://cakephp.org/</a><br><a href=\"https://book.cakephp.org/3.0/en/index.html\">https://book.cakephp.org/3.0/en/index.html</a></p>\n"},{"title":"cx programming language portal","_content":"","source":"_posts/lang-cx-portal.md","raw":"---\ntitle: cx programming language portal\ncategories:\n- programming language\ntags:\n- portal\n- programming language\n---\n","slug":"lang-cx-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-04-18T13:23:57.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rfy0041kxsvwkurnxev","content":"","excerpt":"","more":""},{"title":"Hello World","_content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n","source":"_posts/hello-world.md","raw":"---\ntitle: Hello World\ncategories:\n- tmp\n---\nWelcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n","slug":"hello-world","published":1,"date":"2017-01-11T09:26:51.000Z","updated":"2017-01-11T16:32:46.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rg40044kxsvngx5ivkb","content":"<p>Welcome to <a href=\"https://hexo.io/\" target=\"_blank\" rel=\"external\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\" target=\"_blank\" rel=\"external\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\" target=\"_blank\" rel=\"external\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\" target=\"_blank\" rel=\"external\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo new <span class=\"string\">\"My New Post\"</span></div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\" target=\"_blank\" rel=\"external\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo server</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\" target=\"_blank\" rel=\"external\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo generate</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\" target=\"_blank\" rel=\"external\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo deploy</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/deployment.html\" target=\"_blank\" rel=\"external\">Deployment</a></p>\n","excerpt":"","more":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo new <span class=\"string\">\"My New Post\"</span></div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo server</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo generate</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo deploy</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/deployment.html\">Deployment</a></p>\n"},{"title":"docker swarm portal","_content":"\n# about \n\nsource code: https://github.com/docker/swarmkit\ndoc: https://docs.docker.com/swarm/overview/\n\n# swarm next (swarm mode)\n\n- reference: http://mt.sohu.com/20160818/n464799101.shtml\n\nSwarmNext（Swarm 模式）是对原有Docker Swarm的巨大改善。在Docker中有服务对象将使缩放、滚动更新、服务发现、负载均衡和路由网等功能更容易实现，这也使得Swarm能够赶上Kubernetes之类的某些功能。Docker已经在1.12版中支持SwarmNext和Swarm，使得已经部署了Swarm的生产用户不会受到部分升级的影响。SwarmNext并不具有所有的功能，包括与Compose和存储插件的集成。不久这一点将被添加到SwarmNext。从长远看，我认为Swarm将会过时，而SwarmNext将会成为在Swarm中进行编排的唯一模式。Swarmkit作为一个开源项目，允许对Swarmkit进行独立开发，允许任何为分布应用开发编排系统的人将其作为一个独立模块进行使用。\n\n# swarm \n\n- link: https://docs.docker.com/engine/swarm/\n- tut: https://docs.docker.com/engine/swarm/swarm-tutorial/\n- cli: https://docs.docker.com/engine/swarm/#swarm-mode-cli-commands\n\n# swarmkit\n","source":"_posts/docker-swarm-portal.md","raw":"---\ntitle: docker swarm portal\ncategories:\n- docker\ntags:\n- detail\n---\n\n# about \n\nsource code: https://github.com/docker/swarmkit\ndoc: https://docs.docker.com/swarm/overview/\n\n# swarm next (swarm mode)\n\n- reference: http://mt.sohu.com/20160818/n464799101.shtml\n\nSwarmNext（Swarm 模式）是对原有Docker Swarm的巨大改善。在Docker中有服务对象将使缩放、滚动更新、服务发现、负载均衡和路由网等功能更容易实现，这也使得Swarm能够赶上Kubernetes之类的某些功能。Docker已经在1.12版中支持SwarmNext和Swarm，使得已经部署了Swarm的生产用户不会受到部分升级的影响。SwarmNext并不具有所有的功能，包括与Compose和存储插件的集成。不久这一点将被添加到SwarmNext。从长远看，我认为Swarm将会过时，而SwarmNext将会成为在Swarm中进行编排的唯一模式。Swarmkit作为一个开源项目，允许对Swarmkit进行独立开发，允许任何为分布应用开发编排系统的人将其作为一个独立模块进行使用。\n\n# swarm \n\n- link: https://docs.docker.com/engine/swarm/\n- tut: https://docs.docker.com/engine/swarm/swarm-tutorial/\n- cli: https://docs.docker.com/engine/swarm/#swarm-mode-cli-commands\n\n# swarmkit\n","slug":"docker-swarm-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-02-10T05:40:36.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rg80047kxsv3ojfvy9b","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><p>source code: <a href=\"https://github.com/docker/swarmkit\" target=\"_blank\" rel=\"external\">https://github.com/docker/swarmkit</a><br>doc: <a href=\"https://docs.docker.com/swarm/overview/\" target=\"_blank\" rel=\"external\">https://docs.docker.com/swarm/overview/</a></p>\n<h1 id=\"swarm-next-swarm-mode\"><a href=\"#swarm-next-swarm-mode\" class=\"headerlink\" title=\"swarm next (swarm mode)\"></a>swarm next (swarm mode)</h1><ul>\n<li>reference: <a href=\"http://mt.sohu.com/20160818/n464799101.shtml\" target=\"_blank\" rel=\"external\">http://mt.sohu.com/20160818/n464799101.shtml</a></li>\n</ul>\n<p>SwarmNext（Swarm 模式）是对原有Docker Swarm的巨大改善。在Docker中有服务对象将使缩放、滚动更新、服务发现、负载均衡和路由网等功能更容易实现，这也使得Swarm能够赶上Kubernetes之类的某些功能。Docker已经在1.12版中支持SwarmNext和Swarm，使得已经部署了Swarm的生产用户不会受到部分升级的影响。SwarmNext并不具有所有的功能，包括与Compose和存储插件的集成。不久这一点将被添加到SwarmNext。从长远看，我认为Swarm将会过时，而SwarmNext将会成为在Swarm中进行编排的唯一模式。Swarmkit作为一个开源项目，允许对Swarmkit进行独立开发，允许任何为分布应用开发编排系统的人将其作为一个独立模块进行使用。</p>\n<h1 id=\"swarm\"><a href=\"#swarm\" class=\"headerlink\" title=\"swarm\"></a>swarm</h1><ul>\n<li>link: <a href=\"https://docs.docker.com/engine/swarm/\" target=\"_blank\" rel=\"external\">https://docs.docker.com/engine/swarm/</a></li>\n<li>tut: <a href=\"https://docs.docker.com/engine/swarm/swarm-tutorial/\" target=\"_blank\" rel=\"external\">https://docs.docker.com/engine/swarm/swarm-tutorial/</a></li>\n<li>cli: <a href=\"https://docs.docker.com/engine/swarm/#swarm-mode-cli-commands\" target=\"_blank\" rel=\"external\">https://docs.docker.com/engine/swarm/#swarm-mode-cli-commands</a></li>\n</ul>\n<h1 id=\"swarmkit\"><a href=\"#swarmkit\" class=\"headerlink\" title=\"swarmkit\"></a>swarmkit</h1>","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><p>source code: <a href=\"https://github.com/docker/swarmkit\">https://github.com/docker/swarmkit</a><br>doc: <a href=\"https://docs.docker.com/swarm/overview/\">https://docs.docker.com/swarm/overview/</a></p>\n<h1 id=\"swarm-next-swarm-mode\"><a href=\"#swarm-next-swarm-mode\" class=\"headerlink\" title=\"swarm next (swarm mode)\"></a>swarm next (swarm mode)</h1><ul>\n<li>reference: <a href=\"http://mt.sohu.com/20160818/n464799101.shtml\">http://mt.sohu.com/20160818/n464799101.shtml</a></li>\n</ul>\n<p>SwarmNext（Swarm 模式）是对原有Docker Swarm的巨大改善。在Docker中有服务对象将使缩放、滚动更新、服务发现、负载均衡和路由网等功能更容易实现，这也使得Swarm能够赶上Kubernetes之类的某些功能。Docker已经在1.12版中支持SwarmNext和Swarm，使得已经部署了Swarm的生产用户不会受到部分升级的影响。SwarmNext并不具有所有的功能，包括与Compose和存储插件的集成。不久这一点将被添加到SwarmNext。从长远看，我认为Swarm将会过时，而SwarmNext将会成为在Swarm中进行编排的唯一模式。Swarmkit作为一个开源项目，允许对Swarmkit进行独立开发，允许任何为分布应用开发编排系统的人将其作为一个独立模块进行使用。</p>\n<h1 id=\"swarm\"><a href=\"#swarm\" class=\"headerlink\" title=\"swarm\"></a>swarm</h1><ul>\n<li>link: <a href=\"https://docs.docker.com/engine/swarm/\">https://docs.docker.com/engine/swarm/</a></li>\n<li>tut: <a href=\"https://docs.docker.com/engine/swarm/swarm-tutorial/\">https://docs.docker.com/engine/swarm/swarm-tutorial/</a></li>\n<li>cli: <a href=\"https://docs.docker.com/engine/swarm/#swarm-mode-cli-commands\">https://docs.docker.com/engine/swarm/#swarm-mode-cli-commands</a></li>\n</ul>\n<h1 id=\"swarmkit\"><a href=\"#swarmkit\" class=\"headerlink\" title=\"swarmkit\"></a>swarmkit</h1>"},{"title":"django core","_content":"\n# about\n\n    Full stack web frameworks.The most popular web framework in Python.\n\n    Django makes it easier to build better Web apps more quickly and with less code.\n    \n    Django is a high-level Python Web framework that encourages rapid development and clean, pragmatic design. Built by experienced developers, it takes care of much of the hassle of Web development, so you can focus on writing your app without needing to reinvent the wheel. It’s free and open source.\n\n    Ridiculously fast: Django was designed to help developers take applications from concept to completion as quickly as possible.\n\n    Reassuringly secure: Django takes security seriously and helps developers avoid many common security mistakes.\n\n    Exceedingly scalable: Some of the busiest sites on the Web leverage Django’s ability to quickly and flexibly scale.\n\n# link\n\n    - official: https://www.djangoproject.com/\n    - tut: https://docs.djangoproject.com/en/1.10/intro/tutorial01/\n    - awesome: https://github.com/rosarior/awesome-django\n    - community: http://django-china.cn/\n\n# install\n\n    - [install on mac (bare metal)]()\n    - [install on mac (docker)]()\n\n# read\n\n    - djangobook: http://docs.30c.org/djangobook2/; http://djangobook.py3k.cn/2.0/\n\n# mooc\n    \n    - http://www.imooc.com/learn/790\n\n    \n\n","source":"_posts/lang-python-django-core.md","raw":"---\ntitle: django core\ncategories:\n- python\ntags:\n- core\n- django\n---\n\n# about\n\n    Full stack web frameworks.The most popular web framework in Python.\n\n    Django makes it easier to build better Web apps more quickly and with less code.\n    \n    Django is a high-level Python Web framework that encourages rapid development and clean, pragmatic design. Built by experienced developers, it takes care of much of the hassle of Web development, so you can focus on writing your app without needing to reinvent the wheel. It’s free and open source.\n\n    Ridiculously fast: Django was designed to help developers take applications from concept to completion as quickly as possible.\n\n    Reassuringly secure: Django takes security seriously and helps developers avoid many common security mistakes.\n\n    Exceedingly scalable: Some of the busiest sites on the Web leverage Django’s ability to quickly and flexibly scale.\n\n# link\n\n    - official: https://www.djangoproject.com/\n    - tut: https://docs.djangoproject.com/en/1.10/intro/tutorial01/\n    - awesome: https://github.com/rosarior/awesome-django\n    - community: http://django-china.cn/\n\n# install\n\n    - [install on mac (bare metal)]()\n    - [install on mac (docker)]()\n\n# read\n\n    - djangobook: http://docs.30c.org/djangobook2/; http://djangobook.py3k.cn/2.0/\n\n# mooc\n    \n    - http://www.imooc.com/learn/790\n\n    \n\n","slug":"lang-python-django-core","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-12T09:47:39.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rgc004bkxsvx990biiu","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><pre><code>Full stack web frameworks.The most popular web framework in Python.\n\nDjango makes it easier to build better Web apps more quickly and with less code.\n\nDjango is a high-level Python Web framework that encourages rapid development and clean, pragmatic design. Built by experienced developers, it takes care of much of the hassle of Web development, so you can focus on writing your app without needing to reinvent the wheel. It’s free and open source.\n\nRidiculously fast: Django was designed to help developers take applications from concept to completion as quickly as possible.\n\nReassuringly secure: Django takes security seriously and helps developers avoid many common security mistakes.\n\nExceedingly scalable: Some of the busiest sites on the Web leverage Django’s ability to quickly and flexibly scale.\n</code></pre><h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><pre><code>- official: https://www.djangoproject.com/\n- tut: https://docs.djangoproject.com/en/1.10/intro/tutorial01/\n- awesome: https://github.com/rosarior/awesome-django\n- community: http://django-china.cn/\n</code></pre><h1 id=\"install\"><a href=\"#install\" class=\"headerlink\" title=\"install\"></a>install</h1><pre><code>- [install on mac (bare metal)]()\n- [install on mac (docker)]()\n</code></pre><h1 id=\"read\"><a href=\"#read\" class=\"headerlink\" title=\"read\"></a>read</h1><pre><code>- djangobook: http://docs.30c.org/djangobook2/; http://djangobook.py3k.cn/2.0/\n</code></pre><h1 id=\"mooc\"><a href=\"#mooc\" class=\"headerlink\" title=\"mooc\"></a>mooc</h1><pre><code>- http://www.imooc.com/learn/790\n</code></pre>","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><pre><code>Full stack web frameworks.The most popular web framework in Python.\n\nDjango makes it easier to build better Web apps more quickly and with less code.\n\nDjango is a high-level Python Web framework that encourages rapid development and clean, pragmatic design. Built by experienced developers, it takes care of much of the hassle of Web development, so you can focus on writing your app without needing to reinvent the wheel. It’s free and open source.\n\nRidiculously fast: Django was designed to help developers take applications from concept to completion as quickly as possible.\n\nReassuringly secure: Django takes security seriously and helps developers avoid many common security mistakes.\n\nExceedingly scalable: Some of the busiest sites on the Web leverage Django’s ability to quickly and flexibly scale.\n</code></pre><h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><pre><code>- official: https://www.djangoproject.com/\n- tut: https://docs.djangoproject.com/en/1.10/intro/tutorial01/\n- awesome: https://github.com/rosarior/awesome-django\n- community: http://django-china.cn/\n</code></pre><h1 id=\"install\"><a href=\"#install\" class=\"headerlink\" title=\"install\"></a>install</h1><pre><code>- [install on mac (bare metal)]()\n- [install on mac (docker)]()\n</code></pre><h1 id=\"read\"><a href=\"#read\" class=\"headerlink\" title=\"read\"></a>read</h1><pre><code>- djangobook: http://docs.30c.org/djangobook2/; http://djangobook.py3k.cn/2.0/\n</code></pre><h1 id=\"mooc\"><a href=\"#mooc\" class=\"headerlink\" title=\"mooc\"></a>mooc</h1><pre><code>- http://www.imooc.com/learn/790\n</code></pre>"},{"title":"docker swarm mode","_content":"\n# link\n\n- tut: https://docs.docker.com/engine/swarm/swarm-tutorial/\n\n\n# about\n\nDocker Engine 1.12 includes swarm mode for natively managing a cluster of Docker Engines called a swarm. Use the Docker CLI to create a swarm, deploy application services to a swarm, and manage swarm behavior.\n\n# feature\n\n    - Cluster management integrated with Docker Engine: \n\n    Use the Docker Engine CLI to create a swarm of Docker Engines where you can deploy application services. You don’t need additional orchestration software to create or manage a swarm.\n\n    - Decentralized design: \n\n    Instead of handling differentiation between node roles at deployment time, the Docker Engine handles any specialization at runtime. You can deploy both kinds of nodes, managers and workers, using the Docker Engine. This means you can build an entire swarm from a single disk image.\n\n    - Declarative service model: \n\n    Docker Engine uses a declarative approach to let you define the desired state of the various services in your application stack. For example, you might describe an application comprised of a web front end service with message queueing services and a database backend.\n\n    - Scaling: \n\n    For each service, you can declare the number of tasks you want to run. When you scale up or down, the swarm manager automatically adapts by adding or removing tasks to maintain the desired state.\n\n    - Desired state reconciliation: \n\n    The swarm manager node constantly monitors the cluster state and reconciles any differences between the actual state your expressed desired state. For example, if you set up a service to run 10 replicas of a container, and a worker machine hosting two of those replicas crashes, the manager will create two new replicas to replace the replicas that crashed. The swarm manager assigns the new replicas to workers that are running and available.\n\n    - Multi-host networking: \n\n    You can specify an overlay network for your services. The swarm manager automatically assigns addresses to the containers on the overlay network when it initializes or updates the application.\n\n    - Service discovery: \n\n    Swarm manager nodes assign each service in the swarm a unique DNS name and load balances running containers. You can query every container running in the swarm through a DNS server embedded in the swarm.\n\n    - Load balancing: \n\n    You can expose the ports for services to an external load balancer. Internally, the swarm lets you specify how to distribute service containers between nodes.\n\n    - Secure by default: \n\n    Each node in the swarm enforces TLS mutual authentication and encryption to secure communications between itself and all other nodes. You have the option to use self-signed root certificates or certificates from a custom root CA.\n\n    - Rolling updates: \n\n    At rollout time you can apply service updates to nodes incrementally. The swarm manager lets you control the delay between service deployment to different sets of nodes. If anything goes wrong, you can roll-back a task to a previous version of the service.\n\n\n# key concepts\n\n- link: https://docs.docker.com/engine/swarm/key-concepts/ (Docker Engine 1.12.)\n\n- What is a swarm?\n\n    The cluster management and orchestration features embedded in the Docker Engine are built using SwarmKit. Docker engines participating in a cluster are running in swarm mode. You enable swarm mode for an engine by either initializing a swarm or joining an existing swarm.\n\n    A swarm is a cluster of Docker engines, or nodes, where you deploy services. The Docker Engine CLI and API include commands to manage swarm nodes (e.g., add or remove nodes), and deploy and orchestrate services across the swarm.\n\n    When you run Docker without using swarm mode, you execute container commands. When you run the Docker in swarm mode, you orchestrate services. You can run swarm services and standalone containers on the same Docker instances.\n\n- What is a node?\n\n    A node is an instance of the Docker engine participating in the swarm. You can also think of this as a Docker node. You can run one or more nodes on a single physical computer or cloud server, but production swarm deployments typically include Docker nodes distributed across multiple physical and cloud machines.\n\n    To deploy your application to a swarm, you submit a service definition to a manager node. The manager node dispatches units of work called tasks to worker nodes.\n\n    Manager nodes also perform the orchestration and cluster management functions required to maintain the desired state of the swarm. Manager nodes elect a single leader to conduct orchestration tasks.\n\n    Worker nodes receive and execute tasks dispatched from manager nodes. By default manager nodes also run services as worker nodes, but you can configure them to run manager tasks exclusively and be manager-only nodes. An agent runs on each worker node and reports on the tasks assigned to it. The worker node notifies the manager node of the current state of its assigned tasks so that the manager can maintain the desired state of each worker.\n\n- Services and tasks\n\n    A service is the definition of the tasks to execute on the worker nodes. It is the central structure of the swarm system and the primary root of user interaction with the swarm.\n\n    When you create a service, you specify which container image to use and which commands to execute inside running containers.\n\n    In the replicated services model, the swarm manager distributes a specific number of replica tasks among the nodes based upon the scale you set in the desired state.\n\n    For global services, the swarm runs one task for the service on every available node in the cluster.\n\n    A task carries a Docker container and the commands to run inside the container. It is the atomic scheduling unit of swarm. Manager nodes assign tasks to worker nodes according to the number of replicas set in the service scale. Once a task is assigned to a node, it cannot move to another node. It can only run on the assigned node or fail.\n\n- Load balancing\n\n    The swarm manager uses ingress load balancing to expose the services you want to make available externally to the swarm. The swarm manager can automatically assign the service a PublishedPort or you can configure a PublishedPort for the service. You can specify any unused port. If you do not specify a port, the swarm manager assigns the service a port in the 30000-32767 range.\n\n    External components, such as cloud load balancers, can access the service on the PublishedPort of any node in the cluster whether or not the node is currently running the task for the service. All nodes in the swarm route ingress connections to a running task instance.\n\n    Swarm mode has an internal DNS component that automatically assigns each service in the swarm a DNS entry. The swarm manager uses internal load balancing to distribute requests among services within the cluster based upon the DNS name of the service.\n\n# docker swarm mode command\n\n- swarm init\n\n- swarm join\n\n- service create\n\n- service inspect\n\n- service ls\n\n- service rm\n\n- service scale\n\n- service ps\n\n- service update","source":"_posts/docker-swarmnext-detail.md","raw":"---\ntitle: docker swarm mode\ncategories:\n- docker\ntags:\n- core\n- swarm\n---\n\n# link\n\n- tut: https://docs.docker.com/engine/swarm/swarm-tutorial/\n\n\n# about\n\nDocker Engine 1.12 includes swarm mode for natively managing a cluster of Docker Engines called a swarm. Use the Docker CLI to create a swarm, deploy application services to a swarm, and manage swarm behavior.\n\n# feature\n\n    - Cluster management integrated with Docker Engine: \n\n    Use the Docker Engine CLI to create a swarm of Docker Engines where you can deploy application services. You don’t need additional orchestration software to create or manage a swarm.\n\n    - Decentralized design: \n\n    Instead of handling differentiation between node roles at deployment time, the Docker Engine handles any specialization at runtime. You can deploy both kinds of nodes, managers and workers, using the Docker Engine. This means you can build an entire swarm from a single disk image.\n\n    - Declarative service model: \n\n    Docker Engine uses a declarative approach to let you define the desired state of the various services in your application stack. For example, you might describe an application comprised of a web front end service with message queueing services and a database backend.\n\n    - Scaling: \n\n    For each service, you can declare the number of tasks you want to run. When you scale up or down, the swarm manager automatically adapts by adding or removing tasks to maintain the desired state.\n\n    - Desired state reconciliation: \n\n    The swarm manager node constantly monitors the cluster state and reconciles any differences between the actual state your expressed desired state. For example, if you set up a service to run 10 replicas of a container, and a worker machine hosting two of those replicas crashes, the manager will create two new replicas to replace the replicas that crashed. The swarm manager assigns the new replicas to workers that are running and available.\n\n    - Multi-host networking: \n\n    You can specify an overlay network for your services. The swarm manager automatically assigns addresses to the containers on the overlay network when it initializes or updates the application.\n\n    - Service discovery: \n\n    Swarm manager nodes assign each service in the swarm a unique DNS name and load balances running containers. You can query every container running in the swarm through a DNS server embedded in the swarm.\n\n    - Load balancing: \n\n    You can expose the ports for services to an external load balancer. Internally, the swarm lets you specify how to distribute service containers between nodes.\n\n    - Secure by default: \n\n    Each node in the swarm enforces TLS mutual authentication and encryption to secure communications between itself and all other nodes. You have the option to use self-signed root certificates or certificates from a custom root CA.\n\n    - Rolling updates: \n\n    At rollout time you can apply service updates to nodes incrementally. The swarm manager lets you control the delay between service deployment to different sets of nodes. If anything goes wrong, you can roll-back a task to a previous version of the service.\n\n\n# key concepts\n\n- link: https://docs.docker.com/engine/swarm/key-concepts/ (Docker Engine 1.12.)\n\n- What is a swarm?\n\n    The cluster management and orchestration features embedded in the Docker Engine are built using SwarmKit. Docker engines participating in a cluster are running in swarm mode. You enable swarm mode for an engine by either initializing a swarm or joining an existing swarm.\n\n    A swarm is a cluster of Docker engines, or nodes, where you deploy services. The Docker Engine CLI and API include commands to manage swarm nodes (e.g., add or remove nodes), and deploy and orchestrate services across the swarm.\n\n    When you run Docker without using swarm mode, you execute container commands. When you run the Docker in swarm mode, you orchestrate services. You can run swarm services and standalone containers on the same Docker instances.\n\n- What is a node?\n\n    A node is an instance of the Docker engine participating in the swarm. You can also think of this as a Docker node. You can run one or more nodes on a single physical computer or cloud server, but production swarm deployments typically include Docker nodes distributed across multiple physical and cloud machines.\n\n    To deploy your application to a swarm, you submit a service definition to a manager node. The manager node dispatches units of work called tasks to worker nodes.\n\n    Manager nodes also perform the orchestration and cluster management functions required to maintain the desired state of the swarm. Manager nodes elect a single leader to conduct orchestration tasks.\n\n    Worker nodes receive and execute tasks dispatched from manager nodes. By default manager nodes also run services as worker nodes, but you can configure them to run manager tasks exclusively and be manager-only nodes. An agent runs on each worker node and reports on the tasks assigned to it. The worker node notifies the manager node of the current state of its assigned tasks so that the manager can maintain the desired state of each worker.\n\n- Services and tasks\n\n    A service is the definition of the tasks to execute on the worker nodes. It is the central structure of the swarm system and the primary root of user interaction with the swarm.\n\n    When you create a service, you specify which container image to use and which commands to execute inside running containers.\n\n    In the replicated services model, the swarm manager distributes a specific number of replica tasks among the nodes based upon the scale you set in the desired state.\n\n    For global services, the swarm runs one task for the service on every available node in the cluster.\n\n    A task carries a Docker container and the commands to run inside the container. It is the atomic scheduling unit of swarm. Manager nodes assign tasks to worker nodes according to the number of replicas set in the service scale. Once a task is assigned to a node, it cannot move to another node. It can only run on the assigned node or fail.\n\n- Load balancing\n\n    The swarm manager uses ingress load balancing to expose the services you want to make available externally to the swarm. The swarm manager can automatically assign the service a PublishedPort or you can configure a PublishedPort for the service. You can specify any unused port. If you do not specify a port, the swarm manager assigns the service a port in the 30000-32767 range.\n\n    External components, such as cloud load balancers, can access the service on the PublishedPort of any node in the cluster whether or not the node is currently running the task for the service. All nodes in the swarm route ingress connections to a running task instance.\n\n    Swarm mode has an internal DNS component that automatically assigns each service in the swarm a DNS entry. The swarm manager uses internal load balancing to distribute requests among services within the cluster based upon the DNS name of the service.\n\n# docker swarm mode command\n\n- swarm init\n\n- swarm join\n\n- service create\n\n- service inspect\n\n- service ls\n\n- service rm\n\n- service scale\n\n- service ps\n\n- service update","slug":"docker-swarmnext-detail","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-15T13:29:09.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rgg004ekxsv3ihvwk1l","content":"<h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><ul>\n<li>tut: <a href=\"https://docs.docker.com/engine/swarm/swarm-tutorial/\" target=\"_blank\" rel=\"external\">https://docs.docker.com/engine/swarm/swarm-tutorial/</a></li>\n</ul>\n<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><p>Docker Engine 1.12 includes swarm mode for natively managing a cluster of Docker Engines called a swarm. Use the Docker CLI to create a swarm, deploy application services to a swarm, and manage swarm behavior.</p>\n<h1 id=\"feature\"><a href=\"#feature\" class=\"headerlink\" title=\"feature\"></a>feature</h1><pre><code>- Cluster management integrated with Docker Engine: \n\nUse the Docker Engine CLI to create a swarm of Docker Engines where you can deploy application services. You don’t need additional orchestration software to create or manage a swarm.\n\n- Decentralized design: \n\nInstead of handling differentiation between node roles at deployment time, the Docker Engine handles any specialization at runtime. You can deploy both kinds of nodes, managers and workers, using the Docker Engine. This means you can build an entire swarm from a single disk image.\n\n- Declarative service model: \n\nDocker Engine uses a declarative approach to let you define the desired state of the various services in your application stack. For example, you might describe an application comprised of a web front end service with message queueing services and a database backend.\n\n- Scaling: \n\nFor each service, you can declare the number of tasks you want to run. When you scale up or down, the swarm manager automatically adapts by adding or removing tasks to maintain the desired state.\n\n- Desired state reconciliation: \n\nThe swarm manager node constantly monitors the cluster state and reconciles any differences between the actual state your expressed desired state. For example, if you set up a service to run 10 replicas of a container, and a worker machine hosting two of those replicas crashes, the manager will create two new replicas to replace the replicas that crashed. The swarm manager assigns the new replicas to workers that are running and available.\n\n- Multi-host networking: \n\nYou can specify an overlay network for your services. The swarm manager automatically assigns addresses to the containers on the overlay network when it initializes or updates the application.\n\n- Service discovery: \n\nSwarm manager nodes assign each service in the swarm a unique DNS name and load balances running containers. You can query every container running in the swarm through a DNS server embedded in the swarm.\n\n- Load balancing: \n\nYou can expose the ports for services to an external load balancer. Internally, the swarm lets you specify how to distribute service containers between nodes.\n\n- Secure by default: \n\nEach node in the swarm enforces TLS mutual authentication and encryption to secure communications between itself and all other nodes. You have the option to use self-signed root certificates or certificates from a custom root CA.\n\n- Rolling updates: \n\nAt rollout time you can apply service updates to nodes incrementally. The swarm manager lets you control the delay between service deployment to different sets of nodes. If anything goes wrong, you can roll-back a task to a previous version of the service.\n</code></pre><h1 id=\"key-concepts\"><a href=\"#key-concepts\" class=\"headerlink\" title=\"key concepts\"></a>key concepts</h1><ul>\n<li><p>link: <a href=\"https://docs.docker.com/engine/swarm/key-concepts/\" target=\"_blank\" rel=\"external\">https://docs.docker.com/engine/swarm/key-concepts/</a> (Docker Engine 1.12.)</p>\n</li>\n<li><p>What is a swarm?</p>\n<p>  The cluster management and orchestration features embedded in the Docker Engine are built using SwarmKit. Docker engines participating in a cluster are running in swarm mode. You enable swarm mode for an engine by either initializing a swarm or joining an existing swarm.</p>\n<p>  A swarm is a cluster of Docker engines, or nodes, where you deploy services. The Docker Engine CLI and API include commands to manage swarm nodes (e.g., add or remove nodes), and deploy and orchestrate services across the swarm.</p>\n<p>  When you run Docker without using swarm mode, you execute container commands. When you run the Docker in swarm mode, you orchestrate services. You can run swarm services and standalone containers on the same Docker instances.</p>\n</li>\n<li><p>What is a node?</p>\n<p>  A node is an instance of the Docker engine participating in the swarm. You can also think of this as a Docker node. You can run one or more nodes on a single physical computer or cloud server, but production swarm deployments typically include Docker nodes distributed across multiple physical and cloud machines.</p>\n<p>  To deploy your application to a swarm, you submit a service definition to a manager node. The manager node dispatches units of work called tasks to worker nodes.</p>\n<p>  Manager nodes also perform the orchestration and cluster management functions required to maintain the desired state of the swarm. Manager nodes elect a single leader to conduct orchestration tasks.</p>\n<p>  Worker nodes receive and execute tasks dispatched from manager nodes. By default manager nodes also run services as worker nodes, but you can configure them to run manager tasks exclusively and be manager-only nodes. An agent runs on each worker node and reports on the tasks assigned to it. The worker node notifies the manager node of the current state of its assigned tasks so that the manager can maintain the desired state of each worker.</p>\n</li>\n<li><p>Services and tasks</p>\n<p>  A service is the definition of the tasks to execute on the worker nodes. It is the central structure of the swarm system and the primary root of user interaction with the swarm.</p>\n<p>  When you create a service, you specify which container image to use and which commands to execute inside running containers.</p>\n<p>  In the replicated services model, the swarm manager distributes a specific number of replica tasks among the nodes based upon the scale you set in the desired state.</p>\n<p>  For global services, the swarm runs one task for the service on every available node in the cluster.</p>\n<p>  A task carries a Docker container and the commands to run inside the container. It is the atomic scheduling unit of swarm. Manager nodes assign tasks to worker nodes according to the number of replicas set in the service scale. Once a task is assigned to a node, it cannot move to another node. It can only run on the assigned node or fail.</p>\n</li>\n<li><p>Load balancing</p>\n<p>  The swarm manager uses ingress load balancing to expose the services you want to make available externally to the swarm. The swarm manager can automatically assign the service a PublishedPort or you can configure a PublishedPort for the service. You can specify any unused port. If you do not specify a port, the swarm manager assigns the service a port in the 30000-32767 range.</p>\n<p>  External components, such as cloud load balancers, can access the service on the PublishedPort of any node in the cluster whether or not the node is currently running the task for the service. All nodes in the swarm route ingress connections to a running task instance.</p>\n<p>  Swarm mode has an internal DNS component that automatically assigns each service in the swarm a DNS entry. The swarm manager uses internal load balancing to distribute requests among services within the cluster based upon the DNS name of the service.</p>\n</li>\n</ul>\n<h1 id=\"docker-swarm-mode-command\"><a href=\"#docker-swarm-mode-command\" class=\"headerlink\" title=\"docker swarm mode command\"></a>docker swarm mode command</h1><ul>\n<li><p>swarm init</p>\n</li>\n<li><p>swarm join</p>\n</li>\n<li><p>service create</p>\n</li>\n<li><p>service inspect</p>\n</li>\n<li><p>service ls</p>\n</li>\n<li><p>service rm</p>\n</li>\n<li><p>service scale</p>\n</li>\n<li><p>service ps</p>\n</li>\n<li><p>service update</p>\n</li>\n</ul>\n","excerpt":"","more":"<h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><ul>\n<li>tut: <a href=\"https://docs.docker.com/engine/swarm/swarm-tutorial/\">https://docs.docker.com/engine/swarm/swarm-tutorial/</a></li>\n</ul>\n<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><p>Docker Engine 1.12 includes swarm mode for natively managing a cluster of Docker Engines called a swarm. Use the Docker CLI to create a swarm, deploy application services to a swarm, and manage swarm behavior.</p>\n<h1 id=\"feature\"><a href=\"#feature\" class=\"headerlink\" title=\"feature\"></a>feature</h1><pre><code>- Cluster management integrated with Docker Engine: \n\nUse the Docker Engine CLI to create a swarm of Docker Engines where you can deploy application services. You don’t need additional orchestration software to create or manage a swarm.\n\n- Decentralized design: \n\nInstead of handling differentiation between node roles at deployment time, the Docker Engine handles any specialization at runtime. You can deploy both kinds of nodes, managers and workers, using the Docker Engine. This means you can build an entire swarm from a single disk image.\n\n- Declarative service model: \n\nDocker Engine uses a declarative approach to let you define the desired state of the various services in your application stack. For example, you might describe an application comprised of a web front end service with message queueing services and a database backend.\n\n- Scaling: \n\nFor each service, you can declare the number of tasks you want to run. When you scale up or down, the swarm manager automatically adapts by adding or removing tasks to maintain the desired state.\n\n- Desired state reconciliation: \n\nThe swarm manager node constantly monitors the cluster state and reconciles any differences between the actual state your expressed desired state. For example, if you set up a service to run 10 replicas of a container, and a worker machine hosting two of those replicas crashes, the manager will create two new replicas to replace the replicas that crashed. The swarm manager assigns the new replicas to workers that are running and available.\n\n- Multi-host networking: \n\nYou can specify an overlay network for your services. The swarm manager automatically assigns addresses to the containers on the overlay network when it initializes or updates the application.\n\n- Service discovery: \n\nSwarm manager nodes assign each service in the swarm a unique DNS name and load balances running containers. You can query every container running in the swarm through a DNS server embedded in the swarm.\n\n- Load balancing: \n\nYou can expose the ports for services to an external load balancer. Internally, the swarm lets you specify how to distribute service containers between nodes.\n\n- Secure by default: \n\nEach node in the swarm enforces TLS mutual authentication and encryption to secure communications between itself and all other nodes. You have the option to use self-signed root certificates or certificates from a custom root CA.\n\n- Rolling updates: \n\nAt rollout time you can apply service updates to nodes incrementally. The swarm manager lets you control the delay between service deployment to different sets of nodes. If anything goes wrong, you can roll-back a task to a previous version of the service.\n</code></pre><h1 id=\"key-concepts\"><a href=\"#key-concepts\" class=\"headerlink\" title=\"key concepts\"></a>key concepts</h1><ul>\n<li><p>link: <a href=\"https://docs.docker.com/engine/swarm/key-concepts/\">https://docs.docker.com/engine/swarm/key-concepts/</a> (Docker Engine 1.12.)</p>\n</li>\n<li><p>What is a swarm?</p>\n<p>  The cluster management and orchestration features embedded in the Docker Engine are built using SwarmKit. Docker engines participating in a cluster are running in swarm mode. You enable swarm mode for an engine by either initializing a swarm or joining an existing swarm.</p>\n<p>  A swarm is a cluster of Docker engines, or nodes, where you deploy services. The Docker Engine CLI and API include commands to manage swarm nodes (e.g., add or remove nodes), and deploy and orchestrate services across the swarm.</p>\n<p>  When you run Docker without using swarm mode, you execute container commands. When you run the Docker in swarm mode, you orchestrate services. You can run swarm services and standalone containers on the same Docker instances.</p>\n</li>\n<li><p>What is a node?</p>\n<p>  A node is an instance of the Docker engine participating in the swarm. You can also think of this as a Docker node. You can run one or more nodes on a single physical computer or cloud server, but production swarm deployments typically include Docker nodes distributed across multiple physical and cloud machines.</p>\n<p>  To deploy your application to a swarm, you submit a service definition to a manager node. The manager node dispatches units of work called tasks to worker nodes.</p>\n<p>  Manager nodes also perform the orchestration and cluster management functions required to maintain the desired state of the swarm. Manager nodes elect a single leader to conduct orchestration tasks.</p>\n<p>  Worker nodes receive and execute tasks dispatched from manager nodes. By default manager nodes also run services as worker nodes, but you can configure them to run manager tasks exclusively and be manager-only nodes. An agent runs on each worker node and reports on the tasks assigned to it. The worker node notifies the manager node of the current state of its assigned tasks so that the manager can maintain the desired state of each worker.</p>\n</li>\n<li><p>Services and tasks</p>\n<p>  A service is the definition of the tasks to execute on the worker nodes. It is the central structure of the swarm system and the primary root of user interaction with the swarm.</p>\n<p>  When you create a service, you specify which container image to use and which commands to execute inside running containers.</p>\n<p>  In the replicated services model, the swarm manager distributes a specific number of replica tasks among the nodes based upon the scale you set in the desired state.</p>\n<p>  For global services, the swarm runs one task for the service on every available node in the cluster.</p>\n<p>  A task carries a Docker container and the commands to run inside the container. It is the atomic scheduling unit of swarm. Manager nodes assign tasks to worker nodes according to the number of replicas set in the service scale. Once a task is assigned to a node, it cannot move to another node. It can only run on the assigned node or fail.</p>\n</li>\n<li><p>Load balancing</p>\n<p>  The swarm manager uses ingress load balancing to expose the services you want to make available externally to the swarm. The swarm manager can automatically assign the service a PublishedPort or you can configure a PublishedPort for the service. You can specify any unused port. If you do not specify a port, the swarm manager assigns the service a port in the 30000-32767 range.</p>\n<p>  External components, such as cloud load balancers, can access the service on the PublishedPort of any node in the cluster whether or not the node is currently running the task for the service. All nodes in the swarm route ingress connections to a running task instance.</p>\n<p>  Swarm mode has an internal DNS component that automatically assigns each service in the swarm a DNS entry. The swarm manager uses internal load balancing to distribute requests among services within the cluster based upon the DNS name of the service.</p>\n</li>\n</ul>\n<h1 id=\"docker-swarm-mode-command\"><a href=\"#docker-swarm-mode-command\" class=\"headerlink\" title=\"docker swarm mode command\"></a>docker swarm mode command</h1><ul>\n<li><p>swarm init</p>\n</li>\n<li><p>swarm join</p>\n</li>\n<li><p>service create</p>\n</li>\n<li><p>service inspect</p>\n</li>\n<li><p>service ls</p>\n</li>\n<li><p>service rm</p>\n</li>\n<li><p>service scale</p>\n</li>\n<li><p>service ps</p>\n</li>\n<li><p>service update</p>\n</li>\n</ul>\n"},{"title":"install django on mac","update":"2017-01-12T09:43:23.000Z","_content":"\n# install on mac (bare metal)\n\n# install on mac (docker)\n- download & install docker for mac;\n    - link: https://docs.docker.com/compose/django/;\n    - Define the project components;\n        1. create folder;\n        2. create Dockerfile;\n            ```\n             FROM python:2.7\n             ENV PYTHONUNBUFFERED 1\n             RUN mkdir /code\n             WORKDIR /code\n             ADD requirements.txt /code/\n             RUN pip install -r requirements.txt\n             ADD . /code/\n            ```\n        3. create requirements.txt;\n            ```\n             Django\n             psycopg2\n            ```\n        4. create docker-compose.yml\n            ```\n             version: '2'\n             services:\n               db:\n                 image: postgres\n               web:\n                 build: .\n                 command: python manage.py runserver 0.0.0.0:8000\n                 volumes:\n                   - .:/code\n                 ports:\n                   - \"8000:8000\"\n                 depends_on:\n                   - db\n            ```\n    - Create a Django project;\n        1. goto root dir;\n        2. docker-compose run web django-admin.py startproject composeexample .\n        3. ls -l; sudo chown -R $USER:$USER .;\n    - Connect the database;\n        1. edit composeexample/settings.py;\n        ```\n        DATABASES = {\n             'default': {\n                 'ENGINE': 'django.db.backends.postgresql',\n                 'NAME': 'postgres',\n                 'USER': 'postgres',\n                 'HOST': 'db',\n                 'PORT': 5432,\n             }\n         }\n        ```\n        2. $ docker-compose up","source":"_posts/lang-python-django-install.md","raw":"---\ntitle: install django on mac\nupdate: 2017-01-12 17:43:23\ncategories:\n- python\ntags: \n- install\n- python\n- django\n- docker\n---\n\n# install on mac (bare metal)\n\n# install on mac (docker)\n- download & install docker for mac;\n    - link: https://docs.docker.com/compose/django/;\n    - Define the project components;\n        1. create folder;\n        2. create Dockerfile;\n            ```\n             FROM python:2.7\n             ENV PYTHONUNBUFFERED 1\n             RUN mkdir /code\n             WORKDIR /code\n             ADD requirements.txt /code/\n             RUN pip install -r requirements.txt\n             ADD . /code/\n            ```\n        3. create requirements.txt;\n            ```\n             Django\n             psycopg2\n            ```\n        4. create docker-compose.yml\n            ```\n             version: '2'\n             services:\n               db:\n                 image: postgres\n               web:\n                 build: .\n                 command: python manage.py runserver 0.0.0.0:8000\n                 volumes:\n                   - .:/code\n                 ports:\n                   - \"8000:8000\"\n                 depends_on:\n                   - db\n            ```\n    - Create a Django project;\n        1. goto root dir;\n        2. docker-compose run web django-admin.py startproject composeexample .\n        3. ls -l; sudo chown -R $USER:$USER .;\n    - Connect the database;\n        1. edit composeexample/settings.py;\n        ```\n        DATABASES = {\n             'default': {\n                 'ENGINE': 'django.db.backends.postgresql',\n                 'NAME': 'postgres',\n                 'USER': 'postgres',\n                 'HOST': 'db',\n                 'PORT': 5432,\n             }\n         }\n        ```\n        2. $ docker-compose up","slug":"lang-python-django-install","published":1,"date":"2017-01-12T09:43:23.000Z","updated":"2017-01-12T09:49:02.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rif007okxsvd9beb2ji","content":"<h1 id=\"install-on-mac-bare-metal\"><a href=\"#install-on-mac-bare-metal\" class=\"headerlink\" title=\"install on mac (bare metal)\"></a>install on mac (bare metal)</h1><h1 id=\"install-on-mac-docker\"><a href=\"#install-on-mac-docker\" class=\"headerlink\" title=\"install on mac (docker)\"></a>install on mac (docker)</h1><ul>\n<li><p>download &amp; install docker for mac;</p>\n<ul>\n<li>link: <a href=\"https://docs.docker.com/compose/django/\" target=\"_blank\" rel=\"external\">https://docs.docker.com/compose/django/</a>;</li>\n<li><p>Define the project components;</p>\n<ol>\n<li>create folder;</li>\n<li><p>create Dockerfile;</p>\n <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">FROM python:2.7</div><div class=\"line\">ENV PYTHONUNBUFFERED 1</div><div class=\"line\">RUN mkdir /code</div><div class=\"line\">WORKDIR /code</div><div class=\"line\">ADD requirements.txt /code/</div><div class=\"line\">RUN pip install -r requirements.txt</div><div class=\"line\">ADD . /code/</div></pre></td></tr></table></figure>\n</li>\n<li><p>create requirements.txt;</p>\n <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">Django</div><div class=\"line\">psycopg2</div></pre></td></tr></table></figure>\n</li>\n<li><p>create docker-compose.yml</p>\n <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\">version: &apos;2&apos;</div><div class=\"line\">services:</div><div class=\"line\">  db:</div><div class=\"line\">    image: postgres</div><div class=\"line\">  web:</div><div class=\"line\">    build: .</div><div class=\"line\">    command: python manage.py runserver 0.0.0.0:8000</div><div class=\"line\">    volumes:</div><div class=\"line\">      - .:/code</div><div class=\"line\">    ports:</div><div class=\"line\">      - &quot;8000:8000&quot;</div><div class=\"line\">    depends_on:</div><div class=\"line\">      - db</div></pre></td></tr></table></figure>\n</li>\n</ol>\n</li>\n<li><p>Create a Django project;</p>\n<ol>\n<li>goto root dir;</li>\n<li>docker-compose run web django-admin.py startproject composeexample .</li>\n<li>ls -l; sudo chown -R $USER:$USER .;</li>\n</ol>\n</li>\n<li><p>Connect the database;</p>\n<ol>\n<li><p>edit composeexample/settings.py;</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">DATABASES = &#123;</div><div class=\"line\">     &apos;default&apos;: &#123;</div><div class=\"line\">         &apos;ENGINE&apos;: &apos;django.db.backends.postgresql&apos;,</div><div class=\"line\">         &apos;NAME&apos;: &apos;postgres&apos;,</div><div class=\"line\">         &apos;USER&apos;: &apos;postgres&apos;,</div><div class=\"line\">         &apos;HOST&apos;: &apos;db&apos;,</div><div class=\"line\">         &apos;PORT&apos;: 5432,</div><div class=\"line\">     &#125;</div><div class=\"line\"> &#125;</div></pre></td></tr></table></figure>\n</li>\n<li><p>$ docker-compose up</p>\n</li>\n</ol>\n</li>\n</ul>\n</li>\n</ul>\n","excerpt":"","more":"<h1 id=\"install-on-mac-bare-metal\"><a href=\"#install-on-mac-bare-metal\" class=\"headerlink\" title=\"install on mac (bare metal)\"></a>install on mac (bare metal)</h1><h1 id=\"install-on-mac-docker\"><a href=\"#install-on-mac-docker\" class=\"headerlink\" title=\"install on mac (docker)\"></a>install on mac (docker)</h1><ul>\n<li><p>download &amp; install docker for mac;</p>\n<ul>\n<li>link: <a href=\"https://docs.docker.com/compose/django/\">https://docs.docker.com/compose/django/</a>;</li>\n<li><p>Define the project components;</p>\n<ol>\n<li>create folder;</li>\n<li><p>create Dockerfile;</p>\n <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">FROM python:2.7</div><div class=\"line\">ENV PYTHONUNBUFFERED 1</div><div class=\"line\">RUN mkdir /code</div><div class=\"line\">WORKDIR /code</div><div class=\"line\">ADD requirements.txt /code/</div><div class=\"line\">RUN pip install -r requirements.txt</div><div class=\"line\">ADD . /code/</div></pre></td></tr></table></figure>\n</li>\n<li><p>create requirements.txt;</p>\n <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">Django</div><div class=\"line\">psycopg2</div></pre></td></tr></table></figure>\n</li>\n<li><p>create docker-compose.yml</p>\n <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\">version: &apos;2&apos;</div><div class=\"line\">services:</div><div class=\"line\">  db:</div><div class=\"line\">    image: postgres</div><div class=\"line\">  web:</div><div class=\"line\">    build: .</div><div class=\"line\">    command: python manage.py runserver 0.0.0.0:8000</div><div class=\"line\">    volumes:</div><div class=\"line\">      - .:/code</div><div class=\"line\">    ports:</div><div class=\"line\">      - &quot;8000:8000&quot;</div><div class=\"line\">    depends_on:</div><div class=\"line\">      - db</div></pre></td></tr></table></figure>\n</li>\n</ol>\n</li>\n<li><p>Create a Django project;</p>\n<ol>\n<li>goto root dir;</li>\n<li>docker-compose run web django-admin.py startproject composeexample .</li>\n<li>ls -l; sudo chown -R $USER:$USER .;</li>\n</ol>\n</li>\n<li><p>Connect the database;</p>\n<ol>\n<li><p>edit composeexample/settings.py;</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">DATABASES = &#123;</div><div class=\"line\">     &apos;default&apos;: &#123;</div><div class=\"line\">         &apos;ENGINE&apos;: &apos;django.db.backends.postgresql&apos;,</div><div class=\"line\">         &apos;NAME&apos;: &apos;postgres&apos;,</div><div class=\"line\">         &apos;USER&apos;: &apos;postgres&apos;,</div><div class=\"line\">         &apos;HOST&apos;: &apos;db&apos;,</div><div class=\"line\">         &apos;PORT&apos;: 5432,</div><div class=\"line\">     &#125;</div><div class=\"line\"> &#125;</div></pre></td></tr></table></figure>\n</li>\n<li><p>$ docker-compose up</p>\n</li>\n</ol>\n</li>\n</ul>\n</li>\n</ul>\n"},{"title":"python portal","_content":"\n# about\n\n# link\n\n    - official: https://www.python.org/\n    - pep: http://zh-google-styleguide.readthedocs.org/en/latest/google-python-styleguide/python_style_rules/（至少过一遍，否则视野会局限，可以不看Python书但需要熟读官方手册）\n    - awesome\n    https://github.com/vinta/awesome-python\n    https://github.com/Junnplus/awesome-python-books\n# doc\n    https://learnpythonthehardway.org/book/\n    http://www.learnpython.org/\n# tut\n    http://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000\n\n# faq\n    https://www.oschina.net/question/tag/python","source":"_posts/lang-python-portal.md","raw":"---\ntitle: python portal\ncategories:\n- python\ntags:\n- portal\n---\n\n# about\n\n# link\n\n    - official: https://www.python.org/\n    - pep: http://zh-google-styleguide.readthedocs.org/en/latest/google-python-styleguide/python_style_rules/（至少过一遍，否则视野会局限，可以不看Python书但需要熟读官方手册）\n    - awesome\n    https://github.com/vinta/awesome-python\n    https://github.com/Junnplus/awesome-python-books\n# doc\n    https://learnpythonthehardway.org/book/\n    http://www.learnpython.org/\n# tut\n    http://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000\n\n# faq\n    https://www.oschina.net/question/tag/python","slug":"lang-python-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-02-27T03:11:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rij007qkxsvoeo04lj3","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><pre><code>- official: https://www.python.org/\n- pep: http://zh-google-styleguide.readthedocs.org/en/latest/google-python-styleguide/python_style_rules/（至少过一遍，否则视野会局限，可以不看Python书但需要熟读官方手册）\n- awesome\nhttps://github.com/vinta/awesome-python\nhttps://github.com/Junnplus/awesome-python-books\n</code></pre><h1 id=\"doc\"><a href=\"#doc\" class=\"headerlink\" title=\"doc\"></a>doc</h1><pre><code>https://learnpythonthehardway.org/book/\nhttp://www.learnpython.org/\n</code></pre><h1 id=\"tut\"><a href=\"#tut\" class=\"headerlink\" title=\"tut\"></a>tut</h1><pre><code>http://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000\n</code></pre><h1 id=\"faq\"><a href=\"#faq\" class=\"headerlink\" title=\"faq\"></a>faq</h1><pre><code>https://www.oschina.net/question/tag/python\n</code></pre>","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><pre><code>- official: https://www.python.org/\n- pep: http://zh-google-styleguide.readthedocs.org/en/latest/google-python-styleguide/python_style_rules/（至少过一遍，否则视野会局限，可以不看Python书但需要熟读官方手册）\n- awesome\nhttps://github.com/vinta/awesome-python\nhttps://github.com/Junnplus/awesome-python-books\n</code></pre><h1 id=\"doc\"><a href=\"#doc\" class=\"headerlink\" title=\"doc\"></a>doc</h1><pre><code>https://learnpythonthehardway.org/book/\nhttp://www.learnpython.org/\n</code></pre><h1 id=\"tut\"><a href=\"#tut\" class=\"headerlink\" title=\"tut\"></a>tut</h1><pre><code>http://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000\n</code></pre><h1 id=\"faq\"><a href=\"#faq\" class=\"headerlink\" title=\"faq\"></a>faq</h1><pre><code>https://www.oschina.net/question/tag/python\n</code></pre>"},{"title":"ruby on rails core","_content":"\n# about\n\n# link\n\nhttps://ruby-china.org/","source":"_posts/lang-ruby-rails-core.md","raw":"---\ntitle: ruby on rails core\ncategories:\n- ruby\ntags:\n- core\n- rails\n---\n\n# about\n\n# link\n\nhttps://ruby-china.org/","slug":"lang-ruby-rails-core","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-02-26T16:39:29.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rio007skxsvn761rdtt","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><p><a href=\"https://ruby-china.org/\" target=\"_blank\" rel=\"external\">https://ruby-china.org/</a></p>\n","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><p><a href=\"https://ruby-china.org/\">https://ruby-china.org/</a></p>\n"},{"title":"microservice portal","_content":"\n# about\n\n- current\n\n    - 多层架构\n\n            表现层，业务逻辑层，数据层\n            分层设计与优化，合理设计接口，每层可再细分，功能模块可重用\n    \n    - 单体模式\n\n            monolith，是目前主流打包方式\n            一个单独的java war文件，rails或node中一个单独的目录\n            优势：业界熟练使用，生态健全，外围工具丰富。易于开发，测试，部署\n            开始项目最简单快捷的方式，充分利用已有代码和工具，不必担心分布式部署。\n            但是应用工程变得负责，敏捷和部署举步维艰，启动时间长，难以采用新技术。可靠性差。\n\n- 微服务架构\n\n    - 优势\n\n            由多个独立运行的微小服务构成\n            使用轻量级通讯机制\n                独立构建部署\n            每个服务保持独立性\n                构建，部署，扩容，容错，数据管理\n            敏捷最大化\n                代码运行速度更高，更短的反馈周期，更简单的使用方法，快速应对变化\n            可以使用不同技术\n                每个服务可以使用独立技术栈\n                易于重构，分散式管理\n            高效团队\n                小规模团队\n                责任明晰，便捷清晰\n                围绕业务功能进行组织，非常灵活\n\n    - 不足\n\n            过度关注服务大小，可能过度拆分\n            分布式系统的构建与部署问题\n            分布式的数据架构\n            测试的复杂度\n            改动带来的沟通成本\n\n# link\n\n    main: http://microservices.io/patterns/microservices.html\n    http://microservices.io/\n\n    conf:\n        http://2017.qconbeijing.com/track/58\n\n# project\n\n    iron.io: Microservices For The Enterprise\n    http://www.iron.io/ \n\n# clusterup\n\n    about\n        life cycle management GUI for docker microservices\n        Real-time monitoring of Docker containers and applications\n        Manage and monitor your app pre-production. We provide app analytics\n    link\n        https://clusterup.io/\n        ","source":"_posts/microservice-portal.md","raw":"---\ntitle: microservice portal\ncategories:\n- microservice\ntags:\n- portal\n---\n\n# about\n\n- current\n\n    - 多层架构\n\n            表现层，业务逻辑层，数据层\n            分层设计与优化，合理设计接口，每层可再细分，功能模块可重用\n    \n    - 单体模式\n\n            monolith，是目前主流打包方式\n            一个单独的java war文件，rails或node中一个单独的目录\n            优势：业界熟练使用，生态健全，外围工具丰富。易于开发，测试，部署\n            开始项目最简单快捷的方式，充分利用已有代码和工具，不必担心分布式部署。\n            但是应用工程变得负责，敏捷和部署举步维艰，启动时间长，难以采用新技术。可靠性差。\n\n- 微服务架构\n\n    - 优势\n\n            由多个独立运行的微小服务构成\n            使用轻量级通讯机制\n                独立构建部署\n            每个服务保持独立性\n                构建，部署，扩容，容错，数据管理\n            敏捷最大化\n                代码运行速度更高，更短的反馈周期，更简单的使用方法，快速应对变化\n            可以使用不同技术\n                每个服务可以使用独立技术栈\n                易于重构，分散式管理\n            高效团队\n                小规模团队\n                责任明晰，便捷清晰\n                围绕业务功能进行组织，非常灵活\n\n    - 不足\n\n            过度关注服务大小，可能过度拆分\n            分布式系统的构建与部署问题\n            分布式的数据架构\n            测试的复杂度\n            改动带来的沟通成本\n\n# link\n\n    main: http://microservices.io/patterns/microservices.html\n    http://microservices.io/\n\n    conf:\n        http://2017.qconbeijing.com/track/58\n\n# project\n\n    iron.io: Microservices For The Enterprise\n    http://www.iron.io/ \n\n# clusterup\n\n    about\n        life cycle management GUI for docker microservices\n        Real-time monitoring of Docker containers and applications\n        Manage and monitor your app pre-production. We provide app analytics\n    link\n        https://clusterup.io/\n        ","slug":"microservice-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-02-04T08:22:33.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rit007vkxsvju2wbaue","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><ul>\n<li><p>current</p>\n<ul>\n<li><p>多层架构</p>\n<pre><code>表现层，业务逻辑层，数据层\n分层设计与优化，合理设计接口，每层可再细分，功能模块可重用\n</code></pre></li>\n<li><p>单体模式</p>\n<pre><code>monolith，是目前主流打包方式\n一个单独的java war文件，rails或node中一个单独的目录\n优势：业界熟练使用，生态健全，外围工具丰富。易于开发，测试，部署\n开始项目最简单快捷的方式，充分利用已有代码和工具，不必担心分布式部署。\n但是应用工程变得负责，敏捷和部署举步维艰，启动时间长，难以采用新技术。可靠性差。\n</code></pre></li>\n</ul>\n</li>\n<li><p>微服务架构</p>\n<ul>\n<li><p>优势</p>\n<pre><code>由多个独立运行的微小服务构成\n使用轻量级通讯机制\n    独立构建部署\n每个服务保持独立性\n    构建，部署，扩容，容错，数据管理\n敏捷最大化\n    代码运行速度更高，更短的反馈周期，更简单的使用方法，快速应对变化\n可以使用不同技术\n    每个服务可以使用独立技术栈\n    易于重构，分散式管理\n高效团队\n    小规模团队\n    责任明晰，便捷清晰\n    围绕业务功能进行组织，非常灵活\n</code></pre></li>\n<li><p>不足</p>\n<pre><code>过度关注服务大小，可能过度拆分\n分布式系统的构建与部署问题\n分布式的数据架构\n测试的复杂度\n改动带来的沟通成本\n</code></pre></li>\n</ul>\n</li>\n</ul>\n<h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><pre><code>main: http://microservices.io/patterns/microservices.html\nhttp://microservices.io/\n\nconf:\n    http://2017.qconbeijing.com/track/58\n</code></pre><h1 id=\"project\"><a href=\"#project\" class=\"headerlink\" title=\"project\"></a>project</h1><pre><code>iron.io: Microservices For The Enterprise\nhttp://www.iron.io/ \n</code></pre><h1 id=\"clusterup\"><a href=\"#clusterup\" class=\"headerlink\" title=\"clusterup\"></a>clusterup</h1><pre><code>about\n    life cycle management GUI for docker microservices\n    Real-time monitoring of Docker containers and applications\n    Manage and monitor your app pre-production. We provide app analytics\nlink\n    https://clusterup.io/\n</code></pre>","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><ul>\n<li><p>current</p>\n<ul>\n<li><p>多层架构</p>\n<pre><code>表现层，业务逻辑层，数据层\n分层设计与优化，合理设计接口，每层可再细分，功能模块可重用\n</code></pre></li>\n<li><p>单体模式</p>\n<pre><code>monolith，是目前主流打包方式\n一个单独的java war文件，rails或node中一个单独的目录\n优势：业界熟练使用，生态健全，外围工具丰富。易于开发，测试，部署\n开始项目最简单快捷的方式，充分利用已有代码和工具，不必担心分布式部署。\n但是应用工程变得负责，敏捷和部署举步维艰，启动时间长，难以采用新技术。可靠性差。\n</code></pre></li>\n</ul>\n</li>\n<li><p>微服务架构</p>\n<ul>\n<li><p>优势</p>\n<pre><code>由多个独立运行的微小服务构成\n使用轻量级通讯机制\n    独立构建部署\n每个服务保持独立性\n    构建，部署，扩容，容错，数据管理\n敏捷最大化\n    代码运行速度更高，更短的反馈周期，更简单的使用方法，快速应对变化\n可以使用不同技术\n    每个服务可以使用独立技术栈\n    易于重构，分散式管理\n高效团队\n    小规模团队\n    责任明晰，便捷清晰\n    围绕业务功能进行组织，非常灵活\n</code></pre></li>\n<li><p>不足</p>\n<pre><code>过度关注服务大小，可能过度拆分\n分布式系统的构建与部署问题\n分布式的数据架构\n测试的复杂度\n改动带来的沟通成本\n</code></pre></li>\n</ul>\n</li>\n</ul>\n<h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><pre><code>main: http://microservices.io/patterns/microservices.html\nhttp://microservices.io/\n\nconf:\n    http://2017.qconbeijing.com/track/58\n</code></pre><h1 id=\"project\"><a href=\"#project\" class=\"headerlink\" title=\"project\"></a>project</h1><pre><code>iron.io: Microservices For The Enterprise\nhttp://www.iron.io/ \n</code></pre><h1 id=\"clusterup\"><a href=\"#clusterup\" class=\"headerlink\" title=\"clusterup\"></a>clusterup</h1><pre><code>about\n    life cycle management GUI for docker microservices\n    Real-time monitoring of Docker containers and applications\n    Manage and monitor your app pre-production. We provide app analytics\nlink\n    https://clusterup.io/\n</code></pre>"},{"title":"http core","_content":"\n#  core","source":"_posts/network-http-core.md","raw":"---\ntitle: http core\ncategories:\n- network\ntags:\n- core\n- http\n---\n\n#  core","slug":"network-http-core","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:34:31.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4riw007ykxsvk7mgt06h","content":"<h1 id=\"core\"><a href=\"#core\" class=\"headerlink\" title=\"core\"></a>core</h1>","excerpt":"","more":"<h1 id=\"core\"><a href=\"#core\" class=\"headerlink\" title=\"core\"></a>core</h1>"},{"title":"network portal","_content":"\n# about\n\n\n# websocket\n","source":"_posts/network-portal.md","raw":"---\ntitle: network portal\ncategories:\n- network\ntags:\n- portal\n---\n\n# about\n\n\n# websocket\n","slug":"network-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-15T13:29:19.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4riz0081kxsv2ehn892i","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><h1 id=\"websocket\"><a href=\"#websocket\" class=\"headerlink\" title=\"websocket\"></a>websocket</h1>","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><h1 id=\"websocket\"><a href=\"#websocket\" class=\"headerlink\" title=\"websocket\"></a>websocket</h1>"},{"title":"linux portal","_content":"\n#  ","source":"_posts/system-linux-portal.md","raw":"---\ntitle: linux portal\ncategories:\n- linux\ntags:\n- portal\n---\n\n#  ","slug":"system-linux-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:34:06.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj1nl4rj10082kxsvt6b2qwbm","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\" \"></a> </h1>","excerpt":"","more":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\" \"></a> </h1>"}],"PostAsset":[],"PostCategory":[{"post_id":"cj1nl4ra90000kxsv6h9n3k9y","category_id":"cj1nl4rak0002kxsvcygcd1t7","_id":"cj1nl4rbb000akxsv7tzh2cmc"},{"post_id":"cj1nl4rag0001kxsvg9dkqgif","category_id":"cj1nl4rak0002kxsvcygcd1t7","_id":"cj1nl4rbl000gkxsvu3jrb2rh"},{"post_id":"cj1nl4rar0004kxsvv2nhnujf","category_id":"cj1nl4rak0002kxsvcygcd1t7","_id":"cj1nl4rbs000nkxsvw7hzzygd"},{"post_id":"cj1nl4rau0005kxsv4e6kk5fw","category_id":"cj1nl4rbo000ikxsvjbv2z6d3","_id":"cj1nl4rc1000vkxsvtfi6tkaj"},{"post_id":"cj1nl4rb20006kxsvdyglatf6","category_id":"cj1nl4rbv000pkxsvohgya92l","_id":"cj1nl4rcd0012kxsvwvw497t1"},{"post_id":"cj1nl4rb80009kxsv7rqkofmc","category_id":"cj1nl4rc2000wkxsvovoq1x80","_id":"cj1nl4rcl0019kxsvh3pgv9nw"},{"post_id":"cj1nl4rbc000bkxsva4a6xg82","category_id":"cj1nl4rcd0014kxsv0yj8i0j5","_id":"cj1nl4rcr001hkxsvslnfnqvu"},{"post_id":"cj1nl4rbi000ekxsvfqpayfu4","category_id":"cj1nl4rbv000pkxsvohgya92l","_id":"cj1nl4rcz001okxsvk2vjuk0e"},{"post_id":"cj1nl4rbm000hkxsvo27rppn1","category_id":"cj1nl4rbv000pkxsvohgya92l","_id":"cj1nl4rd9001vkxsvclc2roer"},{"post_id":"cj1nl4rbp000lkxsvy9z61t50","category_id":"cj1nl4rd4001qkxsv82besi28","_id":"cj1nl4rdh0023kxsvo7iri7yc"},{"post_id":"cj1nl4rbs000okxsvifi7yrjs","category_id":"cj1nl4rdb001xkxsv795sdv5l","_id":"cj1nl4rdo002akxsvdh58dml0"},{"post_id":"cj1nl4rbx000skxsv1p7dq3zk","category_id":"cj1nl4rdh0024kxsvs4zdognq","_id":"cj1nl4rdx002hkxsva1c52qph"},{"post_id":"cj1nl4rbz000ukxsvyvdar8pr","category_id":"cj1nl4rcd0014kxsv0yj8i0j5","_id":"cj1nl4re9002okxsvd1ys5526"},{"post_id":"cj1nl4rc4000zkxsvpuw1mnfs","category_id":"cj1nl4rdy002jkxsv2hvj87nq","_id":"cj1nl4rek002vkxsvvyehiduj"},{"post_id":"cj1nl4rcb0011kxsvfqrdlehk","category_id":"cj1nl4ree002rkxsvcp05s859","_id":"cj1nl4reu0033kxsvy3ess8wq"},{"post_id":"cj1nl4rce0016kxsv4dkav496","category_id":"cj1nl4rem002ykxsvh7q0sd8h","_id":"cj1nl4rf2003akxsv6y99m7a1"},{"post_id":"cj1nl4req0032kxsv9mfll6v4","category_id":"cj1nl4rcd0014kxsv0yj8i0j5","_id":"cj1nl4rf4003ekxsvklc8pe5f"},{"post_id":"cj1nl4rci0018kxsvdfkwhe9j","category_id":"cj1nl4rev0034kxsv9utwmdoq","_id":"cj1nl4rf8003hkxsvmbgouzbf"},{"post_id":"cj1nl4rcl001bkxsvrmvblr5e","category_id":"cj1nl4rf2003bkxsvey1g8zph","_id":"cj1nl4rff003nkxsvrz0gjo3o"},{"post_id":"cj1nl4rcp001fkxsvk0zd9el4","category_id":"cj1nl4rf2003bkxsvey1g8zph","_id":"cj1nl4rfq003tkxsv3kqk3fyg"},{"post_id":"cj1nl4rcs001ikxsv7fw0whgr","category_id":"cj1nl4rf2003bkxsvey1g8zph","_id":"cj1nl4rfx0040kxsvnfr3bdf6"},{"post_id":"cj1nl4rcv001mkxsv8ov7t3bo","category_id":"cj1nl4rf2003bkxsvey1g8zph","_id":"cj1nl4rg80046kxsvhqqh7w8b"},{"post_id":"cj1nl4rd0001pkxsvh8wpboqk","category_id":"cj1nl4rf2003bkxsvey1g8zph","_id":"cj1nl4rgf004dkxsvff4byd4m"},{"post_id":"cj1nl4rg80047kxsv3ojfvy9b","category_id":"cj1nl4rf2003bkxsvey1g8zph","_id":"cj1nl4rgl004ikxsvsf7s1f6v"},{"post_id":"cj1nl4rgc004bkxsvx990biiu","category_id":"cj1nl4rdh0024kxsvs4zdognq","_id":"cj1nl4rgn004kkxsv45jvk766"},{"post_id":"cj1nl4rd6001tkxsv2lkkphnm","category_id":"cj1nl4rf2003bkxsvey1g8zph","_id":"cj1nl4rgq004nkxsvqknu2693"},{"post_id":"cj1nl4rgg004ekxsv3ihvwk1l","category_id":"cj1nl4rf2003bkxsvey1g8zph","_id":"cj1nl4rgr004pkxsv8m1pdn0i"},{"post_id":"cj1nl4rd9001wkxsv37rjqnk0","category_id":"cj1nl4rf2003bkxsvey1g8zph","_id":"cj1nl4rgt004rkxsvgo74vvzi"},{"post_id":"cj1nl4rdc0020kxsva8eeadnz","category_id":"cj1nl4rf2003bkxsvey1g8zph","_id":"cj1nl4rgu004ukxsvfwn3otiu"},{"post_id":"cj1nl4rdf0022kxsvx69sj1cg","category_id":"cj1nl4rf2003bkxsvey1g8zph","_id":"cj1nl4rgw004ykxsvlb0hwark"},{"post_id":"cj1nl4rdj0027kxsvav51iztu","category_id":"cj1nl4rf2003bkxsvey1g8zph","_id":"cj1nl4rgy0053kxsv0itcu6c6"},{"post_id":"cj1nl4rdm0029kxsv979zukli","category_id":"cj1nl4rf2003bkxsvey1g8zph","_id":"cj1nl4rgz0057kxsve5jy56ha"},{"post_id":"cj1nl4rdq002dkxsvxnhy6rnm","category_id":"cj1nl4rf2003bkxsvey1g8zph","_id":"cj1nl4rh1005bkxsvca6ple41"},{"post_id":"cj1nl4rdw002gkxsvobjrkkl9","category_id":"cj1nl4rf2003bkxsvey1g8zph","_id":"cj1nl4rh2005gkxsv8u0ual5a"},{"post_id":"cj1nl4rdy002kkxsv03m04ti0","category_id":"cj1nl4rf2003bkxsvey1g8zph","_id":"cj1nl4rh5005lkxsvhg5jtiom"},{"post_id":"cj1nl4re2002nkxsvy32e1jsl","category_id":"cj1nl4rf2003bkxsvey1g8zph","_id":"cj1nl4rh7005okxsvelyb1x9b"},{"post_id":"cj1nl4reb002qkxsvrcfagwk5","category_id":"cj1nl4rf2003bkxsvey1g8zph","_id":"cj1nl4rh8005tkxsvl4kq441s"},{"post_id":"cj1nl4reh002tkxsvn2nx8m5o","category_id":"cj1nl4rf2003bkxsvey1g8zph","_id":"cj1nl4rh9005wkxsvqal5ru0y"},{"post_id":"cj1nl4rek002wkxsvtpp5poiw","category_id":"cj1nl4rf2003bkxsvey1g8zph","_id":"cj1nl4rha0060kxsvljwddtcb"},{"post_id":"cj1nl4reo0030kxsv60te7vm8","category_id":"cj1nl4rf2003bkxsvey1g8zph","_id":"cj1nl4rhb0065kxsvn1q96dtl"},{"post_id":"cj1nl4rew0037kxsv4mj8yqp5","category_id":"cj1nl4rf2003bkxsvey1g8zph","_id":"cj1nl4rhc006akxsv1cxwo5un"},{"post_id":"cj1nl4rf00039kxsvn0a835sy","category_id":"cj1nl4rf2003bkxsvey1g8zph","_id":"cj1nl4rhf006fkxsv6ij7qx5b"},{"post_id":"cj1nl4rf3003dkxsv48kfkoaq","category_id":"cj1nl4rf2003bkxsvey1g8zph","_id":"cj1nl4rhh006kkxsvkn3cw20l"},{"post_id":"cj1nl4rf6003gkxsvyjy2mrjq","category_id":"cj1nl4rhf006gkxsvnopw7f8u","_id":"cj1nl4rhj006pkxsv2usy9263"},{"post_id":"cj1nl4rf9003kkxsvtlouqbco","category_id":"cj1nl4rhi006mkxsveyxcmf6u","_id":"cj1nl4rhl006vkxsv80ihoq5v"},{"post_id":"cj1nl4rfb003mkxsvedf5bx9f","category_id":"cj1nl4rhi006mkxsveyxcmf6u","_id":"cj1nl4rhq0070kxsv05lw2i47"},{"post_id":"cj1nl4rfg003okxsvahdj6s25","category_id":"cj1nl4rhi006mkxsveyxcmf6u","_id":"cj1nl4rht0075kxsvd30zt28i"},{"post_id":"cj1nl4rfn003skxsvsn2vbv97","category_id":"cj1nl4rhq0071kxsvpd8lvhb5","_id":"cj1nl4rhx007akxsv9vkntu1l"},{"post_id":"cj1nl4rfq003vkxsvfu6sj2e1","category_id":"cj1nl4rhi006mkxsveyxcmf6u","_id":"cj1nl4rhy007fkxsvxyscdztf"},{"post_id":"cj1nl4rfu003zkxsvqf61sj9w","category_id":"cj1nl4rhx007bkxsv69buwdje","_id":"cj1nl4rhz007jkxsv08u7pnx8"},{"post_id":"cj1nl4rfy0041kxsvwkurnxev","category_id":"cj1nl4rhi006mkxsveyxcmf6u","_id":"cj1nl4ri0007mkxsvtqy30arz"},{"post_id":"cj1nl4rg40044kxsvngx5ivkb","category_id":"cj1nl4rhz007kkxsvf43mqixo","_id":"cj1nl4ri0007nkxsvw1zte7au"},{"post_id":"cj1nl4rif007okxsvd9beb2ji","category_id":"cj1nl4rdh0024kxsvs4zdognq","_id":"cj1nl4rit007ukxsvyv4sjtkq"},{"post_id":"cj1nl4rij007qkxsvoeo04lj3","category_id":"cj1nl4rdh0024kxsvs4zdognq","_id":"cj1nl4riw007xkxsvfr1lophj"},{"post_id":"cj1nl4rio007skxsvn761rdtt","category_id":"cj1nl4riv007wkxsv41mm1gag","_id":"cj1nl4rj40087kxsv5dqc5k2a"},{"post_id":"cj1nl4rit007vkxsvju2wbaue","category_id":"cj1nl4rj20083kxsvjulair8n","_id":"cj1nl4rj5008ckxsv77q29par"},{"post_id":"cj1nl4riw007ykxsvk7mgt06h","category_id":"cj1nl4rj40088kxsv442ou335","_id":"cj1nl4rj6008gkxsvmvu5bhkj"},{"post_id":"cj1nl4riz0081kxsv2ehn892i","category_id":"cj1nl4rj40088kxsv442ou335","_id":"cj1nl4rj6008kkxsvhdz3m1ww"},{"post_id":"cj1nl4rj10082kxsvt6b2qwbm","category_id":"cj1nl4rj6008hkxsvoaxl1ke2","_id":"cj1nl4rj7008nkxsvxtts8yep"}],"PostTag":[{"post_id":"cj1nl4ra90000kxsv6h9n3k9y","tag_id":"cj1nl4raq0003kxsv11n6n8gs","_id":"cj1nl4rbk000fkxsvikimk3ra"},{"post_id":"cj1nl4ra90000kxsv6h9n3k9y","tag_id":"cj1nl4rb50008kxsva7eoldku","_id":"cj1nl4rbo000jkxsvqyk0ero0"},{"post_id":"cj1nl4rbi000ekxsvfqpayfu4","tag_id":"cj1nl4raq0003kxsv11n6n8gs","_id":"cj1nl4rbq000mkxsv7vm0g9o7"},{"post_id":"cj1nl4rbp000lkxsvy9z61t50","tag_id":"cj1nl4raq0003kxsv11n6n8gs","_id":"cj1nl4rbw000rkxsvk6pbvl2u"},{"post_id":"cj1nl4rag0001kxsvg9dkqgif","tag_id":"cj1nl4raq0003kxsv11n6n8gs","_id":"cj1nl4rbz000tkxsv3lx3fphj"},{"post_id":"cj1nl4rag0001kxsvg9dkqgif","tag_id":"cj1nl4rbp000kkxsvp5n2bnh5","_id":"cj1nl4rc4000ykxsv4nq0cgyc"},{"post_id":"cj1nl4rbx000skxsv1p7dq3zk","tag_id":"cj1nl4raq0003kxsv11n6n8gs","_id":"cj1nl4rca0010kxsvlrfr60qm"},{"post_id":"cj1nl4rbz000ukxsvyvdar8pr","tag_id":"cj1nl4raq0003kxsv11n6n8gs","_id":"cj1nl4rcd0013kxsv5aj9jctn"},{"post_id":"cj1nl4rc4000zkxsvpuw1mnfs","tag_id":"cj1nl4raq0003kxsv11n6n8gs","_id":"cj1nl4rch0017kxsvswc69hfy"},{"post_id":"cj1nl4rar0004kxsvv2nhnujf","tag_id":"cj1nl4raq0003kxsv11n6n8gs","_id":"cj1nl4rcl001akxsva55x96uu"},{"post_id":"cj1nl4rar0004kxsvv2nhnujf","tag_id":"cj1nl4rc3000xkxsv3gg9bi1w","_id":"cj1nl4rco001ekxsvtzy9awko"},{"post_id":"cj1nl4rcb0011kxsvfqrdlehk","tag_id":"cj1nl4raq0003kxsv11n6n8gs","_id":"cj1nl4rcr001gkxsvevx9pq3h"},{"post_id":"cj1nl4rce0016kxsv4dkav496","tag_id":"cj1nl4raq0003kxsv11n6n8gs","_id":"cj1nl4rcv001lkxsvkzg9pyok"},{"post_id":"cj1nl4rci0018kxsvdfkwhe9j","tag_id":"cj1nl4raq0003kxsv11n6n8gs","_id":"cj1nl4rcz001nkxsvhe4sxzio"},{"post_id":"cj1nl4rau0005kxsv4e6kk5fw","tag_id":"cj1nl4raq0003kxsv11n6n8gs","_id":"cj1nl4rd5001skxsvhji7hl6w"},{"post_id":"cj1nl4rau0005kxsv4e6kk5fw","tag_id":"cj1nl4rcd0015kxsv5rt420we","_id":"cj1nl4rd9001ukxsv84zl1d1y"},{"post_id":"cj1nl4rcs001ikxsv7fw0whgr","tag_id":"cj1nl4raq0003kxsv11n6n8gs","_id":"cj1nl4rdb001ykxsv99t0sdqb"},{"post_id":"cj1nl4rb20006kxsvdyglatf6","tag_id":"cj1nl4rcm001dkxsv45kess7v","_id":"cj1nl4rde0021kxsve75i028c"},{"post_id":"cj1nl4rb20006kxsvdyglatf6","tag_id":"cj1nl4rcu001kkxsvrpn8242w","_id":"cj1nl4rdi0025kxsvycvjl8wh"},{"post_id":"cj1nl4rb80009kxsv7rqkofmc","tag_id":"cj1nl4raq0003kxsv11n6n8gs","_id":"cj1nl4rdm0028kxsvbg6idxw0"},{"post_id":"cj1nl4rb80009kxsv7rqkofmc","tag_id":"cj1nl4rd5001rkxsvgbt5rj6k","_id":"cj1nl4rdp002bkxsvwkla3skj"},{"post_id":"cj1nl4rdf0022kxsvx69sj1cg","tag_id":"cj1nl4rcm001dkxsv45kess7v","_id":"cj1nl4rdv002fkxsvc87bbeh8"},{"post_id":"cj1nl4rbc000bkxsva4a6xg82","tag_id":"cj1nl4rcm001dkxsv45kess7v","_id":"cj1nl4rdy002ikxsv6hx8d8kk"},{"post_id":"cj1nl4rbc000bkxsva4a6xg82","tag_id":"cj1nl4rdi0026kxsvh122162f","_id":"cj1nl4re2002mkxsvra1vcqg4"},{"post_id":"cj1nl4rdw002gkxsvobjrkkl9","tag_id":"cj1nl4rcm001dkxsv45kess7v","_id":"cj1nl4re9002pkxsvmuxvpqsn"},{"post_id":"cj1nl4rbm000hkxsvo27rppn1","tag_id":"cj1nl4rcm001dkxsv45kess7v","_id":"cj1nl4rej002ukxsv3fbxosyh"},{"post_id":"cj1nl4rbm000hkxsvo27rppn1","tag_id":"cj1nl4re0002lkxsvlcq7cepv","_id":"cj1nl4rem002xkxsv6914qo50"},{"post_id":"cj1nl4rbs000okxsvifi7yrjs","tag_id":"cj1nl4reg002skxsvnlz0z3pe","_id":"cj1nl4rep0031kxsvhfene0ma"},{"post_id":"cj1nl4reo0030kxsv60te7vm8","tag_id":"cj1nl4ren002zkxsve85wg0zi","_id":"cj1nl4rev0035kxsvkp42znlk"},{"post_id":"cj1nl4rcl001bkxsvrmvblr5e","tag_id":"cj1nl4ren002zkxsve85wg0zi","_id":"cj1nl4rf00038kxsvqt2vh3hg"},{"post_id":"cj1nl4rcp001fkxsvk0zd9el4","tag_id":"cj1nl4ren002zkxsve85wg0zi","_id":"cj1nl4rf4003fkxsvo1se1nv0"},{"post_id":"cj1nl4rf3003dkxsv48kfkoaq","tag_id":"cj1nl4rcm001dkxsv45kess7v","_id":"cj1nl4rf8003ikxsvfp04kp9c"},{"post_id":"cj1nl4rcv001mkxsv8ov7t3bo","tag_id":"cj1nl4ren002zkxsve85wg0zi","_id":"cj1nl4rfm003rkxsvb6r47mkh"},{"post_id":"cj1nl4rcv001mkxsv8ov7t3bo","tag_id":"cj1nl4rf8003jkxsv51s60vbv","_id":"cj1nl4rfq003ukxsvgcnyeinx"},{"post_id":"cj1nl4rfn003skxsvsn2vbv97","tag_id":"cj1nl4rcm001dkxsv45kess7v","_id":"cj1nl4rfu003ykxsvp2b0yntp"},{"post_id":"cj1nl4rd0001pkxsvh8wpboqk","tag_id":"cj1nl4ren002zkxsve85wg0zi","_id":"cj1nl4rg70045kxsv9ipswjjk"},{"post_id":"cj1nl4rd0001pkxsvh8wpboqk","tag_id":"cj1nl4rft003xkxsv159fw9pm","_id":"cj1nl4rgb0049kxsvbjbx99an"},{"post_id":"cj1nl4rd6001tkxsv2lkkphnm","tag_id":"cj1nl4ren002zkxsve85wg0zi","_id":"cj1nl4rge004ckxsvyk2umsyh"},{"post_id":"cj1nl4rg80047kxsv3ojfvy9b","tag_id":"cj1nl4ren002zkxsve85wg0zi","_id":"cj1nl4rgj004gkxsvvm6ucosa"},{"post_id":"cj1nl4rd9001wkxsv37rjqnk0","tag_id":"cj1nl4ren002zkxsve85wg0zi","_id":"cj1nl4rgl004jkxsv7tnzhjwq"},{"post_id":"cj1nl4rdc0020kxsva8eeadnz","tag_id":"cj1nl4rgk004hkxsvvbswykvl","_id":"cj1nl4rgr004okxsvu4p7ehnk"},{"post_id":"cj1nl4rdj0027kxsvav51iztu","tag_id":"cj1nl4rcm001dkxsv45kess7v","_id":"cj1nl4rgu004tkxsvzjkgkgpx"},{"post_id":"cj1nl4rdj0027kxsvav51iztu","tag_id":"cj1nl4rgq004mkxsv9cjw6lbz","_id":"cj1nl4rgv004wkxsvcy9fmpfh"},{"post_id":"cj1nl4rdm0029kxsv979zukli","tag_id":"cj1nl4rcm001dkxsv45kess7v","_id":"cj1nl4rgw004zkxsvfj4xvivt"},{"post_id":"cj1nl4rdm0029kxsv979zukli","tag_id":"cj1nl4rgt004skxsvvu63ou95","_id":"cj1nl4rgx0051kxsv0z337v6z"},{"post_id":"cj1nl4rdq002dkxsvxnhy6rnm","tag_id":"cj1nl4ren002zkxsve85wg0zi","_id":"cj1nl4rgy0054kxsvy6eso5j0"},{"post_id":"cj1nl4rdy002kkxsv03m04ti0","tag_id":"cj1nl4ren002zkxsve85wg0zi","_id":"cj1nl4rgz0058kxsv6cwzi8y7"},{"post_id":"cj1nl4re2002nkxsvy32e1jsl","tag_id":"cj1nl4rcm001dkxsv45kess7v","_id":"cj1nl4rh1005dkxsv8bdwzc1o"},{"post_id":"cj1nl4re2002nkxsvy32e1jsl","tag_id":"cj1nl4rgz0056kxsvz4s4u72i","_id":"cj1nl4rh2005ekxsvmfe6pdbk"},{"post_id":"cj1nl4reb002qkxsvrcfagwk5","tag_id":"cj1nl4rcm001dkxsv45kess7v","_id":"cj1nl4rh4005ikxsvus4n4fps"},{"post_id":"cj1nl4reb002qkxsvrcfagwk5","tag_id":"cj1nl4rh0005akxsvr91505mt","_id":"cj1nl4rh4005jkxsv2l196gj5"},{"post_id":"cj1nl4reh002tkxsvn2nx8m5o","tag_id":"cj1nl4ren002zkxsve85wg0zi","_id":"cj1nl4rh5005nkxsvsqdy91xe"},{"post_id":"cj1nl4rek002wkxsvtpp5poiw","tag_id":"cj1nl4rcm001dkxsv45kess7v","_id":"cj1nl4rh8005rkxsv22p27ppd"},{"post_id":"cj1nl4rek002wkxsvtpp5poiw","tag_id":"cj1nl4rh4005kkxsvtzxo4rgu","_id":"cj1nl4rh8005skxsv6zrpk66f"},{"post_id":"cj1nl4req0032kxsv9mfll6v4","tag_id":"cj1nl4ren002zkxsve85wg0zi","_id":"cj1nl4rha005zkxsvke65rbku"},{"post_id":"cj1nl4req0032kxsv9mfll6v4","tag_id":"cj1nl4rh7005pkxsvkfu4sywd","_id":"cj1nl4rha0062kxsvzi39w380"},{"post_id":"cj1nl4req0032kxsv9mfll6v4","tag_id":"cj1nl4rh8005vkxsvheda9gmd","_id":"cj1nl4rhb0064kxsvryfkhw0a"},{"post_id":"cj1nl4rew0037kxsv4mj8yqp5","tag_id":"cj1nl4rcm001dkxsv45kess7v","_id":"cj1nl4rhb0067kxsvfblvu30r"},{"post_id":"cj1nl4rew0037kxsv4mj8yqp5","tag_id":"cj1nl4rh9005ykxsvm17naal3","_id":"cj1nl4rhc0069kxsvytll4hfo"},{"post_id":"cj1nl4rf00039kxsvn0a835sy","tag_id":"cj1nl4ren002zkxsve85wg0zi","_id":"cj1nl4rhd006ckxsvi33z9mtk"},{"post_id":"cj1nl4rf00039kxsvn0a835sy","tag_id":"cj1nl4rhb0063kxsvpy2m2mcs","_id":"cj1nl4rhe006ekxsvu4xhjb0m"},{"post_id":"cj1nl4rf6003gkxsvyjy2mrjq","tag_id":"cj1nl4raq0003kxsv11n6n8gs","_id":"cj1nl4rhg006hkxsv3j9cfeo5"},{"post_id":"cj1nl4rf6003gkxsvyjy2mrjq","tag_id":"cj1nl4rhc0068kxsv492zwhs6","_id":"cj1nl4rhg006ikxsvh9mp42ie"},{"post_id":"cj1nl4rf9003kkxsvtlouqbco","tag_id":"cj1nl4raq0003kxsv11n6n8gs","_id":"cj1nl4rhi006lkxsvtpc0l9h9"},{"post_id":"cj1nl4rf9003kkxsvtlouqbco","tag_id":"cj1nl4rhd006dkxsv4e6kg8oa","_id":"cj1nl4rhi006nkxsvitkxgwqy"},{"post_id":"cj1nl4rfb003mkxsvedf5bx9f","tag_id":"cj1nl4raq0003kxsv11n6n8gs","_id":"cj1nl4rhj006qkxsvic9xn2x3"},{"post_id":"cj1nl4rfb003mkxsvedf5bx9f","tag_id":"cj1nl4rhd006dkxsv4e6kg8oa","_id":"cj1nl4rhk006skxsv48s8ixyc"},{"post_id":"cj1nl4rfg003okxsvahdj6s25","tag_id":"cj1nl4raq0003kxsv11n6n8gs","_id":"cj1nl4rhl006ukxsv2lsp2m98"},{"post_id":"cj1nl4rfg003okxsvahdj6s25","tag_id":"cj1nl4rhd006dkxsv4e6kg8oa","_id":"cj1nl4rhn006xkxsvjx86zsmt"},{"post_id":"cj1nl4rfq003vkxsvfu6sj2e1","tag_id":"cj1nl4raq0003kxsv11n6n8gs","_id":"cj1nl4rhq006zkxsvakilrpc8"},{"post_id":"cj1nl4rfq003vkxsvfu6sj2e1","tag_id":"cj1nl4rhd006dkxsv4e6kg8oa","_id":"cj1nl4rhs0072kxsvtbzbvsbp"},{"post_id":"cj1nl4rfu003zkxsvqf61sj9w","tag_id":"cj1nl4raq0003kxsv11n6n8gs","_id":"cj1nl4rht0074kxsvre7vcgcb"},{"post_id":"cj1nl4rfu003zkxsvqf61sj9w","tag_id":"cj1nl4rhd006dkxsv4e6kg8oa","_id":"cj1nl4rhv0077kxsv6s1y8i92"},{"post_id":"cj1nl4rfy0041kxsvwkurnxev","tag_id":"cj1nl4raq0003kxsv11n6n8gs","_id":"cj1nl4rhw0079kxsvz507vvsf"},{"post_id":"cj1nl4rfy0041kxsvwkurnxev","tag_id":"cj1nl4rhd006dkxsv4e6kg8oa","_id":"cj1nl4rhx007ckxsvtz5rbq4w"},{"post_id":"cj1nl4rgc004bkxsvx990biiu","tag_id":"cj1nl4rcm001dkxsv45kess7v","_id":"cj1nl4rhy007ekxsveugoiqg4"},{"post_id":"cj1nl4rgc004bkxsvx990biiu","tag_id":"cj1nl4rhv0078kxsvhwzpz02d","_id":"cj1nl4rhz007hkxsvohicofpl"},{"post_id":"cj1nl4rgg004ekxsv3ihvwk1l","tag_id":"cj1nl4rcm001dkxsv45kess7v","_id":"cj1nl4rhz007ikxsvpyaf1glt"},{"post_id":"cj1nl4rgg004ekxsv3ihvwk1l","tag_id":"cj1nl4rhy007dkxsvurg6ux37","_id":"cj1nl4ri0007lkxsv6us6dhty"},{"post_id":"cj1nl4rij007qkxsvoeo04lj3","tag_id":"cj1nl4raq0003kxsv11n6n8gs","_id":"cj1nl4rit007tkxsvzef1s6ai"},{"post_id":"cj1nl4rit007vkxsvju2wbaue","tag_id":"cj1nl4raq0003kxsv11n6n8gs","_id":"cj1nl4riy0080kxsv0fcmj8xn"},{"post_id":"cj1nl4riz0081kxsv2ehn892i","tag_id":"cj1nl4raq0003kxsv11n6n8gs","_id":"cj1nl4rj30085kxsv112ep0yy"},{"post_id":"cj1nl4rif007okxsvd9beb2ji","tag_id":"cj1nl4rin007rkxsvwnp289jy","_id":"cj1nl4rj40086kxsvahjjfhix"},{"post_id":"cj1nl4rif007okxsvd9beb2ji","tag_id":"cj1nl4riy007zkxsvtsqs5ugk","_id":"cj1nl4rj5008akxsvqgk5whr5"},{"post_id":"cj1nl4rif007okxsvd9beb2ji","tag_id":"cj1nl4rhv0078kxsvhwzpz02d","_id":"cj1nl4rj5008bkxsvv0zbmg0j"},{"post_id":"cj1nl4rif007okxsvd9beb2ji","tag_id":"cj1nl4rh7005pkxsvkfu4sywd","_id":"cj1nl4rj5008ekxsvihs5cega"},{"post_id":"cj1nl4rj10082kxsvt6b2qwbm","tag_id":"cj1nl4raq0003kxsv11n6n8gs","_id":"cj1nl4rj6008fkxsv5qkpyf6u"},{"post_id":"cj1nl4rio007skxsvn761rdtt","tag_id":"cj1nl4rcm001dkxsv45kess7v","_id":"cj1nl4rj6008ikxsvjhlkkqsq"},{"post_id":"cj1nl4rio007skxsvn761rdtt","tag_id":"cj1nl4rj20084kxsvzic9gqcg","_id":"cj1nl4rj6008jkxsvmma9c0b7"},{"post_id":"cj1nl4riw007ykxsvk7mgt06h","tag_id":"cj1nl4rcm001dkxsv45kess7v","_id":"cj1nl4rj6008lkxsvxs9swfmu"},{"post_id":"cj1nl4riw007ykxsvk7mgt06h","tag_id":"cj1nl4rj40089kxsvuvpsf9s2","_id":"cj1nl4rj7008mkxsv7e467hzh"}],"Tag":[{"name":"portal","_id":"cj1nl4raq0003kxsv11n6n8gs"},{"name":"hadoop","_id":"cj1nl4rb50008kxsva7eoldku"},{"name":"machine learning","_id":"cj1nl4rbp000kkxsvp5n2bnh5"},{"name":"storm","_id":"cj1nl4rc3000xkxsv3gg9bi1w"},{"name":"cloud","_id":"cj1nl4rcd0015kxsv5rt420we"},{"name":"core","_id":"cj1nl4rcm001dkxsv45kess7v"},{"name":"cloudfoundry","_id":"cj1nl4rcu001kkxsvrpn8242w"},{"name":"blockchain","_id":"cj1nl4rd5001rkxsvgbt5rj6k"},{"name":"openstack","_id":"cj1nl4rdi0026kxsvh122162f"},{"name":"openshift","_id":"cj1nl4re0002lkxsvlcq7cepv"},{"name":"opensource","_id":"cj1nl4reg002skxsvnlz0z3pe"},{"name":"detail","_id":"cj1nl4ren002zkxsve85wg0zi"},{"name":"compose","_id":"cj1nl4rf8003jkxsv51s60vbv"},{"name":"dockerfile","_id":"cj1nl4rft003xkxsv159fw9pm"},{"name":"cookbook","_id":"cj1nl4rgk004hkxsvvbswykvl"},{"name":"filesystem","_id":"cj1nl4rgq004mkxsv9cjw6lbz"},{"name":"k8s","_id":"cj1nl4rgt004skxsvvu63ou95"},{"name":"network","_id":"cj1nl4rgz0056kxsvz4s4u72i"},{"name":"security","_id":"cj1nl4rh0005akxsvr91505mt"},{"name":"storage","_id":"cj1nl4rh4005kkxsvtzxo4rgu"},{"name":"docker","_id":"cj1nl4rh7005pkxsvkfu4sywd"},{"name":"swarmkit","_id":"cj1nl4rh8005vkxsvheda9gmd"},{"name":"mesos","_id":"cj1nl4rh9005ykxsvm17naal3"},{"name":"yaml","_id":"cj1nl4rhb0063kxsvpy2m2mcs"},{"name":"iot","_id":"cj1nl4rhc0068kxsv492zwhs6"},{"name":"programming language","_id":"cj1nl4rhd006dkxsv4e6kg8oa"},{"name":"django","_id":"cj1nl4rhv0078kxsvhwzpz02d"},{"name":"swarm","_id":"cj1nl4rhy007dkxsvurg6ux37"},{"name":"install","_id":"cj1nl4rin007rkxsvwnp289jy"},{"name":"python","_id":"cj1nl4riy007zkxsvtsqs5ugk"},{"name":"rails","_id":"cj1nl4rj20084kxsvzic9gqcg"},{"name":"http","_id":"cj1nl4rj40089kxsvuvpsf9s2"}]}}