{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"themes/landscape/source/css/style.styl","path":"css/style.styl","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/blank.gif","path":"fancybox/blank.gif","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_loading.gif","path":"fancybox/fancybox_loading.gif","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_overlay.png","path":"fancybox/fancybox_overlay.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_loading@2x.gif","path":"fancybox/fancybox_loading@2x.gif","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_sprite.png","path":"fancybox/fancybox_sprite.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/fancybox_sprite@2x.png","path":"fancybox/fancybox_sprite@2x.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.css","path":"fancybox/jquery.fancybox.css","modified":0,"renderable":1},{"_id":"themes/landscape/source/js/script.js","path":"js/script.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.pack.js","path":"fancybox/jquery.fancybox.pack.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.js","path":"fancybox/jquery.fancybox.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/images/banner.jpg","path":"css/images/banner.jpg","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/FontAwesome.otf","path":"css/fonts/FontAwesome.otf","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.woff","path":"css/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.eot","path":"css/fonts/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/fancybox_buttons.png","path":"fancybox/helpers/fancybox_buttons.png","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-media.js","path":"fancybox/helpers/jquery.fancybox-media.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.js","path":"fancybox/helpers/jquery.fancybox-buttons.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.css","path":"fancybox/helpers/jquery.fancybox-buttons.css","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.css","path":"fancybox/helpers/jquery.fancybox-thumbs.css","modified":0,"renderable":1},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.js","path":"fancybox/helpers/jquery.fancybox-thumbs.js","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.ttf","path":"css/fonts/fontawesome-webfont.ttf","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/images/banner-bak.jpg","path":"css/images/banner-bak.jpg","modified":0,"renderable":1},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.svg","path":"css/fonts/fontawesome-webfont.svg","modified":0,"renderable":1}],"Cache":[{"_id":"source/.DS_Store","hash":"c564de0df0129be1b471591cea73a23e88143759","modified":1487398118000},{"_id":"themes/landscape/.DS_Store","hash":"9457f542cf2c3f2a50b9ecd64858fc6f50b0d0c4","modified":1484147080000},{"_id":"themes/landscape/.gitignore","hash":"58d26d4b5f2f94c2d02a4e4a448088e4a2527c77","modified":1484126821000},{"_id":"themes/landscape/Gruntfile.js","hash":"71adaeaac1f3cc56e36c49d549b8d8a72235c9b9","modified":1484126821000},{"_id":"themes/landscape/README.md","hash":"c7e83cfe8f2c724fc9cac32bd71bb5faf9ceeddb","modified":1484126821000},{"_id":"themes/landscape/LICENSE","hash":"c480fce396b23997ee23cc535518ffaaf7f458f8","modified":1484126821000},{"_id":"themes/landscape/_config.yml","hash":"285fb948f9f50e0d2c040283251b1ce20119bfaa","modified":1484214819000},{"_id":"themes/landscape/package.json","hash":"85358dc34311c6662e841584e206a4679183943f","modified":1484126821000},{"_id":"source/about/index.md","hash":"bfee1378ee8b0c03381c10c44157efb661c62430","modified":1484214601000},{"_id":"source/_posts/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1484211471000},{"_id":"source/_posts/bigdata--portal.md","hash":"24a07091d9b8436f253f5d2b44714e466d9f92c2","modified":1486644435000},{"_id":"source/_posts/bigdata-hadoop-portal.md","hash":"d96e2c479c7fbfd6d38bea35f6f323d2014b38af","modified":1484152554000},{"_id":"source/_posts/cloud--portal.md","hash":"310912ed39872e0f8f558e6d1c53302c1946c5ff","modified":1488130858000},{"_id":"source/_posts/bigdata-ml-portal.md","hash":"d96e2c479c7fbfd6d38bea35f6f323d2014b38af","modified":1484152554000},{"_id":"source/_posts/bigdata-storm-portal.md","hash":"d96e2c479c7fbfd6d38bea35f6f323d2014b38af","modified":1484152554000},{"_id":"source/_posts/cloud-paas-cf-core.md","hash":"a77e50fa1b3f67ada1bc5323779e6b3cd98b6cb7","modified":1484152524000},{"_id":"source/_posts/cloud-iaas-portal.md","hash":"891e7a4e7d25ca8433b2166842e8bf5043aca256","modified":1487397085000},{"_id":"source/_posts/cloud-paas-openshift-core.md","hash":"db53a3ce03df639c195d89c45d70b0f0466d1020","modified":1484152520000},{"_id":"source/_posts/cloud-iaas-openstack-core.md","hash":"a4dfe305ed994558c7b4fc58de795922c7799462","modified":1484152388000},{"_id":"source/_posts/cloud-paas-portal.md","hash":"1a96b1ddfa2eb02998c00983879ee03adbe053f2","modified":1484152535000},{"_id":"source/_posts/cmd_django_docker.txt","hash":"278e19844de45ed90b64d49059539be018bb3fd4","modified":1484211486000},{"_id":"source/_posts/db--portal.md","hash":"571d55ed1ff551646f2dfe0330792d267727c440","modified":1486198547000},{"_id":"source/_posts/db-mysql-portal.md","hash":"200e3ea7381f233610ee05a7da0e829f44bd93d3","modified":1484152415000},{"_id":"source/_posts/db-nosql-portal.md","hash":"200e3ea7381f233610ee05a7da0e829f44bd93d3","modified":1484152415000},{"_id":"source/_posts/cloud-saas-portal.md","hash":"1a96b1ddfa2eb02998c00983879ee03adbe053f2","modified":1484152535000},{"_id":"source/_posts/docker-cloud-detail.md","hash":"08f6e57862e8033af8373ef1ac51da8ffdd9faa6","modified":1484152346000},{"_id":"source/_posts/docker--portal.md","hash":"f8b7bfd54a7f5ca35dbb1011c704c6dd3988820b","modified":1486698059000},{"_id":"source/_posts/docker-compose-detail.md","hash":"5ba4bfe293e5ad18e801d97656eb41ea3d4de743","modified":1486738987000},{"_id":"source/_posts/docker-compose-file-detail.md","hash":"49be9993b1a9bbd740383c33e968a9daad204f70","modified":1484724794000},{"_id":"source/_posts/docker-compose-file-tmp.md","hash":"2bc28e5d777a9ce91daa76e3b563195d2a48da54","modified":1484724228000},{"_id":"source/_posts/docker-core.md","hash":"371801da82979044f60999285279fc46cc8c8b12","modified":1484461693000},{"_id":"source/_posts/docker-cookbook.md","hash":"8caa9454e90d4ade8268c325399b17a29c48c945","modified":1484304719000},{"_id":"source/_posts/devops--portal.md","hash":"ea2b74129f061b91bde65a092fcf5c10b860bad1","modified":1488164936000},{"_id":"source/_posts/docker-dockerfile-detail.md","hash":"a0e7e5e21c02381a9cd232349eed7eca7a492eaf","modified":1484308001000},{"_id":"source/_posts/docker-dockerhub-detail.md","hash":"d73c876795bd2175c53d186b816dd2585f00a254","modified":1484307994000},{"_id":"source/_posts/docker-engine-detail.md","hash":"633500ff5a32f2ea0f2cd07b784086da26ff1973","modified":1486704843000},{"_id":"source/_posts/docker-filesystem-core.md","hash":"f57c094ccfa1ff199415a9bdc571f65fc25651e1","modified":1484465420000},{"_id":"source/_posts/docker-image-detail.md","hash":"21421ab56a9a54fe66765d998db6df3dab394563","modified":1484308232000},{"_id":"source/_posts/docker-k8s-core.md","hash":"c932617dd05b171229eb0cbad913f31974775902","modified":1484152331000},{"_id":"source/_posts/docker-machine-detail.md","hash":"29ffe0a3514e51cd741778fbf87becfcf759f828","modified":1486705720000},{"_id":"source/_posts/docker-mesos-core.md","hash":"d8a3215f44c3d11510bad3bc45813e695847ff0e","modified":1486707416000},{"_id":"source/_posts/docker-security-core.md","hash":"e132f49ec05126a1762374f7a7ad5909610680cb","modified":1484457984000},{"_id":"source/_posts/docker-network-core.md","hash":"4adbd26909d01e284da4302cfbe98dc794398caf","modified":1484457933000},{"_id":"source/_posts/docker-storage-core.md","hash":"aba71f3dda6a43005db308862b385778f0b7ddc3","modified":1484461617000},{"_id":"source/_posts/docker-swarm-portal.md","hash":"bf6b2396067641ad231be2c88ffdded4a226f032","modified":1486705236000},{"_id":"source/_posts/docker-store-detail.md","hash":"08f6e57862e8033af8373ef1ac51da8ffdd9faa6","modified":1484152346000},{"_id":"source/_posts/docker-swarm-detail.md","hash":"90d0b433cbf38b71ca755f7e2588168d07a3fe27","modified":1486708018000},{"_id":"source/_posts/docker-swarmkit-detail.md","hash":"ecbe772c349efe2ac43cac2e81357eeabc7d99ed","modified":1484467444000},{"_id":"source/_posts/docker-yaml-detail.md","hash":"5ce41c15cd0ca11e8a6a621bc0c1a60e16c119fe","modified":1484458296000},{"_id":"source/_posts/hello-world.md","hash":"2821cef1e0b6723d18e7f2162653fe696c4dbdc0","modified":1484152366000},{"_id":"source/_posts/docker-swarmnext-detail.md","hash":"859394563adc3ede668fa8e47d0c826e4b249199","modified":1484486949000},{"_id":"source/_posts/iot-portal.md","hash":"a4dfe305ed994558c7b4fc58de795922c7799462","modified":1484152388000},{"_id":"source/_posts/jd-architect-portal.md","hash":"6932aaea71078034615157134e80029d70c28bcd","modified":1484308101000},{"_id":"source/_posts/jd-fullstack-portal.md","hash":"d96e2c479c7fbfd6d38bea35f6f323d2014b38af","modified":1484152554000},{"_id":"source/_posts/jd--portal.md","hash":"b209676bdbfcec23c14bb1a1b6aefbfe4e265304","modified":1488165438000},{"_id":"source/_posts/lang--portal.md","hash":"2cd25c7e83b0f07406bfa8dfc5ea285c1e1936ce","modified":1484152314000},{"_id":"source/_posts/lang-cx-portal.md","hash":"2cd25c7e83b0f07406bfa8dfc5ea285c1e1936ce","modified":1484152314000},{"_id":"source/_posts/lang-java-portal.md","hash":"6bb68c01c82c399c5be319c509cd74d8514a10bc","modified":1488165014000},{"_id":"source/_posts/lang-js-nodejs-core.md","hash":"c3d8104fd5c3ba26bd0215b83a6f169248a6921c","modified":1484152422000},{"_id":"source/_posts/lang-js-portal.md","hash":"43250aa6d09e932b138ce2c3708ad60826aa8667","modified":1488129053000},{"_id":"source/_posts/lang-python-django-core.md","hash":"b58bfdbcca877f4c3041e6e25fa36b7f2be82140","modified":1484214459000},{"_id":"source/_posts/lang-python-django-install.md","hash":"7f922a7b5eb9ee80cb5cd6884eb3d87f81a0d595","modified":1484214542000},{"_id":"source/_posts/lang-python-portal.md","hash":"2278727305b54f93d9b7015688a6b0d9da78f07d","modified":1488165095000},{"_id":"source/_posts/lang-ruby-rails-core.md","hash":"409d5e7c9bada410b4818fb0f2c1b364623e0e05","modified":1488127169000},{"_id":"source/_posts/microservice-portal.md","hash":"e42d0d7927147598756d9f5b2f6ea7428105fde2","modified":1486196553000},{"_id":"themes/landscape/scripts/fancybox.js","hash":"aa411cd072399df1ddc8e2181a3204678a5177d9","modified":1484126821000},{"_id":"source/_posts/network-http-core.md","hash":"a5853d8ae8a690d707ee6e5e669ea3470c492940","modified":1484152471000},{"_id":"source/_posts/network-portal.md","hash":"1d97325d88b500484030a30cafd44c41b51c358c","modified":1484486959000},{"_id":"source/_posts/system-linux-portal.md","hash":"4e023a556e18a248bb932ff4067513be2779b1c0","modified":1484152446000},{"_id":"themes/landscape/source/.DS_Store","hash":"1429b9c08aeb8c15b3499d150e011720f8ff275c","modified":1484147087000},{"_id":"themes/landscape/layout/archive.ejs","hash":"2703b07cc8ac64ae46d1d263f4653013c7e1666b","modified":1484126821000},{"_id":"themes/landscape/layout/category.ejs","hash":"765426a9c8236828dc34759e604cc2c52292835a","modified":1484126821000},{"_id":"themes/landscape/layout/index.ejs","hash":"aa1b4456907bdb43e629be3931547e2d29ac58c8","modified":1484126821000},{"_id":"themes/landscape/layout/layout.ejs","hash":"f155824ca6130080bb057fa3e868a743c69c4cf5","modified":1484126821000},{"_id":"themes/landscape/layout/page.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1484126821000},{"_id":"themes/landscape/layout/post.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1484126821000},{"_id":"themes/landscape/languages/default.yml","hash":"3083f319b352d21d80fc5e20113ddf27889c9d11","modified":1484126821000},{"_id":"themes/landscape/layout/tag.ejs","hash":"eaa7b4ccb2ca7befb90142e4e68995fb1ea68b2e","modified":1484126821000},{"_id":"themes/landscape/languages/fr.yml","hash":"84ab164b37c6abf625473e9a0c18f6f815dd5fd9","modified":1484126821000},{"_id":"themes/landscape/languages/nl.yml","hash":"12ed59faba1fc4e8cdd1d42ab55ef518dde8039c","modified":1484126821000},{"_id":"themes/landscape/languages/ru.yml","hash":"4fda301bbd8b39f2c714e2c934eccc4b27c0a2b0","modified":1484126821000},{"_id":"themes/landscape/languages/zh-TW.yml","hash":"53ce3000c5f767759c7d2c4efcaa9049788599c3","modified":1484126821000},{"_id":"themes/landscape/languages/no.yml","hash":"965a171e70347215ec726952e63f5b47930931ef","modified":1484126821000},{"_id":"themes/landscape/languages/zh-CN.yml","hash":"ca40697097ab0b3672a80b455d3f4081292d1eed","modified":1484126821000},{"_id":"themes/landscape/source/css/_variables.styl","hash":"06e2f44b92c26c5d71abf01e7b43ee0dfd2010c7","modified":1484148828000},{"_id":"themes/landscape/source/css/style.styl","hash":"a70d9c44dac348d742702f6ba87e5bb3084d65db","modified":1484126821000},{"_id":"themes/landscape/source/css/.DS_Store","hash":"0c4a669591bf1723e84d44bb15e7b684e25ff531","modified":1484146229000},{"_id":"themes/landscape/source/css/_extend.styl","hash":"222fbe6d222531d61c1ef0f868c90f747b1c2ced","modified":1484126821000},{"_id":"themes/landscape/layout/_partial/after-footer.ejs","hash":"9292c640bdf7c8eb6fed2e8a1800f1cc7f43722b","modified":1484145585000},{"_id":"themes/landscape/layout/_partial/archive-post.ejs","hash":"c7a71425a946d05414c069ec91811b5c09a92c47","modified":1484126821000},{"_id":"themes/landscape/layout/_partial/article.ejs","hash":"3ff1260ab513c523a610f1d83b20961b5d140d6b","modified":1484213973000},{"_id":"themes/landscape/layout/_partial/archive.ejs","hash":"931aaaffa0910a48199388ede576184ff15793ee","modified":1484126821000},{"_id":"themes/landscape/layout/_partial/footer.ejs","hash":"f6975a227829834c026b17ee2493d06a16202b94","modified":1484144946000},{"_id":"themes/landscape/layout/_partial/head.ejs","hash":"bfd64f2a831a6acb7f5bae852cae3098a91e1997","modified":1484145371000},{"_id":"themes/landscape/layout/_partial/mobile-nav.ejs","hash":"e952a532dfc583930a666b9d4479c32d4a84b44e","modified":1484126821000},{"_id":"themes/landscape/layout/_partial/header.ejs","hash":"9d7b18ae2a5479d9ae0eb053ea7043ab8a9bd642","modified":1484145235000},{"_id":"themes/landscape/layout/_partial/sidebar.ejs","hash":"930da35cc2d447a92e5ee8f835735e6fd2232469","modified":1484126821000},{"_id":"themes/landscape/layout/_widget/category.ejs","hash":"dd1e5af3c6af3f5d6c85dfd5ca1766faed6a0b05","modified":1484126821000},{"_id":"themes/landscape/layout/_widget/archive.ejs","hash":"beb4a86fcc82a9bdda9289b59db5a1988918bec3","modified":1484126821000},{"_id":"themes/landscape/layout/_widget/recent_posts.ejs","hash":"0d4f064733f8b9e45c0ce131fe4a689d570c883a","modified":1484126821000},{"_id":"themes/landscape/layout/_widget/tag.ejs","hash":"2de380865df9ab5f577f7d3bcadf44261eb5faae","modified":1484126821000},{"_id":"themes/landscape/source/fancybox/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1484126821000},{"_id":"themes/landscape/layout/_widget/tagcloud.ejs","hash":"b4a2079101643f63993dcdb32925c9b071763b46","modified":1484126821000},{"_id":"themes/landscape/source/fancybox/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1484126821000},{"_id":"themes/landscape/source/fancybox/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1484126821000},{"_id":"themes/landscape/source/fancybox/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1484126821000},{"_id":"themes/landscape/source/fancybox/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1484126821000},{"_id":"themes/landscape/source/fancybox/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1484126821000},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.css","hash":"aaa582fb9eb4b7092dc69fcb2d5b1c20cca58ab6","modified":1484126821000},{"_id":"themes/landscape/source/js/script.js","hash":"2876e0b19ce557fca38d7c6f49ca55922ab666a1","modified":1484126821000},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.pack.js","hash":"9e0d51ca1dbe66f6c0c7aefd552dc8122e694a6e","modified":1484126821000},{"_id":"themes/landscape/source/fancybox/jquery.fancybox.js","hash":"d08b03a42d5c4ba456ef8ba33116fdbb7a9cabed","modified":1484126821000},{"_id":"themes/landscape/source/css/images/banner.jpg","hash":"e2c9ff91ca7c221c23e41dba0d4b8dfd90d28a6c","modified":1484146333000},{"_id":"themes/landscape/source/css/_util/mixin.styl","hash":"44f32767d9fd3c1c08a60d91f181ee53c8f0dbb3","modified":1484126821000},{"_id":"themes/landscape/source/css/_util/grid.styl","hash":"0bf55ee5d09f193e249083602ac5fcdb1e571aed","modified":1484126821000},{"_id":"themes/landscape/source/css/_partial/archive.styl","hash":"db15f5677dc68f1730e82190bab69c24611ca292","modified":1484126821000},{"_id":"themes/landscape/source/css/_partial/article.styl","hash":"10685f8787a79f79c9a26c2f943253450c498e3e","modified":1484126821000},{"_id":"themes/landscape/source/css/_partial/footer.styl","hash":"e35a060b8512031048919709a8e7b1ec0e40bc1b","modified":1484126821000},{"_id":"themes/landscape/source/css/_partial/header.styl","hash":"9658cb416b434dc6c3a8c2c15511eb170f363a3d","modified":1484148524000},{"_id":"themes/landscape/source/css/_partial/highlight.styl","hash":"bf4e7be1968dad495b04e83c95eac14c4d0ad7c0","modified":1484126821000},{"_id":"themes/landscape/source/css/_partial/comment.styl","hash":"79d280d8d203abb3bd933ca9b8e38c78ec684987","modified":1484126821000},{"_id":"themes/landscape/source/css/_partial/mobile.styl","hash":"a399cf9e1e1cec3e4269066e2948d7ae5854d745","modified":1484126821000},{"_id":"themes/landscape/source/css/_partial/sidebar-aside.styl","hash":"890349df5145abf46ce7712010c89237900b3713","modified":1484126821000},{"_id":"themes/landscape/source/css/_partial/sidebar-bottom.styl","hash":"8fd4f30d319542babfd31f087ddbac550f000a8a","modified":1484126821000},{"_id":"themes/landscape/source/css/_partial/sidebar.styl","hash":"404ec059dc674a48b9ab89cd83f258dec4dcb24d","modified":1484126821000},{"_id":"themes/landscape/source/css/fonts/FontAwesome.otf","hash":"b5b4f9be85f91f10799e87a083da1d050f842734","modified":1484126821000},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.woff","hash":"04c3bf56d87a0828935bd6b4aee859995f321693","modified":1484126821000},{"_id":"themes/landscape/layout/_partial/post/gallery.ejs","hash":"3d9d81a3c693ff2378ef06ddb6810254e509de5b","modified":1484126821000},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.eot","hash":"7619748fe34c64fb157a57f6d4ef3678f63a8f5e","modified":1484126821000},{"_id":"themes/landscape/layout/_partial/post/date.ejs","hash":"6197802873157656e3077c5099a7dda3d3b01c29","modified":1484126821000},{"_id":"themes/landscape/layout/_partial/post/category.ejs","hash":"c6bcd0e04271ffca81da25bcff5adf3d46f02fc0","modified":1484213953000},{"_id":"themes/landscape/layout/_partial/post/nav.ejs","hash":"16a904de7bceccbb36b4267565f2215704db2880","modified":1484126821000},{"_id":"themes/landscape/layout/_partial/post/tag.ejs","hash":"2fcb0bf9c8847a644167a27824c9bb19ac74dd14","modified":1484126821000},{"_id":"themes/landscape/layout/_partial/post/title.ejs","hash":"2f275739b6f1193c123646a5a31f37d48644c667","modified":1484126821000},{"_id":"themes/landscape/source/fancybox/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1484126821000},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-media.js","hash":"294420f9ff20f4e3584d212b0c262a00a96ecdb3","modified":1484126821000},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.js","hash":"dc3645529a4bf72983a39fa34c1eb9146e082019","modified":1484126821000},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1484126821000},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1484126821000},{"_id":"themes/landscape/source/fancybox/helpers/jquery.fancybox-thumbs.js","hash":"47da1ae5401c24b5c17cc18e2730780f5c1a7a0c","modified":1484126821000},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.ttf","hash":"7f09c97f333917034ad08fa7295e916c9f72fd3f","modified":1484126821000},{"_id":"themes/landscape/source/css/images/banner-bak.jpg","hash":"f44aa591089fcb3ec79770a1e102fd3289a7c6a6","modified":1484126821000},{"_id":"themes/landscape/source/css/fonts/fontawesome-webfont.svg","hash":"46fcc0194d75a0ddac0a038aee41b23456784814","modified":1484126821000},{"_id":"public/about/index.html","hash":"38a142ec640c1a89d57cff340974e3547797758a","modified":1491187935436},{"_id":"public/python/lang-python-django-install/index.html","hash":"eceecca1dad300d8f7e375a681db751ee6516f1e","modified":1491187935438},{"_id":"public/docker/docker-machine-detail/index.html","hash":"667159a911c3572f943b439a68cb448b45c246b1","modified":1491187935446},{"_id":"public/python/bigdata-ml-portal/index.html","hash":"0ca2cb984c0a3d3e133ad8d8b86c5c2a1748b638","modified":1491187935449},{"_id":"public/python/bigdata-storm-portal/index.html","hash":"528b02d60e2521e12f111072c1606e90cd43c748","modified":1491187935439},{"_id":"public/paas/cloud-paas-cf-core/index.html","hash":"f3108a2a53c13bca80c2de40e665815aa16082ef","modified":1491187935439},{"_id":"public/paas/cloud-paas-openshift-core/index.html","hash":"00df1c5fc173c4967952a83732a1a26cf00c1834","modified":1491187935439},{"_id":"public/iaas/cloud-iaas-portal/index.html","hash":"f01b16794c2befd70e51201783289d57f2c13f60","modified":1491187935439},{"_id":"public/iaas/cloud-iaas-openstack-core/index.html","hash":"9506912d491af30a5854cfe2ba5f40c9e8ce9221","modified":1491187935439},{"_id":"public/paas/cloud-paas-portal/index.html","hash":"2c1f4fce4d1086a8131f348e4f5de75d5aa774c8","modified":1491187935439},{"_id":"public/java/db--portal/index.html","hash":"d191e968bfc668580e106ab68ac5e16d7aa950d3","modified":1491187935440},{"_id":"public/java/db-mysql-portal/index.html","hash":"77bef19e03086bbf31d02538bcdda476dd94d28c","modified":1491187935440},{"_id":"public/java/db-nosql-portal/index.html","hash":"5f8bbd1aa6df8c024eeaf2bb0d72958e71422c88","modified":1491187935443},{"_id":"public/docker/docker-cloud-detail/index.html","hash":"ceae1f4723b833c53d5d15b43c7c5c07dfff56ae","modified":1491187935444},{"_id":"public/paas/cloud-saas-portal/index.html","hash":"ba401ea2ac32f0b7f733bf18bf164c29dd97e0ed","modified":1491187935444},{"_id":"public/docker/docker-compose-detail/index.html","hash":"21771c916116d29379d01a4015f43459ff127c2e","modified":1491187935445},{"_id":"public/docker/docker-compose-file-detail/index.html","hash":"0a6640e92fbd148217fa54f32c8128b09960b1bd","modified":1491187935445},{"_id":"public/python/bigdata--portal/index.html","hash":"3ebd7cd690293f04537279b89f8797c9954aea6a","modified":1491187935445},{"_id":"public/docker/docker-core/index.html","hash":"05e71e0cbdcb47b944ec27e7d6927f9f79f67d8b","modified":1491187935445},{"_id":"public/docker/docker-cookbook/index.html","hash":"49538c6a411b66232c5216381a0021e520bf92a8","modified":1491187935445},{"_id":"public/devops/devops--portal/index.html","hash":"3d81fcb399e1beeaad73ca69aa3e98171f0300d7","modified":1491187935445},{"_id":"public/docker/docker-dockerfile-detail/index.html","hash":"919771b8eeeb9d488d81fc394915f8fe7f7581c5","modified":1491187935445},{"_id":"public/docker/docker-dockerhub-detail/index.html","hash":"8047d91678ff4c6753f7ffe834595b357d4e2a72","modified":1491187935446},{"_id":"public/docker/docker-engine-detail/index.html","hash":"823191dfb86203862c5a10407e8120530e20faf9","modified":1491187935446},{"_id":"public/docker/docker-filesystem-core/index.html","hash":"2b3e16854947837059326fd3d1ba289106a50f8e","modified":1491187935446},{"_id":"public/docker/docker-image-detail/index.html","hash":"81d5083ee7b954c01bf85ace07d95a916ae7450a","modified":1491187935446},{"_id":"public/docker/docker-k8s-core/index.html","hash":"801534fe5b043e7af720050b2866fac5d5967ffa","modified":1491187935446},{"_id":"public/python/bigdata-hadoop-portal/index.html","hash":"ababd7bdf0a34ba76e918c5501278307a9725541","modified":1491187935446},{"_id":"public/docker/docker-mesos-core/index.html","hash":"539d8e9ac2ac4609a90244d1b0508d9c56634994","modified":1491187935446},{"_id":"public/docker/docker-security-core/index.html","hash":"c5361c6a46fe9e5cdedacda4dc2e31cd8576b8b9","modified":1491187935446},{"_id":"public/docker/docker-network-core/index.html","hash":"7d4b880cf400f84a5aba5746254bc56af1727963","modified":1491187935438},{"_id":"public/docker/docker-storage-core/index.html","hash":"f2605cecc9580815caabb9fc3cc0d6aea0644831","modified":1491187935446},{"_id":"public/docker/docker-swarm-portal/index.html","hash":"4a0849fb9681547e4738061df9b5095c652114e7","modified":1491187935446},{"_id":"public/docker/docker-store-detail/index.html","hash":"a16b5a764c6a2e04ae3a0430be30ee842888450c","modified":1491187935446},{"_id":"public/docker/docker-swarm-detail/index.html","hash":"7d6d36fd75ac21daec64536ff4ce55748fd5e251","modified":1491187935447},{"_id":"public/iaas/docker-swarmkit-detail/index.html","hash":"d8f404735f486c85b807e26933d5e233f71a476f","modified":1491187935447},{"_id":"public/docker/docker-yaml-detail/index.html","hash":"d787258deb2f609b8d436709f4d2efa3d0f56540","modified":1491187935447},{"_id":"public/network/network-portal/index.html","hash":"32df1a1adb58dc9694a62bec0d27ed9e640b2c47","modified":1491187935449},{"_id":"public/docker/docker-swarmnext-detail/index.html","hash":"4b34a63978ec47ea37d2e8d0c83d6a79c80fdb68","modified":1491187935457},{"_id":"public/iaas/iot-portal/index.html","hash":"313516d09865d5391cd0d115a7074c7e56a2de4c","modified":1491187935447},{"_id":"public/cloud/jd-architect-portal/index.html","hash":"7aec490fe6b0301987d7cecef7a8c8d6dd7c9fec","modified":1491187935447},{"_id":"public/python/jd-fullstack-portal/index.html","hash":"00a139a341ca1038163b7cc59faec578733803fb","modified":1491187935447},{"_id":"public/cloud/jd--portal/index.html","hash":"dddf22e274a4ca2551a9c905b37b428674904a0e","modified":1486131988517},{"_id":"public/cx/lang--portal/index.html","hash":"fdb5c7e55b35155ce31353251dca0a7842c9bce0","modified":1491187935447},{"_id":"public/cx/lang-cx-portal/index.html","hash":"41a9ac18a8ffac30b8bac5621694f244446bcae3","modified":1491187935448},{"_id":"public/java/lang-java-portal/index.html","hash":"9d0efca46e6fe7ed0d35aaa3653aa6fccb5817f0","modified":1491187935448},{"_id":"public/nodejs/lang-js-nodejs-core/index.html","hash":"9162e1e845d8174c6d5a6cbc00ca2a91193916c4","modified":1491187935448},{"_id":"public/js/lang-js-portal/index.html","hash":"9b50a32943e833d7a409dfe6f4ec31485d05c4c5","modified":1491187935448},{"_id":"public/python/lang-python-django-core/index.html","hash":"611cbe09aeb246b464658ec46315683d0760aeeb","modified":1491187935448},{"_id":"public/python/cloud--portal/index.html","hash":"0bbd15d2d76e338fdb2a0b5eee604bc922ad1ed2","modified":1491187935457},{"_id":"public/python/lang-python-portal/index.html","hash":"f6955198da405793f2c9e649252a469968d468b8","modified":1491187935448},{"_id":"public/microservice/microservice-portal/index.html","hash":"5e26933f77417510396e9ed99177906e42e088f2","modified":1491187935448},{"_id":"public/ruby/lang-ruby-rails-core/index.html","hash":"047da02ff84042cea501789e54bc2777a6b97f1b","modified":1491187935448},{"_id":"public/network/network-http-core/index.html","hash":"e09ffc47b6bcf9dc3b9282d2134ece5333dc0c83","modified":1491187935448},{"_id":"public/linux/system-linux-portal/index.html","hash":"0c467adee772b9dbe3ba740a75bcb9c08193169d","modified":1491187935449},{"_id":"public/tmp/hello-world/index.html","hash":"6371a49b28054dba247186886ea46c6733abc55f","modified":1491187935449},{"_id":"public/archives/index.html","hash":"043b0b24ae36257957810717e5abf83de758961b","modified":1491187935451},{"_id":"public/archives/page/2/index.html","hash":"6547956965f3061620188239e806472c1322a21b","modified":1491187935451},{"_id":"public/archives/page/3/index.html","hash":"0742d11dac526fca3d9d26f8ffe4f360c097ddc8","modified":1491187935451},{"_id":"public/archives/page/4/index.html","hash":"ab6511f0e81844c1e3ffd026d2bd51f026731f45","modified":1491187935452},{"_id":"public/archives/page/5/index.html","hash":"605efe05b372427654873fbd58a5d2d4d733e91b","modified":1491187935452},{"_id":"public/archives/page/6/index.html","hash":"5730a40881dfbe308749696a14e07845a9f7c2c4","modified":1491187935452},{"_id":"public/archives/2017/index.html","hash":"7f2c7309b44cc549455f861adfd583b3afd2e9a0","modified":1491187935452},{"_id":"public/archives/2017/page/2/index.html","hash":"3262b0cdc0169147ace26e2d008380171176f99b","modified":1491187935452},{"_id":"public/archives/2017/page/3/index.html","hash":"ffce1e6d2540f57bf27c0b00904b1bbae31d5e19","modified":1491187935452},{"_id":"public/archives/2017/page/4/index.html","hash":"2e6f762ff3d638e9a83736df369c15fc1336b626","modified":1491187935452},{"_id":"public/archives/2017/page/5/index.html","hash":"97625ac6f4d749360e6d9f1b59f4e1038a54a7db","modified":1491187935452},{"_id":"public/archives/2017/page/6/index.html","hash":"697c7b39d1f7872af6ac85dd3d056a0a70cd37a5","modified":1491187935453},{"_id":"public/archives/2017/01/index.html","hash":"38d5bba724349cdfe7bba10f596cc7a80943e452","modified":1491187935453},{"_id":"public/archives/2017/01/page/2/index.html","hash":"e5dd7d1cb829d7840453376e65acd64334c091bc","modified":1491187935453},{"_id":"public/archives/2017/01/page/3/index.html","hash":"51bdb4ac9dc2a6120d508340c23bc7f2f7be59c0","modified":1491187935453},{"_id":"public/archives/2017/01/page/4/index.html","hash":"4ea5d3a8f03003aea32dc561d45d9d03faa6dda6","modified":1491187935453},{"_id":"public/archives/2017/01/page/5/index.html","hash":"7edeb7012ecc92652549c775d8911daef25accd4","modified":1491187935453},{"_id":"public/archives/2017/01/page/6/index.html","hash":"eee43e26ac793f74b6f47663a1a7ff1dc2bbafc0","modified":1491187935453},{"_id":"public/categories/python/index.html","hash":"2c77b91863f8ff8873225d66396161105cc8d23d","modified":1491187935449},{"_id":"public/categories/paas/index.html","hash":"136946c01dd575c2b8e7d705cea361e293ae5ba3","modified":1491187935449},{"_id":"public/categories/iaas/index.html","hash":"0302d0f61c45a61a953a97cc6ea76a81fceb51ed","modified":1491187935450},{"_id":"public/categories/java/index.html","hash":"fc5d8faeeacff25eec94bc48d641cbbc0f842271","modified":1491187935450},{"_id":"public/categories/docker/index.html","hash":"c1820a2e5ea625de6b1bbaaa23530dc02a1b02a1","modified":1491187935450},{"_id":"public/categories/docker/page/2/index.html","hash":"6e92d7d5ef3a58b1219387cfa5b8d422b99cdb29","modified":1491187935450},{"_id":"public/categories/docker/page/3/index.html","hash":"1333495bf6514402487849f3c6320f0636d30f85","modified":1491187935450},{"_id":"public/categories/devops/index.html","hash":"03bf96f8fb65fc196f41c5ea6be3090104187ba0","modified":1491187935450},{"_id":"public/categories/tmp/index.html","hash":"7c0e4d03f1ca1a46509c354057ae1e778b8db22c","modified":1491187935450},{"_id":"public/categories/cloud/index.html","hash":"b42edcc29a01a93522e6a46a806096abc76c170e","modified":1491187935450},{"_id":"public/categories/cx/index.html","hash":"376b60cddca96c7378075278ab7a3fac7e7fd830","modified":1491187935450},{"_id":"public/categories/nodejs/index.html","hash":"c6a4847ffc353c3cac7e5f5ce23203ee6d8db8d2","modified":1491187935450},{"_id":"public/categories/js/index.html","hash":"243078974040e61221ec5df09bb59b91c271ae04","modified":1491187935450},{"_id":"public/categories/microservice/index.html","hash":"8ef10668088dc9c7500133cc362e6b64b497dab6","modified":1491187935450},{"_id":"public/categories/ruby/index.html","hash":"d7185f50146b935f1ba3812ad257cc80980720c5","modified":1491187935450},{"_id":"public/categories/network/index.html","hash":"6b1ac1ec583ff3e57c2a3e8d3596e42d53d0945f","modified":1491187935451},{"_id":"public/categories/linux/index.html","hash":"45b7ecc07761d13dcb43c94d0e0503c331e7d09b","modified":1491187935451},{"_id":"public/tags/portal/index.html","hash":"b49beb8bc8d086c975ccb96914270754fde054a8","modified":1491187935454},{"_id":"public/tags/portal/page/2/index.html","hash":"68ff2826768c938a19b4c9471e5a43fb7f6f37cf","modified":1491187935454},{"_id":"public/tags/portal/page/3/index.html","hash":"af209708d8f0be25919bb99d268d03020577112a","modified":1491187935454},{"_id":"public/tags/core/index.html","hash":"8e32692ca7bab4fd24a7d0bccf9dc9f0c0ca0fe4","modified":1491187935454},{"_id":"public/tags/core/page/2/index.html","hash":"d4ece235a3139cbc24a392a3c1dbf06410450e97","modified":1491187935454},{"_id":"public/tags/cloudfoundry/index.html","hash":"76e509e9fc7239a61617d083621b0f6aad3892f5","modified":1491187935455},{"_id":"public/tags/openshift/index.html","hash":"7b985a15ac52cc5fec81b521257978745aaa083c","modified":1491187935455},{"_id":"public/tags/openstack/index.html","hash":"fed8fba0cf47c08b676f5a2158d5de2e41f0d221","modified":1491187935455},{"_id":"public/tags/detail/index.html","hash":"8a352f02bb9746a630de724b9dd4e67b6c7488c8","modified":1491187935455},{"_id":"public/tags/cookbook/index.html","hash":"e4a27af599b54603ee78349755a8fffb9f827feb","modified":1491187935455},{"_id":"public/tags/filesystem/index.html","hash":"a425c417cccc7dcf5f1b8b2cea6ef650930a4182","modified":1491187935456},{"_id":"public/tags/security/index.html","hash":"4dcb92439893b00cae987da2d2c5857fe6b52288","modified":1491187935456},{"_id":"public/tags/network/index.html","hash":"b760e3b259c081d3ad762f8471ea8add744ce466","modified":1491187935456},{"_id":"public/tags/storage/index.html","hash":"4d1d311432b87dbc082068a68ed4b9e0c55c4c81","modified":1491187935456},{"_id":"public/tags/swarm/index.html","hash":"1a53eea2c1bdb62214ebe9698e9b8d9b7dfe81b0","modified":1491187935456},{"_id":"public/tags/architect/index.html","hash":"cb126c23eba91b06f286128150175c2b7e30e99c","modified":1491187935456},{"_id":"public/tags/django/index.html","hash":"28f24e44505bcb16d2ce69075fde6cd0f4c31d3e","modified":1491187935456},{"_id":"public/tags/install/index.html","hash":"42a7b04be9db7ab1e0682ae9a083505e6bac38dd","modified":1491187935456},{"_id":"public/tags/python/index.html","hash":"7b99932b956132535b0c48d783d9897d80c0ebcc","modified":1491187935456},{"_id":"public/tags/docker/index.html","hash":"65c5884dd93a17918fea5ee69c80b9f0ae7d8429","modified":1491187935456},{"_id":"public/tags/rails/index.html","hash":"3ca884ac847a0466a92a6b7eb6477e4e7b34e1ce","modified":1491187935456},{"_id":"public/tags/http/index.html","hash":"6672fd7896312f49beb7c07e03f76ea30a222b1c","modified":1491187935456},{"_id":"public/uncategorized/docker-compose-file-tmp/index.html","hash":"f080fce53c51be95a76f2d980d02f55e8d9150d8","modified":1491187935457},{"_id":"public/docker/docker--portal/index.html","hash":"60eafd53440e599c82e6825bae5c8814e0024585","modified":1491187935457},{"_id":"public/index.html","hash":"ecc4634b7468ad0e6f18395ab4d8eb3157d3b634","modified":1491187935457},{"_id":"public/page/2/index.html","hash":"0743cf90872ab392d1e3279fe3e2c7c07e005041","modified":1491187935460},{"_id":"public/page/3/index.html","hash":"32ec43bc426bc7d2cd2a6bf6013ffc82745f7963","modified":1491187935460},{"_id":"public/page/4/index.html","hash":"1ef9df3928bcd1609827fe210e6dd145a21d8718","modified":1491187935460},{"_id":"public/page/5/index.html","hash":"24486327b109c6e7d510094e15cef05de2a6bd34","modified":1491187935461},{"_id":"public/page/6/index.html","hash":"414c0d093b26ca1f95162f84784d62ca846e6f60","modified":1491187935461},{"_id":"public/fancybox/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1491187935492},{"_id":"public/fancybox/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1491187935492},{"_id":"public/fancybox/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1491187935492},{"_id":"public/fancybox/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1491187935492},{"_id":"public/fancybox/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1491187935493},{"_id":"public/fancybox/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1491187935493},{"_id":"public/css/images/banner.jpg","hash":"e2c9ff91ca7c221c23e41dba0d4b8dfd90d28a6c","modified":1491187935493},{"_id":"public/css/fonts/FontAwesome.otf","hash":"b5b4f9be85f91f10799e87a083da1d050f842734","modified":1491187935493},{"_id":"public/css/fonts/fontawesome-webfont.woff","hash":"04c3bf56d87a0828935bd6b4aee859995f321693","modified":1491187935493},{"_id":"public/fancybox/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1491187935493},{"_id":"public/css/fonts/fontawesome-webfont.eot","hash":"7619748fe34c64fb157a57f6d4ef3678f63a8f5e","modified":1491187935493},{"_id":"public/css/fonts/fontawesome-webfont.ttf","hash":"7f09c97f333917034ad08fa7295e916c9f72fd3f","modified":1491187937770},{"_id":"public/fancybox/jquery.fancybox.css","hash":"aaa582fb9eb4b7092dc69fcb2d5b1c20cca58ab6","modified":1491187937782},{"_id":"public/js/script.js","hash":"2876e0b19ce557fca38d7c6f49ca55922ab666a1","modified":1491187937782},{"_id":"public/fancybox/helpers/jquery.fancybox-media.js","hash":"294420f9ff20f4e3584d212b0c262a00a96ecdb3","modified":1491187937782},{"_id":"public/fancybox/helpers/jquery.fancybox-buttons.js","hash":"dc3645529a4bf72983a39fa34c1eb9146e082019","modified":1491187937782},{"_id":"public/fancybox/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1491187937782},{"_id":"public/fancybox/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1491187937782},{"_id":"public/fancybox/helpers/jquery.fancybox-thumbs.js","hash":"47da1ae5401c24b5c17cc18e2730780f5c1a7a0c","modified":1491187937782},{"_id":"public/css/style.css","hash":"f1bdee26c06f8dca10fa0df089e978e0469d755a","modified":1491187937783},{"_id":"public/fancybox/jquery.fancybox.pack.js","hash":"9e0d51ca1dbe66f6c0c7aefd552dc8122e694a6e","modified":1491187937783},{"_id":"public/fancybox/jquery.fancybox.js","hash":"d08b03a42d5c4ba456ef8ba33116fdbb7a9cabed","modified":1491187937783},{"_id":"public/css/images/banner-bak.jpg","hash":"f44aa591089fcb3ec79770a1e102fd3289a7c6a6","modified":1491187937800},{"_id":"public/css/fonts/fontawesome-webfont.svg","hash":"46fcc0194d75a0ddac0a038aee41b23456784814","modified":1491187937800},{"_id":"source/_posts/blockchain-portal.md","hash":"7c9544e68778603f0ed6720858a96735e552d271","modified":1491187563000},{"_id":"source/_posts/community.md","hash":"9e11517bac0bac1ba12423dcb868ecf3961735c3","modified":1488164080000},{"_id":"source/_posts/docker-k8s-install.md","hash":"5f3a87642cc04d6429da1161f5eaed2b161030f8","modified":1487414202000},{"_id":"source/_posts/lang-go-portal.md","hash":"c329396b8934a30b9da7d4aa2430107d94840e10","modified":1488129065000},{"_id":"source/_posts/docker-mesos.md","hash":"859394563adc3ede668fa8e47d0c826e4b249199","modified":1484486949000},{"_id":"source/_posts/lang-php-portal.md","hash":"7a389e7b2b6ec5d82a43e2e94082ab1beef0ff33","modified":1488127181000},{"_id":"public/blockchain/blockchain-portal/index.html","hash":"4797551d3fdb4ec460a74e540b5b15376acb90c3","modified":1491187935437},{"_id":"public/php/lang-php-portal/index.html","hash":"f44faba97beb8a65ad7558f4d8525fa82dcbc9c2","modified":1491187935447},{"_id":"public/career/jd--portal/index.html","hash":"69d68fe43e8d5ab6a119569a233bffa0a28ac910","modified":1491187935447},{"_id":"public/community/community/index.html","hash":"f9d46c5c66cf8c499b454013a32523a382bf0b32","modified":1491187935449},{"_id":"public/docker/docker-k8s-install/index.html","hash":"e1ec112c9cdfb91f88bc18f4fbce96b3fcc1f965","modified":1491187935449},{"_id":"public/cx/lang-go-portal/index.html","hash":"8f2fa9cc20919a5482844981a77633cb620e57dd","modified":1491187935449},{"_id":"public/categories/blockchain/index.html","hash":"cdb38d98cdc4c63c3ccc69a56a75d5d8c4517eec","modified":1491187935451},{"_id":"public/categories/community/index.html","hash":"6473c5f5736ec103c7c4a9d5093ef13c13942b0b","modified":1491187935451},{"_id":"public/categories/php/index.html","hash":"25e4e522892ca28d1e08cd28132171b1b4a8050a","modified":1491187935451},{"_id":"public/categories/career/index.html","hash":"076029b39348536c6eb70ed635d8870e447e1126","modified":1491187935451},{"_id":"public/archives/page/7/index.html","hash":"1372dcacc610b372a150b6d4c66d7d45a6ee11a6","modified":1491187935452},{"_id":"public/archives/2017/page/7/index.html","hash":"b0b524e17f58c011ecaaa644380fdccc802fae20","modified":1491187935453},{"_id":"public/archives/2017/01/page/7/index.html","hash":"244948a9dca37b41fb98aa800104fa88d4e8fe21","modified":1491187935453},{"_id":"public/archives/2017/04/index.html","hash":"fb3bb898523c091b47ac7021019f218b49a9a5df","modified":1491187935453},{"_id":"public/tags/core/page/3/index.html","hash":"5834a80acac10d4d3a93120cf6397b36d382d26c","modified":1491187935455},{"_id":"public/tags/blockchain/index.html","hash":"916b31e58b28ad398e54ce294a2d0eb92cbc25db","modified":1491187935456},{"_id":"public/tags/opensource/index.html","hash":"4336aed0b7166aad47f05583ca12286c67037fe4","modified":1491187935457},{"_id":"public/docker/docker-mesos/index.html","hash":"56c2261dcd424259d88be8e940e573c8624ac614","modified":1491187935457},{"_id":"public/page/7/index.html","hash":"ab5bf02e508ab287ab7c639ee14fab48ae960425","modified":1491187935461}],"Category":[{"name":"python","_id":"ciypwm36s000321sv2ayq74mg"},{"name":"paas","_id":"ciypwm37q000n21sv6dlm59y0"},{"name":"iaas","_id":"ciypwm37x000v21svrncf6pj1"},{"name":"java","_id":"ciypwm38k001h21svxlgxac0o"},{"name":"docker","_id":"ciypwm396002421svh3qvjzup"},{"name":"devops","_id":"ciypwm3am003e21svty5ib55t"},{"name":"tmp","_id":"ciypwm3b3004121svtkx85z4d"},{"name":"cloud","_id":"ciypwm3b9004921svnu7nqm17"},{"name":"cx","_id":"ciypwm3bj004q21sve4wap485"},{"name":"nodejs","_id":"ciypwm3c0005621svwg9s5r8n"},{"name":"js","_id":"ciypwm3c6005e21sv23z2ug6y"},{"name":"microservice","_id":"ciypwm3cd005m21sva7pdlkq9"},{"name":"ruby","_id":"ciypwm3ce005u21sv403nox6t"},{"name":"network","_id":"ciypwm3cg006021svbgr4bjye"},{"name":"linux","_id":"ciypwm3ci006c21svajl2sgwk"},{"name":"blockchain","_id":"cj11isjlr0002bcsv83i8b4xi"},{"name":"community","_id":"cj11isjnu0008bcsv1k7za5xf"},{"name":"php","_id":"cj11isjo3000fbcsvru41hd89"},{"name":"career","_id":"cj11isjrj000pbcsvkw8id6en"}],"Data":[],"Page":[{"title":"about","date":"2017-01-12T17:49:37.000Z","_content":"\n123","source":"about/index.md","raw":"---\ntitle: about\ndate: 2017-01-12 17:49:37\n---\n\n123","updated":"2017-01-12T09:50:01.000Z","path":"about/index.html","comments":1,"layout":"page","_id":"ciypwm35q000021sv7ev9hb2x","content":"<p>123</p>\n","excerpt":"","more":"<p>123</p>\n"}],"Post":[{"title":"python portal","_content":"\n# tensorflow\nhttps://www.tensorflow.org/\nhttp://www.tensorfly.cn/\n\n# kylin\nhttp://www.infoq.com/cn/dsghzfdhs\nhttp://kylin.apache.org/cn/\nhttps://www.amazon.cn/%E5%9F%BA%E4%BA%8EApache-Kylin%E6%9E%84%E5%BB%BA%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B9%B3%E5%8F%B0-%E8%92%8B%E5%AE%88%E5%A3%AE/dp/B01N9GO1XC/ref=cm_cr_othr_d_product_top?ie=UTF8\n","source":"_posts/bigdata--portal.md","raw":"---\ntitle: python portal\ncategories:\n- python\ntags:\n- portal\n---\n\n# tensorflow\nhttps://www.tensorflow.org/\nhttp://www.tensorfly.cn/\n\n# kylin\nhttp://www.infoq.com/cn/dsghzfdhs\nhttp://kylin.apache.org/cn/\nhttps://www.amazon.cn/%E5%9F%BA%E4%BA%8EApache-Kylin%E6%9E%84%E5%BB%BA%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B9%B3%E5%8F%B0-%E8%92%8B%E5%AE%88%E5%A3%AE/dp/B01N9GO1XC/ref=cm_cr_othr_d_product_top?ie=UTF8\n","slug":"bigdata--portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-02-09T12:47:15.000Z","_id":"ciypwm36l000121sv4vi8ieps","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"tensorflow\"><a href=\"#tensorflow\" class=\"headerlink\" title=\"tensorflow\"></a>tensorflow</h1><p><a href=\"https://www.tensorflow.org/\" target=\"_blank\" rel=\"external\">https://www.tensorflow.org/</a><br><a href=\"http://www.tensorfly.cn/\" target=\"_blank\" rel=\"external\">http://www.tensorfly.cn/</a></p>\n<h1 id=\"kylin\"><a href=\"#kylin\" class=\"headerlink\" title=\"kylin\"></a>kylin</h1><p><a href=\"http://www.infoq.com/cn/dsghzfdhs\" target=\"_blank\" rel=\"external\">http://www.infoq.com/cn/dsghzfdhs</a><br><a href=\"http://kylin.apache.org/cn/\" target=\"_blank\" rel=\"external\">http://kylin.apache.org/cn/</a><br><a href=\"https://www.amazon.cn/%E5%9F%BA%E4%BA%8EApache-Kylin%E6%9E%84%E5%BB%BA%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B9%B3%E5%8F%B0-%E8%92%8B%E5%AE%88%E5%A3%AE/dp/B01N9GO1XC/ref=cm_cr_othr_d_product_top?ie=UTF8\" target=\"_blank\" rel=\"external\">https://www.amazon.cn/%E5%9F%BA%E4%BA%8EApache-Kylin%E6%9E%84%E5%BB%BA%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B9%B3%E5%8F%B0-%E8%92%8B%E5%AE%88%E5%A3%AE/dp/B01N9GO1XC/ref=cm_cr_othr_d_product_top?ie=UTF8</a></p>\n","excerpt":"","more":"<h1 id=\"tensorflow\"><a href=\"#tensorflow\" class=\"headerlink\" title=\"tensorflow\"></a>tensorflow</h1><p><a href=\"https://www.tensorflow.org/\">https://www.tensorflow.org/</a><br><a href=\"http://www.tensorfly.cn/\">http://www.tensorfly.cn/</a></p>\n<h1 id=\"kylin\"><a href=\"#kylin\" class=\"headerlink\" title=\"kylin\"></a>kylin</h1><p><a href=\"http://www.infoq.com/cn/dsghzfdhs\">http://www.infoq.com/cn/dsghzfdhs</a><br><a href=\"http://kylin.apache.org/cn/\">http://kylin.apache.org/cn/</a><br><a href=\"https://www.amazon.cn/%E5%9F%BA%E4%BA%8EApache-Kylin%E6%9E%84%E5%BB%BA%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B9%B3%E5%8F%B0-%E8%92%8B%E5%AE%88%E5%A3%AE/dp/B01N9GO1XC/ref=cm_cr_othr_d_product_top?ie=UTF8\">https://www.amazon.cn/%E5%9F%BA%E4%BA%8EApache-Kylin%E6%9E%84%E5%BB%BA%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B9%B3%E5%8F%B0-%E8%92%8B%E5%AE%88%E5%A3%AE/dp/B01N9GO1XC/ref=cm_cr_othr_d_product_top?ie=UTF8</a></p>\n"},{"title":"python portal","_content":"\n# 123","source":"_posts/bigdata-hadoop-portal.md","raw":"---\ntitle: python portal\ncategories:\n- python\ntags:\n- portal\n---\n\n# 123","slug":"bigdata-hadoop-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:35:54.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm36p000221svq1phbbk0","content":"<h1 id=\"123\"><a href=\"#123\" class=\"headerlink\" title=\"123\"></a>123</h1>","excerpt":"","more":"<h1 id=\"123\"><a href=\"#123\" class=\"headerlink\" title=\"123\"></a>123</h1>"},{"title":"cloud portal","_content":"\n# aws\nhttps://github.com/donnemartin/saws\n\n# top resource\ninfoq: http://2017.qconbeijing.com/tracks\n\nhttps://www.oschina.net/p/dubbo\nhttps://github.com/alibaba/dubbo\n\n\n\n## zookeeper\n\n- about\n\n        ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services. All of these kinds of services are used in some form or another by distributed applications. Each time they are implemented there is a lot of work that goes into fixing the bugs and race conditions that are inevitable. Because of the difficulty of implementing these kinds of services, applications initially usually skimp on them ,which make them brittle in the presence of change and difficult to manage. Even when done correctly, different implementations of these services lead to management complexity when the applications are deployed.\n\n        考虑一下有多个服务器的分布式系统，每台服务器都负责保存数据，在数据上执行操作。这样的潜在例子包括分布式搜索引擎、分布式构建系统或者已知的系统如Apache Hadoop。所有这些分布式系统的一个常见问题是，你如何在任一时间点确定哪些服务器活着并且在工作中。最重要的是，当面对这些分布式计算的难题，例如网络失败、带宽限制、可变延迟连接、安全问题以及任何网络环境，甚至跨多个数据中心时可能发生的错误时，你如何可靠地做这些事。这些正是Apache ZooKeeper所关注的问题，它是一个快速、高可用、容错、分布式的协调服务。你可以使用ZooKeeper构建可靠的、分布式的数据结构，用于群组成员、领导人选举、协同工作流和配置服务，以及广义的分布式数据结构如锁、队列、屏障（Barrier）和锁存器（Latch）。许多知名且成功的项目依赖于ZooKeeper，其中包括HBase、Hadoop 2.0、Solr Cloud、Neo4J、Apache Blur（Incubating）和Accumulo。\n\n        ZooKeeper是一个分布式的、分层级的文件系统，能促进客户端间的松耦合，并提供最终一致的，类似于传统文件系统中文件和目录的Znode视图。它提供了基本的操作，例如创建、删除和检查Znode是否存在。它提供了事件驱动模型，客户端能观察特定Znode的变化，例如现有Znode增加了一个新的子节点。ZooKeeper运行多个ZooKeeper服务器，称为Ensemble，以获得高可用性。每个服务器都持有分布式文件系统的内存复本，为客户端的读取请求提供服务。\n\n- usage\n\n        所有分布式的协商和一致都可以利用zk实现。可以理解为一个分布式的带有订阅功能的小型元数据库。\n        使用方式自然就是通过上面这句话而定。比如你需要一个订阅功能的数据库，发布你的信息给其他客户端，享受订阅功能。又比如你有很多信息是由很多客户端同时竞争写入zk,但只允许第一个到达的写入，就可以享受zk的一致性。\n\n        现在流行的分布式系统已经完全无法脱离zookeeper。列举几个简单的例子：\n        storm中用zookeeper来协调同步集群中机器的状态（并不传递消息）。基本不会有负载，对机器性能没什么要求。jstorm也是用zk来做一致性服务。\n\n        dubbo中采用zookeeper来作注册中心，在阿里内部采用的是基于数据库的注册中心，我们自己用dubbo的时候采用了zookeeper，配置起来非常方便。当然如果采用redis或者自己写个简单的基于内存的注册中心也是可以的。从稳定性来讲，个人觉得zookeeper是比较好的选择，毕竟zookeeper集群中的机器只要不是半数以上宕掉，服务就是可用的。dubbo服务的消费者和提供者都需要用到zookeeper，然而消费者和提供者之间的长连接建立后，zookeeper参与程度就比较弱了(仅需要接受一些心跳包)，除非此时有新的提供者消费者加入或者离开(需要更新节点数据)。因此负载也是非常低的，基本不用考虑性能问题。zookeeper还同时起到了监控各个服务的作用。\n\n        HBase要用到zk，这个已经有答主提到，就不用细说了。Hadoop也是需要用到zookeeper的，用来管理集群中的NameNode。差点忘了，Kafka集群依赖于ZooKeeper。kafka通过zookeeper实现生产者在消费端的负载均衡，动态的集群扩展等等。\n\n\nresource:\n    http://zookeeper.apache.org/\n    http://www.ibm.com/developerworks/cn/opensource/os-cn-zookeeper/\n\n# distributed messaging\n\n## kafka\n\nabout\n\n    Apache Kafka是分布式发布-订阅消息系统。它最初由LinkedIn公司开发，之后成为Apache项目的一部分。Kafka是一种快速、可扩展的、设计内在就是分布式的，分区的和可复制的提交日志服务。Kafka是linkedin开源的MQ系统，主要特点是基于Pull的模式来处理消息消费，追求高吞吐量，一开始的目的就是用于日志收集和传输，0.8开始支持复制，不支持事务，适合产生大量数据的互联网服务的数据收集业务。\n\n    Apache Kafka与传统消息系统相比，有以下不同：\n    它被设计为一个分布式系统，易于向外扩展；\n    它同时为发布和订阅提供高吞吐量；\n    它支持多订阅者，当失败时能自动平衡消费者；\n    它将消息持久化到磁盘，因此可用于批量消费，例如ETL，以及实时应用程序。\n\n    Kafka 使用自己的协议。Kafka 自身服务和消费者都需要依赖 Zookeeper。RabbitMQ 在有大量消息堆积的情况下性能会下降，Kafka不会。毕竟AMQP设计的初衷不是用来持久化海量消息的，而Kafka一开始是用来处理海量日志的。\n\n\nresource：\n    http://kafka.apache.org/\n    https://github.com/apache/kafka\n\n    http://www.infoq.com/cn/articles/apache-kafka\n    http://www.infoq.com/cn/presentations/use-apache-kafka-to-transfer-key-business-message?utm_source=infoq&utm_medium=related_content_link&utm_campaign=relatedContent_articles_clk\n\n    must read: \n        https://github.com/oldratlee/translations/blob/master/log-what-every-software-engineer-should-know-about-real-time-datas-unifying/README.md\n        http://www.cnblogs.com/foreach-break/p/notes_about_distributed_system_and_The_log.html\n        \n    faq：https://www.zhihu.com/question/22480085\n\n## rabbitmq\nRabbitMQ 支持 AMQP（二进制），STOMP（文本），MQTT（二进制），HTTP（里面包装其他协议）等协议。\n\n## rocketmq\nabout\n    RocketMQ 是一款分布式、队列模型的消息中间件，具有以下特点：\n\n    能够保证严格的消息顺序\n    提供丰富的消息拉取模式\n    高效的订阅者水平扩展能力\n    实时的消息订阅机制\n    亿级消息堆积能力\n    Metaq3.0 版本改名，产品名称改为RocketMQ\n\n    RMQ的结构分为四个部分：生产者、消费者、nameserver、brokerserver\n    nameserver：nameserver接收broker的请求注册broker路由信息。收client的请求根据某个topic获取所有到broker的路由信息。\n    brokerserver：消息的接收和推送，\n    生产者：发送消息，将消息推送给brokerserver。\n    消费者：接收消息，从brokerserver上获取消息。\n\nresource:\n    https://www.aliyun.com/product/ons\n    https://github.com/apache/incubator-rocketmq\n    https://rocketmq.incubator.apache.org/\n    http://www.jianshu.com/p/453c6e7ff81c\n    http://www.jialeens.com/archives/681.html?utm_source=tuicool&utm_medium=referral\n\n## zeromq\nZeroMQ 和 RabbitMQ/Kafka 不同，它只是一个异步消息库，在套接字的基础上提供了类似于消息代理的机制。使用 ZeroMQ 的话，需要对自己的业务代码进行改造，不利于服务解耦。\n\n\n# distribute kv\n\n## etcd\nabout\n        etcd 是一个应用在分布式环境下的 key/value 存储服务。利用 etcd 的特性，应用程序可以在集群中共享信息、配置或作服务发现，etcd 会在集群的各个节点中复制这些数据并保证这些数据始终正确。etcd 无论是在 CoreOS 还是 Kubernetes 体系中都是不可或缺的一环。\n\n        2. 规范词汇表\n        etcd 0.5.0 版首次对 etcd 代码、文档及 CLI 中使用的术语进行了定义。\n\n        2.1. node\n\n        node 指一个 raft 状态机实例。每个 node 都具有唯一的标识，并在处于 leader 状态时记录其它节点的步进数。\n\n        2.2. member\n\n        member 指一个 etcd 实例。member 运行在每个 node 上，并向这一 node 上的其它应用程序提供服务。\n\n        2.3. Cluster\n\n        Cluster 由多个 member 组成。每个 member 中的 node 遵循 raft 共识协议来复制日志。Cluster 接收来自 member 的提案消息，将其提交并存储于本地磁盘。\n\n        2.4. Peer\n\n        同一 Cluster 中的其它 member。\n\n        2.5. Client\n\n        Client 指调用 Cluster API 的对象。\n\n        3. Raft 共识算法\n        etcd 集群的工作原理基于 raft 共识算法 (The Raft Consensus Algorithm)。etcd 在 0.5.0 版本中重新实现了 raft 算法，而非像之前那样依赖于第三方库 go-raft 。raft 共识算法的优点在于可以在高效的解决分布式系统中各个节点日志内容一致性问题的同时，也使得集群具备一定的容错能力。即使集群中出现部分节点故障、网络故障等问题，仍可保证其余大多数节点正确的步进。甚至当更多的节点（一般来说超过集群节点总数的一半）出现故障而导致集群不可用时，依然可以保证节点中的数据不会出现错误的结果。\n\n\nhttps://github.com/coreos/etcd\nhttp://www.infoq.com/cn/articles/coreos-analyse-etcd/\n\n\n# IOE resource\n\nIBM doc:\nhttp://www.ibm.com/analytics/cn/zh/\n\nOracle doc: http://docs.oracle.com/en/\nSAP doc: http://www.sap.com/developer.html\nSAP case: http://www.bestsapchina.com/ResourceCenter/i-t-s.html","source":"_posts/cloud--portal.md","raw":"---\ntitle: cloud portal\ncategories:\n- python\ntags:\n- portal\n---\n\n# aws\nhttps://github.com/donnemartin/saws\n\n# top resource\ninfoq: http://2017.qconbeijing.com/tracks\n\nhttps://www.oschina.net/p/dubbo\nhttps://github.com/alibaba/dubbo\n\n\n\n## zookeeper\n\n- about\n\n        ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services. All of these kinds of services are used in some form or another by distributed applications. Each time they are implemented there is a lot of work that goes into fixing the bugs and race conditions that are inevitable. Because of the difficulty of implementing these kinds of services, applications initially usually skimp on them ,which make them brittle in the presence of change and difficult to manage. Even when done correctly, different implementations of these services lead to management complexity when the applications are deployed.\n\n        考虑一下有多个服务器的分布式系统，每台服务器都负责保存数据，在数据上执行操作。这样的潜在例子包括分布式搜索引擎、分布式构建系统或者已知的系统如Apache Hadoop。所有这些分布式系统的一个常见问题是，你如何在任一时间点确定哪些服务器活着并且在工作中。最重要的是，当面对这些分布式计算的难题，例如网络失败、带宽限制、可变延迟连接、安全问题以及任何网络环境，甚至跨多个数据中心时可能发生的错误时，你如何可靠地做这些事。这些正是Apache ZooKeeper所关注的问题，它是一个快速、高可用、容错、分布式的协调服务。你可以使用ZooKeeper构建可靠的、分布式的数据结构，用于群组成员、领导人选举、协同工作流和配置服务，以及广义的分布式数据结构如锁、队列、屏障（Barrier）和锁存器（Latch）。许多知名且成功的项目依赖于ZooKeeper，其中包括HBase、Hadoop 2.0、Solr Cloud、Neo4J、Apache Blur（Incubating）和Accumulo。\n\n        ZooKeeper是一个分布式的、分层级的文件系统，能促进客户端间的松耦合，并提供最终一致的，类似于传统文件系统中文件和目录的Znode视图。它提供了基本的操作，例如创建、删除和检查Znode是否存在。它提供了事件驱动模型，客户端能观察特定Znode的变化，例如现有Znode增加了一个新的子节点。ZooKeeper运行多个ZooKeeper服务器，称为Ensemble，以获得高可用性。每个服务器都持有分布式文件系统的内存复本，为客户端的读取请求提供服务。\n\n- usage\n\n        所有分布式的协商和一致都可以利用zk实现。可以理解为一个分布式的带有订阅功能的小型元数据库。\n        使用方式自然就是通过上面这句话而定。比如你需要一个订阅功能的数据库，发布你的信息给其他客户端，享受订阅功能。又比如你有很多信息是由很多客户端同时竞争写入zk,但只允许第一个到达的写入，就可以享受zk的一致性。\n\n        现在流行的分布式系统已经完全无法脱离zookeeper。列举几个简单的例子：\n        storm中用zookeeper来协调同步集群中机器的状态（并不传递消息）。基本不会有负载，对机器性能没什么要求。jstorm也是用zk来做一致性服务。\n\n        dubbo中采用zookeeper来作注册中心，在阿里内部采用的是基于数据库的注册中心，我们自己用dubbo的时候采用了zookeeper，配置起来非常方便。当然如果采用redis或者自己写个简单的基于内存的注册中心也是可以的。从稳定性来讲，个人觉得zookeeper是比较好的选择，毕竟zookeeper集群中的机器只要不是半数以上宕掉，服务就是可用的。dubbo服务的消费者和提供者都需要用到zookeeper，然而消费者和提供者之间的长连接建立后，zookeeper参与程度就比较弱了(仅需要接受一些心跳包)，除非此时有新的提供者消费者加入或者离开(需要更新节点数据)。因此负载也是非常低的，基本不用考虑性能问题。zookeeper还同时起到了监控各个服务的作用。\n\n        HBase要用到zk，这个已经有答主提到，就不用细说了。Hadoop也是需要用到zookeeper的，用来管理集群中的NameNode。差点忘了，Kafka集群依赖于ZooKeeper。kafka通过zookeeper实现生产者在消费端的负载均衡，动态的集群扩展等等。\n\n\nresource:\n    http://zookeeper.apache.org/\n    http://www.ibm.com/developerworks/cn/opensource/os-cn-zookeeper/\n\n# distributed messaging\n\n## kafka\n\nabout\n\n    Apache Kafka是分布式发布-订阅消息系统。它最初由LinkedIn公司开发，之后成为Apache项目的一部分。Kafka是一种快速、可扩展的、设计内在就是分布式的，分区的和可复制的提交日志服务。Kafka是linkedin开源的MQ系统，主要特点是基于Pull的模式来处理消息消费，追求高吞吐量，一开始的目的就是用于日志收集和传输，0.8开始支持复制，不支持事务，适合产生大量数据的互联网服务的数据收集业务。\n\n    Apache Kafka与传统消息系统相比，有以下不同：\n    它被设计为一个分布式系统，易于向外扩展；\n    它同时为发布和订阅提供高吞吐量；\n    它支持多订阅者，当失败时能自动平衡消费者；\n    它将消息持久化到磁盘，因此可用于批量消费，例如ETL，以及实时应用程序。\n\n    Kafka 使用自己的协议。Kafka 自身服务和消费者都需要依赖 Zookeeper。RabbitMQ 在有大量消息堆积的情况下性能会下降，Kafka不会。毕竟AMQP设计的初衷不是用来持久化海量消息的，而Kafka一开始是用来处理海量日志的。\n\n\nresource：\n    http://kafka.apache.org/\n    https://github.com/apache/kafka\n\n    http://www.infoq.com/cn/articles/apache-kafka\n    http://www.infoq.com/cn/presentations/use-apache-kafka-to-transfer-key-business-message?utm_source=infoq&utm_medium=related_content_link&utm_campaign=relatedContent_articles_clk\n\n    must read: \n        https://github.com/oldratlee/translations/blob/master/log-what-every-software-engineer-should-know-about-real-time-datas-unifying/README.md\n        http://www.cnblogs.com/foreach-break/p/notes_about_distributed_system_and_The_log.html\n        \n    faq：https://www.zhihu.com/question/22480085\n\n## rabbitmq\nRabbitMQ 支持 AMQP（二进制），STOMP（文本），MQTT（二进制），HTTP（里面包装其他协议）等协议。\n\n## rocketmq\nabout\n    RocketMQ 是一款分布式、队列模型的消息中间件，具有以下特点：\n\n    能够保证严格的消息顺序\n    提供丰富的消息拉取模式\n    高效的订阅者水平扩展能力\n    实时的消息订阅机制\n    亿级消息堆积能力\n    Metaq3.0 版本改名，产品名称改为RocketMQ\n\n    RMQ的结构分为四个部分：生产者、消费者、nameserver、brokerserver\n    nameserver：nameserver接收broker的请求注册broker路由信息。收client的请求根据某个topic获取所有到broker的路由信息。\n    brokerserver：消息的接收和推送，\n    生产者：发送消息，将消息推送给brokerserver。\n    消费者：接收消息，从brokerserver上获取消息。\n\nresource:\n    https://www.aliyun.com/product/ons\n    https://github.com/apache/incubator-rocketmq\n    https://rocketmq.incubator.apache.org/\n    http://www.jianshu.com/p/453c6e7ff81c\n    http://www.jialeens.com/archives/681.html?utm_source=tuicool&utm_medium=referral\n\n## zeromq\nZeroMQ 和 RabbitMQ/Kafka 不同，它只是一个异步消息库，在套接字的基础上提供了类似于消息代理的机制。使用 ZeroMQ 的话，需要对自己的业务代码进行改造，不利于服务解耦。\n\n\n# distribute kv\n\n## etcd\nabout\n        etcd 是一个应用在分布式环境下的 key/value 存储服务。利用 etcd 的特性，应用程序可以在集群中共享信息、配置或作服务发现，etcd 会在集群的各个节点中复制这些数据并保证这些数据始终正确。etcd 无论是在 CoreOS 还是 Kubernetes 体系中都是不可或缺的一环。\n\n        2. 规范词汇表\n        etcd 0.5.0 版首次对 etcd 代码、文档及 CLI 中使用的术语进行了定义。\n\n        2.1. node\n\n        node 指一个 raft 状态机实例。每个 node 都具有唯一的标识，并在处于 leader 状态时记录其它节点的步进数。\n\n        2.2. member\n\n        member 指一个 etcd 实例。member 运行在每个 node 上，并向这一 node 上的其它应用程序提供服务。\n\n        2.3. Cluster\n\n        Cluster 由多个 member 组成。每个 member 中的 node 遵循 raft 共识协议来复制日志。Cluster 接收来自 member 的提案消息，将其提交并存储于本地磁盘。\n\n        2.4. Peer\n\n        同一 Cluster 中的其它 member。\n\n        2.5. Client\n\n        Client 指调用 Cluster API 的对象。\n\n        3. Raft 共识算法\n        etcd 集群的工作原理基于 raft 共识算法 (The Raft Consensus Algorithm)。etcd 在 0.5.0 版本中重新实现了 raft 算法，而非像之前那样依赖于第三方库 go-raft 。raft 共识算法的优点在于可以在高效的解决分布式系统中各个节点日志内容一致性问题的同时，也使得集群具备一定的容错能力。即使集群中出现部分节点故障、网络故障等问题，仍可保证其余大多数节点正确的步进。甚至当更多的节点（一般来说超过集群节点总数的一半）出现故障而导致集群不可用时，依然可以保证节点中的数据不会出现错误的结果。\n\n\nhttps://github.com/coreos/etcd\nhttp://www.infoq.com/cn/articles/coreos-analyse-etcd/\n\n\n# IOE resource\n\nIBM doc:\nhttp://www.ibm.com/analytics/cn/zh/\n\nOracle doc: http://docs.oracle.com/en/\nSAP doc: http://www.sap.com/developer.html\nSAP case: http://www.bestsapchina.com/ResourceCenter/i-t-s.html","slug":"cloud--portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-02-26T17:40:58.000Z","_id":"ciypwm36y000521svf4gipt4m","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"aws\"><a href=\"#aws\" class=\"headerlink\" title=\"aws\"></a>aws</h1><p><a href=\"https://github.com/donnemartin/saws\" target=\"_blank\" rel=\"external\">https://github.com/donnemartin/saws</a></p>\n<h1 id=\"top-resource\"><a href=\"#top-resource\" class=\"headerlink\" title=\"top resource\"></a>top resource</h1><p>infoq: <a href=\"http://2017.qconbeijing.com/tracks\" target=\"_blank\" rel=\"external\">http://2017.qconbeijing.com/tracks</a></p>\n<p><a href=\"https://www.oschina.net/p/dubbo\" target=\"_blank\" rel=\"external\">https://www.oschina.net/p/dubbo</a><br><a href=\"https://github.com/alibaba/dubbo\" target=\"_blank\" rel=\"external\">https://github.com/alibaba/dubbo</a></p>\n<h2 id=\"zookeeper\"><a href=\"#zookeeper\" class=\"headerlink\" title=\"zookeeper\"></a>zookeeper</h2><ul>\n<li><p>about</p>\n<pre><code>ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services. All of these kinds of services are used in some form or another by distributed applications. Each time they are implemented there is a lot of work that goes into fixing the bugs and race conditions that are inevitable. Because of the difficulty of implementing these kinds of services, applications initially usually skimp on them ,which make them brittle in the presence of change and difficult to manage. Even when done correctly, different implementations of these services lead to management complexity when the applications are deployed.\n\n考虑一下有多个服务器的分布式系统，每台服务器都负责保存数据，在数据上执行操作。这样的潜在例子包括分布式搜索引擎、分布式构建系统或者已知的系统如Apache Hadoop。所有这些分布式系统的一个常见问题是，你如何在任一时间点确定哪些服务器活着并且在工作中。最重要的是，当面对这些分布式计算的难题，例如网络失败、带宽限制、可变延迟连接、安全问题以及任何网络环境，甚至跨多个数据中心时可能发生的错误时，你如何可靠地做这些事。这些正是Apache ZooKeeper所关注的问题，它是一个快速、高可用、容错、分布式的协调服务。你可以使用ZooKeeper构建可靠的、分布式的数据结构，用于群组成员、领导人选举、协同工作流和配置服务，以及广义的分布式数据结构如锁、队列、屏障（Barrier）和锁存器（Latch）。许多知名且成功的项目依赖于ZooKeeper，其中包括HBase、Hadoop 2.0、Solr Cloud、Neo4J、Apache Blur（Incubating）和Accumulo。\n\nZooKeeper是一个分布式的、分层级的文件系统，能促进客户端间的松耦合，并提供最终一致的，类似于传统文件系统中文件和目录的Znode视图。它提供了基本的操作，例如创建、删除和检查Znode是否存在。它提供了事件驱动模型，客户端能观察特定Znode的变化，例如现有Znode增加了一个新的子节点。ZooKeeper运行多个ZooKeeper服务器，称为Ensemble，以获得高可用性。每个服务器都持有分布式文件系统的内存复本，为客户端的读取请求提供服务。\n</code></pre></li>\n<li><p>usage</p>\n<pre><code>所有分布式的协商和一致都可以利用zk实现。可以理解为一个分布式的带有订阅功能的小型元数据库。\n使用方式自然就是通过上面这句话而定。比如你需要一个订阅功能的数据库，发布你的信息给其他客户端，享受订阅功能。又比如你有很多信息是由很多客户端同时竞争写入zk,但只允许第一个到达的写入，就可以享受zk的一致性。\n\n现在流行的分布式系统已经完全无法脱离zookeeper。列举几个简单的例子：\nstorm中用zookeeper来协调同步集群中机器的状态（并不传递消息）。基本不会有负载，对机器性能没什么要求。jstorm也是用zk来做一致性服务。\n\ndubbo中采用zookeeper来作注册中心，在阿里内部采用的是基于数据库的注册中心，我们自己用dubbo的时候采用了zookeeper，配置起来非常方便。当然如果采用redis或者自己写个简单的基于内存的注册中心也是可以的。从稳定性来讲，个人觉得zookeeper是比较好的选择，毕竟zookeeper集群中的机器只要不是半数以上宕掉，服务就是可用的。dubbo服务的消费者和提供者都需要用到zookeeper，然而消费者和提供者之间的长连接建立后，zookeeper参与程度就比较弱了(仅需要接受一些心跳包)，除非此时有新的提供者消费者加入或者离开(需要更新节点数据)。因此负载也是非常低的，基本不用考虑性能问题。zookeeper还同时起到了监控各个服务的作用。\n\nHBase要用到zk，这个已经有答主提到，就不用细说了。Hadoop也是需要用到zookeeper的，用来管理集群中的NameNode。差点忘了，Kafka集群依赖于ZooKeeper。kafka通过zookeeper实现生产者在消费端的负载均衡，动态的集群扩展等等。\n</code></pre></li>\n</ul>\n<p>resource:<br>    <a href=\"http://zookeeper.apache.org/\" target=\"_blank\" rel=\"external\">http://zookeeper.apache.org/</a><br>    <a href=\"http://www.ibm.com/developerworks/cn/opensource/os-cn-zookeeper/\" target=\"_blank\" rel=\"external\">http://www.ibm.com/developerworks/cn/opensource/os-cn-zookeeper/</a></p>\n<h1 id=\"distributed-messaging\"><a href=\"#distributed-messaging\" class=\"headerlink\" title=\"distributed messaging\"></a>distributed messaging</h1><h2 id=\"kafka\"><a href=\"#kafka\" class=\"headerlink\" title=\"kafka\"></a>kafka</h2><p>about</p>\n<pre><code>Apache Kafka是分布式发布-订阅消息系统。它最初由LinkedIn公司开发，之后成为Apache项目的一部分。Kafka是一种快速、可扩展的、设计内在就是分布式的，分区的和可复制的提交日志服务。Kafka是linkedin开源的MQ系统，主要特点是基于Pull的模式来处理消息消费，追求高吞吐量，一开始的目的就是用于日志收集和传输，0.8开始支持复制，不支持事务，适合产生大量数据的互联网服务的数据收集业务。\n\nApache Kafka与传统消息系统相比，有以下不同：\n它被设计为一个分布式系统，易于向外扩展；\n它同时为发布和订阅提供高吞吐量；\n它支持多订阅者，当失败时能自动平衡消费者；\n它将消息持久化到磁盘，因此可用于批量消费，例如ETL，以及实时应用程序。\n\nKafka 使用自己的协议。Kafka 自身服务和消费者都需要依赖 Zookeeper。RabbitMQ 在有大量消息堆积的情况下性能会下降，Kafka不会。毕竟AMQP设计的初衷不是用来持久化海量消息的，而Kafka一开始是用来处理海量日志的。\n</code></pre><p>resource：<br>    <a href=\"http://kafka.apache.org/\" target=\"_blank\" rel=\"external\">http://kafka.apache.org/</a><br>    <a href=\"https://github.com/apache/kafka\" target=\"_blank\" rel=\"external\">https://github.com/apache/kafka</a></p>\n<pre><code>http://www.infoq.com/cn/articles/apache-kafka\nhttp://www.infoq.com/cn/presentations/use-apache-kafka-to-transfer-key-business-message?utm_source=infoq&amp;utm_medium=related_content_link&amp;utm_campaign=relatedContent_articles_clk\n\nmust read: \n    https://github.com/oldratlee/translations/blob/master/log-what-every-software-engineer-should-know-about-real-time-datas-unifying/README.md\n    http://www.cnblogs.com/foreach-break/p/notes_about_distributed_system_and_The_log.html\n\nfaq：https://www.zhihu.com/question/22480085\n</code></pre><h2 id=\"rabbitmq\"><a href=\"#rabbitmq\" class=\"headerlink\" title=\"rabbitmq\"></a>rabbitmq</h2><p>RabbitMQ 支持 AMQP（二进制），STOMP（文本），MQTT（二进制），HTTP（里面包装其他协议）等协议。</p>\n<h2 id=\"rocketmq\"><a href=\"#rocketmq\" class=\"headerlink\" title=\"rocketmq\"></a>rocketmq</h2><p>about<br>    RocketMQ 是一款分布式、队列模型的消息中间件，具有以下特点：</p>\n<pre><code>能够保证严格的消息顺序\n提供丰富的消息拉取模式\n高效的订阅者水平扩展能力\n实时的消息订阅机制\n亿级消息堆积能力\nMetaq3.0 版本改名，产品名称改为RocketMQ\n\nRMQ的结构分为四个部分：生产者、消费者、nameserver、brokerserver\nnameserver：nameserver接收broker的请求注册broker路由信息。收client的请求根据某个topic获取所有到broker的路由信息。\nbrokerserver：消息的接收和推送，\n生产者：发送消息，将消息推送给brokerserver。\n消费者：接收消息，从brokerserver上获取消息。\n</code></pre><p>resource:<br>    <a href=\"https://www.aliyun.com/product/ons\" target=\"_blank\" rel=\"external\">https://www.aliyun.com/product/ons</a><br>    <a href=\"https://github.com/apache/incubator-rocketmq\" target=\"_blank\" rel=\"external\">https://github.com/apache/incubator-rocketmq</a><br>    <a href=\"https://rocketmq.incubator.apache.org/\" target=\"_blank\" rel=\"external\">https://rocketmq.incubator.apache.org/</a><br>    <a href=\"http://www.jianshu.com/p/453c6e7ff81c\" target=\"_blank\" rel=\"external\">http://www.jianshu.com/p/453c6e7ff81c</a><br>    <a href=\"http://www.jialeens.com/archives/681.html?utm_source=tuicool&amp;utm_medium=referral\" target=\"_blank\" rel=\"external\">http://www.jialeens.com/archives/681.html?utm_source=tuicool&amp;utm_medium=referral</a></p>\n<h2 id=\"zeromq\"><a href=\"#zeromq\" class=\"headerlink\" title=\"zeromq\"></a>zeromq</h2><p>ZeroMQ 和 RabbitMQ/Kafka 不同，它只是一个异步消息库，在套接字的基础上提供了类似于消息代理的机制。使用 ZeroMQ 的话，需要对自己的业务代码进行改造，不利于服务解耦。</p>\n<h1 id=\"distribute-kv\"><a href=\"#distribute-kv\" class=\"headerlink\" title=\"distribute kv\"></a>distribute kv</h1><h2 id=\"etcd\"><a href=\"#etcd\" class=\"headerlink\" title=\"etcd\"></a>etcd</h2><p>about<br>        etcd 是一个应用在分布式环境下的 key/value 存储服务。利用 etcd 的特性，应用程序可以在集群中共享信息、配置或作服务发现，etcd 会在集群的各个节点中复制这些数据并保证这些数据始终正确。etcd 无论是在 CoreOS 还是 Kubernetes 体系中都是不可或缺的一环。</p>\n<pre><code>2. 规范词汇表\netcd 0.5.0 版首次对 etcd 代码、文档及 CLI 中使用的术语进行了定义。\n\n2.1. node\n\nnode 指一个 raft 状态机实例。每个 node 都具有唯一的标识，并在处于 leader 状态时记录其它节点的步进数。\n\n2.2. member\n\nmember 指一个 etcd 实例。member 运行在每个 node 上，并向这一 node 上的其它应用程序提供服务。\n\n2.3. Cluster\n\nCluster 由多个 member 组成。每个 member 中的 node 遵循 raft 共识协议来复制日志。Cluster 接收来自 member 的提案消息，将其提交并存储于本地磁盘。\n\n2.4. Peer\n\n同一 Cluster 中的其它 member。\n\n2.5. Client\n\nClient 指调用 Cluster API 的对象。\n\n3. Raft 共识算法\netcd 集群的工作原理基于 raft 共识算法 (The Raft Consensus Algorithm)。etcd 在 0.5.0 版本中重新实现了 raft 算法，而非像之前那样依赖于第三方库 go-raft 。raft 共识算法的优点在于可以在高效的解决分布式系统中各个节点日志内容一致性问题的同时，也使得集群具备一定的容错能力。即使集群中出现部分节点故障、网络故障等问题，仍可保证其余大多数节点正确的步进。甚至当更多的节点（一般来说超过集群节点总数的一半）出现故障而导致集群不可用时，依然可以保证节点中的数据不会出现错误的结果。\n</code></pre><p><a href=\"https://github.com/coreos/etcd\" target=\"_blank\" rel=\"external\">https://github.com/coreos/etcd</a><br><a href=\"http://www.infoq.com/cn/articles/coreos-analyse-etcd/\" target=\"_blank\" rel=\"external\">http://www.infoq.com/cn/articles/coreos-analyse-etcd/</a></p>\n<h1 id=\"IOE-resource\"><a href=\"#IOE-resource\" class=\"headerlink\" title=\"IOE resource\"></a>IOE resource</h1><p>IBM doc:<br><a href=\"http://www.ibm.com/analytics/cn/zh/\" target=\"_blank\" rel=\"external\">http://www.ibm.com/analytics/cn/zh/</a></p>\n<p>Oracle doc: <a href=\"http://docs.oracle.com/en/\" target=\"_blank\" rel=\"external\">http://docs.oracle.com/en/</a><br>SAP doc: <a href=\"http://www.sap.com/developer.html\" target=\"_blank\" rel=\"external\">http://www.sap.com/developer.html</a><br>SAP case: <a href=\"http://www.bestsapchina.com/ResourceCenter/i-t-s.html\" target=\"_blank\" rel=\"external\">http://www.bestsapchina.com/ResourceCenter/i-t-s.html</a></p>\n","excerpt":"","more":"<h1 id=\"aws\"><a href=\"#aws\" class=\"headerlink\" title=\"aws\"></a>aws</h1><p><a href=\"https://github.com/donnemartin/saws\">https://github.com/donnemartin/saws</a></p>\n<h1 id=\"top-resource\"><a href=\"#top-resource\" class=\"headerlink\" title=\"top resource\"></a>top resource</h1><p>infoq: <a href=\"http://2017.qconbeijing.com/tracks\">http://2017.qconbeijing.com/tracks</a></p>\n<p><a href=\"https://www.oschina.net/p/dubbo\">https://www.oschina.net/p/dubbo</a><br><a href=\"https://github.com/alibaba/dubbo\">https://github.com/alibaba/dubbo</a></p>\n<h2 id=\"zookeeper\"><a href=\"#zookeeper\" class=\"headerlink\" title=\"zookeeper\"></a>zookeeper</h2><ul>\n<li><p>about</p>\n<pre><code>ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services. All of these kinds of services are used in some form or another by distributed applications. Each time they are implemented there is a lot of work that goes into fixing the bugs and race conditions that are inevitable. Because of the difficulty of implementing these kinds of services, applications initially usually skimp on them ,which make them brittle in the presence of change and difficult to manage. Even when done correctly, different implementations of these services lead to management complexity when the applications are deployed.\n\n考虑一下有多个服务器的分布式系统，每台服务器都负责保存数据，在数据上执行操作。这样的潜在例子包括分布式搜索引擎、分布式构建系统或者已知的系统如Apache Hadoop。所有这些分布式系统的一个常见问题是，你如何在任一时间点确定哪些服务器活着并且在工作中。最重要的是，当面对这些分布式计算的难题，例如网络失败、带宽限制、可变延迟连接、安全问题以及任何网络环境，甚至跨多个数据中心时可能发生的错误时，你如何可靠地做这些事。这些正是Apache ZooKeeper所关注的问题，它是一个快速、高可用、容错、分布式的协调服务。你可以使用ZooKeeper构建可靠的、分布式的数据结构，用于群组成员、领导人选举、协同工作流和配置服务，以及广义的分布式数据结构如锁、队列、屏障（Barrier）和锁存器（Latch）。许多知名且成功的项目依赖于ZooKeeper，其中包括HBase、Hadoop 2.0、Solr Cloud、Neo4J、Apache Blur（Incubating）和Accumulo。\n\nZooKeeper是一个分布式的、分层级的文件系统，能促进客户端间的松耦合，并提供最终一致的，类似于传统文件系统中文件和目录的Znode视图。它提供了基本的操作，例如创建、删除和检查Znode是否存在。它提供了事件驱动模型，客户端能观察特定Znode的变化，例如现有Znode增加了一个新的子节点。ZooKeeper运行多个ZooKeeper服务器，称为Ensemble，以获得高可用性。每个服务器都持有分布式文件系统的内存复本，为客户端的读取请求提供服务。\n</code></pre></li>\n<li><p>usage</p>\n<pre><code>所有分布式的协商和一致都可以利用zk实现。可以理解为一个分布式的带有订阅功能的小型元数据库。\n使用方式自然就是通过上面这句话而定。比如你需要一个订阅功能的数据库，发布你的信息给其他客户端，享受订阅功能。又比如你有很多信息是由很多客户端同时竞争写入zk,但只允许第一个到达的写入，就可以享受zk的一致性。\n\n现在流行的分布式系统已经完全无法脱离zookeeper。列举几个简单的例子：\nstorm中用zookeeper来协调同步集群中机器的状态（并不传递消息）。基本不会有负载，对机器性能没什么要求。jstorm也是用zk来做一致性服务。\n\ndubbo中采用zookeeper来作注册中心，在阿里内部采用的是基于数据库的注册中心，我们自己用dubbo的时候采用了zookeeper，配置起来非常方便。当然如果采用redis或者自己写个简单的基于内存的注册中心也是可以的。从稳定性来讲，个人觉得zookeeper是比较好的选择，毕竟zookeeper集群中的机器只要不是半数以上宕掉，服务就是可用的。dubbo服务的消费者和提供者都需要用到zookeeper，然而消费者和提供者之间的长连接建立后，zookeeper参与程度就比较弱了(仅需要接受一些心跳包)，除非此时有新的提供者消费者加入或者离开(需要更新节点数据)。因此负载也是非常低的，基本不用考虑性能问题。zookeeper还同时起到了监控各个服务的作用。\n\nHBase要用到zk，这个已经有答主提到，就不用细说了。Hadoop也是需要用到zookeeper的，用来管理集群中的NameNode。差点忘了，Kafka集群依赖于ZooKeeper。kafka通过zookeeper实现生产者在消费端的负载均衡，动态的集群扩展等等。\n</code></pre></li>\n</ul>\n<p>resource:<br>    <a href=\"http://zookeeper.apache.org/\">http://zookeeper.apache.org/</a><br>    <a href=\"http://www.ibm.com/developerworks/cn/opensource/os-cn-zookeeper/\">http://www.ibm.com/developerworks/cn/opensource/os-cn-zookeeper/</a></p>\n<h1 id=\"distributed-messaging\"><a href=\"#distributed-messaging\" class=\"headerlink\" title=\"distributed messaging\"></a>distributed messaging</h1><h2 id=\"kafka\"><a href=\"#kafka\" class=\"headerlink\" title=\"kafka\"></a>kafka</h2><p>about</p>\n<pre><code>Apache Kafka是分布式发布-订阅消息系统。它最初由LinkedIn公司开发，之后成为Apache项目的一部分。Kafka是一种快速、可扩展的、设计内在就是分布式的，分区的和可复制的提交日志服务。Kafka是linkedin开源的MQ系统，主要特点是基于Pull的模式来处理消息消费，追求高吞吐量，一开始的目的就是用于日志收集和传输，0.8开始支持复制，不支持事务，适合产生大量数据的互联网服务的数据收集业务。\n\nApache Kafka与传统消息系统相比，有以下不同：\n它被设计为一个分布式系统，易于向外扩展；\n它同时为发布和订阅提供高吞吐量；\n它支持多订阅者，当失败时能自动平衡消费者；\n它将消息持久化到磁盘，因此可用于批量消费，例如ETL，以及实时应用程序。\n\nKafka 使用自己的协议。Kafka 自身服务和消费者都需要依赖 Zookeeper。RabbitMQ 在有大量消息堆积的情况下性能会下降，Kafka不会。毕竟AMQP设计的初衷不是用来持久化海量消息的，而Kafka一开始是用来处理海量日志的。\n</code></pre><p>resource：<br>    <a href=\"http://kafka.apache.org/\">http://kafka.apache.org/</a><br>    <a href=\"https://github.com/apache/kafka\">https://github.com/apache/kafka</a></p>\n<pre><code>http://www.infoq.com/cn/articles/apache-kafka\nhttp://www.infoq.com/cn/presentations/use-apache-kafka-to-transfer-key-business-message?utm_source=infoq&amp;utm_medium=related_content_link&amp;utm_campaign=relatedContent_articles_clk\n\nmust read: \n    https://github.com/oldratlee/translations/blob/master/log-what-every-software-engineer-should-know-about-real-time-datas-unifying/README.md\n    http://www.cnblogs.com/foreach-break/p/notes_about_distributed_system_and_The_log.html\n\nfaq：https://www.zhihu.com/question/22480085\n</code></pre><h2 id=\"rabbitmq\"><a href=\"#rabbitmq\" class=\"headerlink\" title=\"rabbitmq\"></a>rabbitmq</h2><p>RabbitMQ 支持 AMQP（二进制），STOMP（文本），MQTT（二进制），HTTP（里面包装其他协议）等协议。</p>\n<h2 id=\"rocketmq\"><a href=\"#rocketmq\" class=\"headerlink\" title=\"rocketmq\"></a>rocketmq</h2><p>about<br>    RocketMQ 是一款分布式、队列模型的消息中间件，具有以下特点：</p>\n<pre><code>能够保证严格的消息顺序\n提供丰富的消息拉取模式\n高效的订阅者水平扩展能力\n实时的消息订阅机制\n亿级消息堆积能力\nMetaq3.0 版本改名，产品名称改为RocketMQ\n\nRMQ的结构分为四个部分：生产者、消费者、nameserver、brokerserver\nnameserver：nameserver接收broker的请求注册broker路由信息。收client的请求根据某个topic获取所有到broker的路由信息。\nbrokerserver：消息的接收和推送，\n生产者：发送消息，将消息推送给brokerserver。\n消费者：接收消息，从brokerserver上获取消息。\n</code></pre><p>resource:<br>    <a href=\"https://www.aliyun.com/product/ons\">https://www.aliyun.com/product/ons</a><br>    <a href=\"https://github.com/apache/incubator-rocketmq\">https://github.com/apache/incubator-rocketmq</a><br>    <a href=\"https://rocketmq.incubator.apache.org/\">https://rocketmq.incubator.apache.org/</a><br>    <a href=\"http://www.jianshu.com/p/453c6e7ff81c\">http://www.jianshu.com/p/453c6e7ff81c</a><br>    <a href=\"http://www.jialeens.com/archives/681.html?utm_source=tuicool&amp;utm_medium=referral\">http://www.jialeens.com/archives/681.html?utm_source=tuicool&amp;utm_medium=referral</a></p>\n<h2 id=\"zeromq\"><a href=\"#zeromq\" class=\"headerlink\" title=\"zeromq\"></a>zeromq</h2><p>ZeroMQ 和 RabbitMQ/Kafka 不同，它只是一个异步消息库，在套接字的基础上提供了类似于消息代理的机制。使用 ZeroMQ 的话，需要对自己的业务代码进行改造，不利于服务解耦。</p>\n<h1 id=\"distribute-kv\"><a href=\"#distribute-kv\" class=\"headerlink\" title=\"distribute kv\"></a>distribute kv</h1><h2 id=\"etcd\"><a href=\"#etcd\" class=\"headerlink\" title=\"etcd\"></a>etcd</h2><p>about<br>        etcd 是一个应用在分布式环境下的 key/value 存储服务。利用 etcd 的特性，应用程序可以在集群中共享信息、配置或作服务发现，etcd 会在集群的各个节点中复制这些数据并保证这些数据始终正确。etcd 无论是在 CoreOS 还是 Kubernetes 体系中都是不可或缺的一环。</p>\n<pre><code>2. 规范词汇表\netcd 0.5.0 版首次对 etcd 代码、文档及 CLI 中使用的术语进行了定义。\n\n2.1. node\n\nnode 指一个 raft 状态机实例。每个 node 都具有唯一的标识，并在处于 leader 状态时记录其它节点的步进数。\n\n2.2. member\n\nmember 指一个 etcd 实例。member 运行在每个 node 上，并向这一 node 上的其它应用程序提供服务。\n\n2.3. Cluster\n\nCluster 由多个 member 组成。每个 member 中的 node 遵循 raft 共识协议来复制日志。Cluster 接收来自 member 的提案消息，将其提交并存储于本地磁盘。\n\n2.4. Peer\n\n同一 Cluster 中的其它 member。\n\n2.5. Client\n\nClient 指调用 Cluster API 的对象。\n\n3. Raft 共识算法\netcd 集群的工作原理基于 raft 共识算法 (The Raft Consensus Algorithm)。etcd 在 0.5.0 版本中重新实现了 raft 算法，而非像之前那样依赖于第三方库 go-raft 。raft 共识算法的优点在于可以在高效的解决分布式系统中各个节点日志内容一致性问题的同时，也使得集群具备一定的容错能力。即使集群中出现部分节点故障、网络故障等问题，仍可保证其余大多数节点正确的步进。甚至当更多的节点（一般来说超过集群节点总数的一半）出现故障而导致集群不可用时，依然可以保证节点中的数据不会出现错误的结果。\n</code></pre><p><a href=\"https://github.com/coreos/etcd\">https://github.com/coreos/etcd</a><br><a href=\"http://www.infoq.com/cn/articles/coreos-analyse-etcd/\">http://www.infoq.com/cn/articles/coreos-analyse-etcd/</a></p>\n<h1 id=\"IOE-resource\"><a href=\"#IOE-resource\" class=\"headerlink\" title=\"IOE resource\"></a>IOE resource</h1><p>IBM doc:<br><a href=\"http://www.ibm.com/analytics/cn/zh/\">http://www.ibm.com/analytics/cn/zh/</a></p>\n<p>Oracle doc: <a href=\"http://docs.oracle.com/en/\">http://docs.oracle.com/en/</a><br>SAP doc: <a href=\"http://www.sap.com/developer.html\">http://www.sap.com/developer.html</a><br>SAP case: <a href=\"http://www.bestsapchina.com/ResourceCenter/i-t-s.html\">http://www.bestsapchina.com/ResourceCenter/i-t-s.html</a></p>\n"},{"title":"python portal","_content":"\n# 123","source":"_posts/bigdata-ml-portal.md","raw":"---\ntitle: python portal\ncategories:\n- python\ntags:\n- portal\n---\n\n# 123","slug":"bigdata-ml-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:35:54.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm370000621sv5wqww7o5","content":"<h1 id=\"123\"><a href=\"#123\" class=\"headerlink\" title=\"123\"></a>123</h1>","excerpt":"","more":"<h1 id=\"123\"><a href=\"#123\" class=\"headerlink\" title=\"123\"></a>123</h1>"},{"title":"python portal","_content":"\n# 123","source":"_posts/bigdata-storm-portal.md","raw":"---\ntitle: python portal\ncategories:\n- python\ntags:\n- portal\n---\n\n# 123","slug":"bigdata-storm-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:35:54.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm373000721sve8sra0p8","content":"<h1 id=\"123\"><a href=\"#123\" class=\"headerlink\" title=\"123\"></a>123</h1>","excerpt":"","more":"<h1 id=\"123\"><a href=\"#123\" class=\"headerlink\" title=\"123\"></a>123</h1>"},{"title":"cloudfoundry core","_content":"\n# ","source":"_posts/cloud-paas-cf-core.md","raw":"---\ntitle: cloudfoundry core\ncategories:\n- paas\ntags:\n- core\n- cloudfoundry\n---\n\n# ","slug":"cloud-paas-cf-core","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:35:24.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm37b000b21svs9suqb35","content":"<p># </p>\n","excerpt":"","more":"<p># </p>\n"},{"title":"iaas core","_content":"\n# iaas core\n\n# iaas tool\n\ncloudinit\nhttps://cloudinit.readthedocs.io/en/latest/index.html","source":"_posts/cloud-iaas-portal.md","raw":"---\ntitle: iaas core\ncategories:\n- iaas\ntags:\n- portal\n---\n\n# iaas core\n\n# iaas tool\n\ncloudinit\nhttps://cloudinit.readthedocs.io/en/latest/index.html","slug":"cloud-iaas-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-02-18T05:51:25.000Z","_id":"ciypwm37g000e21sv4w08jqwz","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"iaas-core\"><a href=\"#iaas-core\" class=\"headerlink\" title=\"iaas core\"></a>iaas core</h1><h1 id=\"iaas-tool\"><a href=\"#iaas-tool\" class=\"headerlink\" title=\"iaas tool\"></a>iaas tool</h1><p>cloudinit<br><a href=\"https://cloudinit.readthedocs.io/en/latest/index.html\" target=\"_blank\" rel=\"external\">https://cloudinit.readthedocs.io/en/latest/index.html</a></p>\n","excerpt":"","more":"<h1 id=\"iaas-core\"><a href=\"#iaas-core\" class=\"headerlink\" title=\"iaas core\"></a>iaas core</h1><h1 id=\"iaas-tool\"><a href=\"#iaas-tool\" class=\"headerlink\" title=\"iaas tool\"></a>iaas tool</h1><p>cloudinit<br><a href=\"https://cloudinit.readthedocs.io/en/latest/index.html\">https://cloudinit.readthedocs.io/en/latest/index.html</a></p>\n"},{"title":"openshift core","_content":"\n# ","source":"_posts/cloud-paas-openshift-core.md","raw":"---\ntitle: openshift core\ncategories:\n- paas\ntags:\n- core\n- openshift\n---\n\n# ","slug":"cloud-paas-openshift-core","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:35:20.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm37k000j21svx26uzv21","content":"<p># </p>\n","excerpt":"","more":"<p># </p>\n"},{"title":"openstack core","_content":"\n# core","source":"_posts/cloud-iaas-openstack-core.md","raw":"---\ntitle: openstack core\ncategories:\n- iaas\ntags:\n- core\n- openstack\n---\n\n# core","slug":"cloud-iaas-openstack-core","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:33:08.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm37n000m21svlowv6kfh","content":"<h1 id=\"core\"><a href=\"#core\" class=\"headerlink\" title=\"core\"></a>core</h1>","excerpt":"","more":"<h1 id=\"core\"><a href=\"#core\" class=\"headerlink\" title=\"core\"></a>core</h1>"},{"title":"paas portal","_content":"\n","source":"_posts/cloud-paas-portal.md","raw":"---\ntitle: paas portal\ncategories:\n- paas\ntags:\n- portal\n---\n\n","slug":"cloud-paas-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:35:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm37s000r21svd81wuly4","content":"","excerpt":"","more":""},{"title":"java portal","_content":"\n#  portal\n\nyugong\nhttps://github.com/alibaba/yugong\n\nhttps://github.com/alibaba/AliSQL","source":"_posts/db--portal.md","raw":"---\ntitle: java portal\ncategories:\n- java\ntags:\n- portal\n---\n\n#  portal\n\nyugong\nhttps://github.com/alibaba/yugong\n\nhttps://github.com/alibaba/AliSQL","slug":"db--portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-02-04T08:55:47.000Z","_id":"ciypwm37v000u21svtfny6hag","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"portal\"><a href=\"#portal\" class=\"headerlink\" title=\"portal\"></a>portal</h1><p>yugong<br><a href=\"https://github.com/alibaba/yugong\" target=\"_blank\" rel=\"external\">https://github.com/alibaba/yugong</a></p>\n<p><a href=\"https://github.com/alibaba/AliSQL\" target=\"_blank\" rel=\"external\">https://github.com/alibaba/AliSQL</a></p>\n","excerpt":"","more":"<h1 id=\"portal\"><a href=\"#portal\" class=\"headerlink\" title=\"portal\"></a>portal</h1><p>yugong<br><a href=\"https://github.com/alibaba/yugong\">https://github.com/alibaba/yugong</a></p>\n<p><a href=\"https://github.com/alibaba/AliSQL\">https://github.com/alibaba/AliSQL</a></p>\n"},{"title":"java portal","_content":"\n#  portal","source":"_posts/db-mysql-portal.md","raw":"---\ntitle: java portal\ncategories:\n- java\ntags:\n- portal\n---\n\n#  portal","slug":"db-mysql-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:33:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm37z000y21sv7ku7mwqz","content":"<h1 id=\"portal\"><a href=\"#portal\" class=\"headerlink\" title=\"portal\"></a>portal</h1>","excerpt":"","more":"<h1 id=\"portal\"><a href=\"#portal\" class=\"headerlink\" title=\"portal\"></a>portal</h1>"},{"title":"java portal","_content":"\n#  portal","source":"_posts/db-nosql-portal.md","raw":"---\ntitle: java portal\ncategories:\n- java\ntags:\n- portal\n---\n\n#  portal","slug":"db-nosql-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:33:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm381001121svtap4yfqx","content":"<h1 id=\"portal\"><a href=\"#portal\" class=\"headerlink\" title=\"portal\"></a>portal</h1>","excerpt":"","more":"<h1 id=\"portal\"><a href=\"#portal\" class=\"headerlink\" title=\"portal\"></a>portal</h1>"},{"title":"mesos core","_content":"\n#  core","source":"_posts/docker-cloud-detail.md","raw":"---\ntitle: mesos core\ncategories:\n- docker\ntags:\n- core\n---\n\n#  core","slug":"docker-cloud-detail","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:32:26.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm387001621sv0tyyvktn","content":"<h1 id=\"core\"><a href=\"#core\" class=\"headerlink\" title=\"core\"></a>core</h1>","excerpt":"","more":"<h1 id=\"core\"><a href=\"#core\" class=\"headerlink\" title=\"core\"></a>core</h1>"},{"title":"paas portal","_content":"\n","source":"_posts/cloud-saas-portal.md","raw":"---\ntitle: paas portal\ncategories:\n- paas\ntags:\n- portal\n---\n\n","slug":"cloud-saas-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:35:35.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm38b001821svqerj6t12","content":"","excerpt":"","more":""},{"title":"docker portal","_content":"\n# resource\n\nofficial:\n[https://www.docker.com/](https://www.docker.com/)\n[https://blog.docker.com/](https://blog.docker.com/)\n[https://linuxcontainers.org/](https://linuxcontainers.org/)\n\nawesome:\n[https://github.com/Friz-zy/awesome-linux-containers](https://github.com/Friz-zy/awesome-linux-containers)\n\ngithub:\n[docker-library](https://github.com/docker-library)\n[dockerfile](https://github.com/tianon/dockerfiles)   \n[docker-cheat-sheet](https://github.com/wsargent/docker-cheat-sheet)\n[Chef Cookbook for Docker](https://github.com/chef-cookbooks/docker)\n\ncommunity:\n[dockerpool](http://dockerpool.com/)\n[dockerone](http://dockerone.com)\n[devops-china](http://devops-china.org)\n[ceph china](http://bbs.ceph.org.cn/)    \ncloud stack(http://www.cloudstack-china.org/)\natcontainer(http://atcontainer.com/)\ngoogle plus(https://plus.google.com/u/0/+DockerIo)\nzhihu(https://www.zhihu.com/topic/19950993)\nreddit\nstackoverflow(http://superuser.com/questions/tagged/docker)\n\norganization:\nopen container project: [link](http://www.opencontainers.org/), [doc](http://blog.docker.com/2015/06/open-container-project-foundation/)\noci:[](https://www.opencontainers.org/), [](https://github.com/opencontainers)\nabout:尽管Docker获得广大公有云厂商的大力支持，但是目前容器技术生态中已经存在许多分支与分歧，如rkt项目。为了解决容器生态中的差异化问题，为了从根本上解决生产环境中运用Docker的风险，Google，Intel，Redhat，Microsoft，EMC，IBM，Amazon，VMware，Oracle，Pivotal，Rancher，HPE，Facebook，Twitter等IT大厂于2015年6月共同宣布成立OCI（Open Container Initiative）组织。OCI组织的目标在于建立通用的容器技术标准。除了保障与延续既有容器服务的生命周期外，还通过不断推出标准的创新的容器解决方案赋能开发者。而OCI成员企业也会秉持开放，安全，弹性等核心价值观来发展容器生态。客观而言，OCI组织的出现确立了容器技术的标准，避免容器技术被单一厂商垄断。统一技术标准后，广大企业不用担心未来新兴的容器技术不兼容Docker。\nCNCF: Cloud Native Computing Foundation\n\nconf\nQConf:http://2016.qconshanghai.com/, http://qconferences.com/\nDockerCon\n    link\n        http://www.slideshare.net/Docker/presentations\n        http://www.dockercon.com/\n    2015\n        guidebook app\n        http://europe-2015.dockercon.com/\n        http://dockerconeu2015.sched.org/\n    2016\n        http://2016.dockercon.com/\n        https://blog.docker.com/2016/06/dockercon-general-session-day-1-and-day-2-videos/\nContainerCon\nContainerCamp\nOperability 1.0\nGoTo Conference\nSoftware Circus\n容器技术大会\n    http://atcontainer.com/\n有容云\n    http://www.bagevent.com/event/176371\n\ndocker vs x\n    docker vs virtual machine\n        link\n            http://stackoverflow.com/questions/16047306/how-is-docker-different-from-a-normal-virtual-machine#\n            http://stackoverflow.com/questions/25444099/why-docker-has-ability-to-run-different-linux-distribution\n            http://stackoverflow.com/questions/18496940/how-to-deal-with-persistent-storage-e-g-databases-in-docker\n\ntraining:\n    http://edu.51cto.com/\n    https://csphere.cn/training/lesson/2\n    http://git.oschina.net/dockerf/docker-practice\n    http://git.oschina.net/dockerf/docker-training\n    mooc:https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS151.x+2T2016/info\n    \nread:\n- infoq: [infoq cn](http://www.infoq.com/cn/dockers/), [infoq en](https://www.infoq.com/docker-2)\n- bot: [yidian](http://www.yidianzixun.com/home?page=channel&keyword=docker), [toutiao](http://toutiao.com/tag85482990/)\n- book\n    Docker Cookbook\n    Docker技术入门与实战:publish,[opensource](http://dockerpool.com/static/books/docker_practice/index.html)\n    Docker源码分析\n    Docker技术详解与实践 lts        \n    [docker入门实战](http://yuedu.baidu.com/ebook/d817967416fc700abb68fca1)\n    [Service discovery with Docker](http://adetante.github.io/articles/service-discovery-with-docker-1/ )\n- tut\n    [](http://www.alauda.cn/tutorial/)\n    [](http://help.daocloud.io/)        \n    [](https://coreos.com/os/docs/latest/quickstart.html   )        \n- course\n    https://training.docker.com/self-paced-training\n    http://www.infoq.com/cn/presentations/antgroup-financial-cloud-paas-docker-practice              \nlanguages:\n- php\n    https://github.com/schmunk42/docker-yii2-app-basic\n    https://github.com/eko/docker-symfony\n    https://github.com/harshjv/docker-laravel\n    https://github.com/tkyk/docker-compose-lamp\n    https://github.com/docker-library/php\n    compose\n    https://github.com/DockerNuts/docker-compose-php-mysql/blob/master/docker-compose.yml\n    https://github.com/tkyk/docker-compose-lamp\n    https://github.com/larryprice/docker-compose-example\n- py\n    https://github.com/dockerfiles/django-uwsgi-nginx\n    https://github.com/mbentley/docker-django-uwsgi-nginx\n    https://github.com/dockerfiles/django-uwsgi-nginx\n    django:https://docs.docker.com/compose/django/\n    flask:dockercook 3.1\n- ruby\n    rails:https://docs.docker.com/compose/rails/\n- js\n    https://github.com/b00giZm/docker-compose-nodejs-examples/tree/master/01-express-nodemon\n- other\n    https://github.com/docker/compose/blob/master/SWARM.md\n    wordpress:https://docs.docker.com/compose/wordpress/\n\n12 factor\n    http://www.the12factorapp.com/\n    https://12factor.net/\n    https://chixq.com/articles/12-factor-app/\n\n\n        ","source":"_posts/docker--portal.md","raw":"---\ntitle: docker portal\ncategories:\n- docker\ntags:\n- portal\n---\n\n# resource\n\nofficial:\n[https://www.docker.com/](https://www.docker.com/)\n[https://blog.docker.com/](https://blog.docker.com/)\n[https://linuxcontainers.org/](https://linuxcontainers.org/)\n\nawesome:\n[https://github.com/Friz-zy/awesome-linux-containers](https://github.com/Friz-zy/awesome-linux-containers)\n\ngithub:\n[docker-library](https://github.com/docker-library)\n[dockerfile](https://github.com/tianon/dockerfiles)   \n[docker-cheat-sheet](https://github.com/wsargent/docker-cheat-sheet)\n[Chef Cookbook for Docker](https://github.com/chef-cookbooks/docker)\n\ncommunity:\n[dockerpool](http://dockerpool.com/)\n[dockerone](http://dockerone.com)\n[devops-china](http://devops-china.org)\n[ceph china](http://bbs.ceph.org.cn/)    \ncloud stack(http://www.cloudstack-china.org/)\natcontainer(http://atcontainer.com/)\ngoogle plus(https://plus.google.com/u/0/+DockerIo)\nzhihu(https://www.zhihu.com/topic/19950993)\nreddit\nstackoverflow(http://superuser.com/questions/tagged/docker)\n\norganization:\nopen container project: [link](http://www.opencontainers.org/), [doc](http://blog.docker.com/2015/06/open-container-project-foundation/)\noci:[](https://www.opencontainers.org/), [](https://github.com/opencontainers)\nabout:尽管Docker获得广大公有云厂商的大力支持，但是目前容器技术生态中已经存在许多分支与分歧，如rkt项目。为了解决容器生态中的差异化问题，为了从根本上解决生产环境中运用Docker的风险，Google，Intel，Redhat，Microsoft，EMC，IBM，Amazon，VMware，Oracle，Pivotal，Rancher，HPE，Facebook，Twitter等IT大厂于2015年6月共同宣布成立OCI（Open Container Initiative）组织。OCI组织的目标在于建立通用的容器技术标准。除了保障与延续既有容器服务的生命周期外，还通过不断推出标准的创新的容器解决方案赋能开发者。而OCI成员企业也会秉持开放，安全，弹性等核心价值观来发展容器生态。客观而言，OCI组织的出现确立了容器技术的标准，避免容器技术被单一厂商垄断。统一技术标准后，广大企业不用担心未来新兴的容器技术不兼容Docker。\nCNCF: Cloud Native Computing Foundation\n\nconf\nQConf:http://2016.qconshanghai.com/, http://qconferences.com/\nDockerCon\n    link\n        http://www.slideshare.net/Docker/presentations\n        http://www.dockercon.com/\n    2015\n        guidebook app\n        http://europe-2015.dockercon.com/\n        http://dockerconeu2015.sched.org/\n    2016\n        http://2016.dockercon.com/\n        https://blog.docker.com/2016/06/dockercon-general-session-day-1-and-day-2-videos/\nContainerCon\nContainerCamp\nOperability 1.0\nGoTo Conference\nSoftware Circus\n容器技术大会\n    http://atcontainer.com/\n有容云\n    http://www.bagevent.com/event/176371\n\ndocker vs x\n    docker vs virtual machine\n        link\n            http://stackoverflow.com/questions/16047306/how-is-docker-different-from-a-normal-virtual-machine#\n            http://stackoverflow.com/questions/25444099/why-docker-has-ability-to-run-different-linux-distribution\n            http://stackoverflow.com/questions/18496940/how-to-deal-with-persistent-storage-e-g-databases-in-docker\n\ntraining:\n    http://edu.51cto.com/\n    https://csphere.cn/training/lesson/2\n    http://git.oschina.net/dockerf/docker-practice\n    http://git.oschina.net/dockerf/docker-training\n    mooc:https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS151.x+2T2016/info\n    \nread:\n- infoq: [infoq cn](http://www.infoq.com/cn/dockers/), [infoq en](https://www.infoq.com/docker-2)\n- bot: [yidian](http://www.yidianzixun.com/home?page=channel&keyword=docker), [toutiao](http://toutiao.com/tag85482990/)\n- book\n    Docker Cookbook\n    Docker技术入门与实战:publish,[opensource](http://dockerpool.com/static/books/docker_practice/index.html)\n    Docker源码分析\n    Docker技术详解与实践 lts        \n    [docker入门实战](http://yuedu.baidu.com/ebook/d817967416fc700abb68fca1)\n    [Service discovery with Docker](http://adetante.github.io/articles/service-discovery-with-docker-1/ )\n- tut\n    [](http://www.alauda.cn/tutorial/)\n    [](http://help.daocloud.io/)        \n    [](https://coreos.com/os/docs/latest/quickstart.html   )        \n- course\n    https://training.docker.com/self-paced-training\n    http://www.infoq.com/cn/presentations/antgroup-financial-cloud-paas-docker-practice              \nlanguages:\n- php\n    https://github.com/schmunk42/docker-yii2-app-basic\n    https://github.com/eko/docker-symfony\n    https://github.com/harshjv/docker-laravel\n    https://github.com/tkyk/docker-compose-lamp\n    https://github.com/docker-library/php\n    compose\n    https://github.com/DockerNuts/docker-compose-php-mysql/blob/master/docker-compose.yml\n    https://github.com/tkyk/docker-compose-lamp\n    https://github.com/larryprice/docker-compose-example\n- py\n    https://github.com/dockerfiles/django-uwsgi-nginx\n    https://github.com/mbentley/docker-django-uwsgi-nginx\n    https://github.com/dockerfiles/django-uwsgi-nginx\n    django:https://docs.docker.com/compose/django/\n    flask:dockercook 3.1\n- ruby\n    rails:https://docs.docker.com/compose/rails/\n- js\n    https://github.com/b00giZm/docker-compose-nodejs-examples/tree/master/01-express-nodemon\n- other\n    https://github.com/docker/compose/blob/master/SWARM.md\n    wordpress:https://docs.docker.com/compose/wordpress/\n\n12 factor\n    http://www.the12factorapp.com/\n    https://12factor.net/\n    https://chixq.com/articles/12-factor-app/\n\n\n        ","slug":"docker--portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-02-10T03:40:59.000Z","_id":"ciypwm38f001d21svebn6o2x0","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"resource\"><a href=\"#resource\" class=\"headerlink\" title=\"resource\"></a>resource</h1><p>official:<br><a href=\"https://www.docker.com/\" target=\"_blank\" rel=\"external\">https://www.docker.com/</a><br><a href=\"https://blog.docker.com/\" target=\"_blank\" rel=\"external\">https://blog.docker.com/</a><br><a href=\"https://linuxcontainers.org/\" target=\"_blank\" rel=\"external\">https://linuxcontainers.org/</a></p>\n<p>awesome:<br><a href=\"https://github.com/Friz-zy/awesome-linux-containers\" target=\"_blank\" rel=\"external\">https://github.com/Friz-zy/awesome-linux-containers</a></p>\n<p>github:<br><a href=\"https://github.com/docker-library\" target=\"_blank\" rel=\"external\">docker-library</a><br><a href=\"https://github.com/tianon/dockerfiles\" target=\"_blank\" rel=\"external\">dockerfile</a><br><a href=\"https://github.com/wsargent/docker-cheat-sheet\" target=\"_blank\" rel=\"external\">docker-cheat-sheet</a><br><a href=\"https://github.com/chef-cookbooks/docker\" target=\"_blank\" rel=\"external\">Chef Cookbook for Docker</a></p>\n<p>community:<br><a href=\"http://dockerpool.com/\" target=\"_blank\" rel=\"external\">dockerpool</a><br><a href=\"http://dockerone.com\" target=\"_blank\" rel=\"external\">dockerone</a><br><a href=\"http://devops-china.org\" target=\"_blank\" rel=\"external\">devops-china</a><br><a href=\"http://bbs.ceph.org.cn/\" target=\"_blank\" rel=\"external\">ceph china</a><br>cloud stack(<a href=\"http://www.cloudstack-china.org/\" target=\"_blank\" rel=\"external\">http://www.cloudstack-china.org/</a>)<br>atcontainer(<a href=\"http://atcontainer.com/\" target=\"_blank\" rel=\"external\">http://atcontainer.com/</a>)<br>google plus(<a href=\"https://plus.google.com/u/0/+DockerIo\" target=\"_blank\" rel=\"external\">https://plus.google.com/u/0/+DockerIo</a>)<br>zhihu(<a href=\"https://www.zhihu.com/topic/19950993\" target=\"_blank\" rel=\"external\">https://www.zhihu.com/topic/19950993</a>)<br>reddit<br>stackoverflow(<a href=\"http://superuser.com/questions/tagged/docker\" target=\"_blank\" rel=\"external\">http://superuser.com/questions/tagged/docker</a>)</p>\n<p>organization:<br>open container project: <a href=\"http://www.opencontainers.org/\" target=\"_blank\" rel=\"external\">link</a>, <a href=\"http://blog.docker.com/2015/06/open-container-project-foundation/\" target=\"_blank\" rel=\"external\">doc</a><br>oci:<a href=\"https://www.opencontainers.org/\" target=\"_blank\" rel=\"external\"></a>, <a href=\"https://github.com/opencontainers\" target=\"_blank\" rel=\"external\"></a><br>about:尽管Docker获得广大公有云厂商的大力支持，但是目前容器技术生态中已经存在许多分支与分歧，如rkt项目。为了解决容器生态中的差异化问题，为了从根本上解决生产环境中运用Docker的风险，Google，Intel，Redhat，Microsoft，EMC，IBM，Amazon，VMware，Oracle，Pivotal，Rancher，HPE，Facebook，Twitter等IT大厂于2015年6月共同宣布成立OCI（Open Container Initiative）组织。OCI组织的目标在于建立通用的容器技术标准。除了保障与延续既有容器服务的生命周期外，还通过不断推出标准的创新的容器解决方案赋能开发者。而OCI成员企业也会秉持开放，安全，弹性等核心价值观来发展容器生态。客观而言，OCI组织的出现确立了容器技术的标准，避免容器技术被单一厂商垄断。统一技术标准后，广大企业不用担心未来新兴的容器技术不兼容Docker。<br>CNCF: Cloud Native Computing Foundation</p>\n<p>conf<br>QConf:<a href=\"http://2016.qconshanghai.com/\" target=\"_blank\" rel=\"external\">http://2016.qconshanghai.com/</a>, <a href=\"http://qconferences.com/\" target=\"_blank\" rel=\"external\">http://qconferences.com/</a><br>DockerCon<br>    link<br>        <a href=\"http://www.slideshare.net/Docker/presentations\" target=\"_blank\" rel=\"external\">http://www.slideshare.net/Docker/presentations</a><br>        <a href=\"http://www.dockercon.com/\" target=\"_blank\" rel=\"external\">http://www.dockercon.com/</a><br>    2015<br>        guidebook app<br>        <a href=\"http://europe-2015.dockercon.com/\" target=\"_blank\" rel=\"external\">http://europe-2015.dockercon.com/</a><br>        <a href=\"http://dockerconeu2015.sched.org/\" target=\"_blank\" rel=\"external\">http://dockerconeu2015.sched.org/</a><br>    2016<br>        <a href=\"http://2016.dockercon.com/\" target=\"_blank\" rel=\"external\">http://2016.dockercon.com/</a><br>        <a href=\"https://blog.docker.com/2016/06/dockercon-general-session-day-1-and-day-2-videos/\" target=\"_blank\" rel=\"external\">https://blog.docker.com/2016/06/dockercon-general-session-day-1-and-day-2-videos/</a><br>ContainerCon<br>ContainerCamp<br>Operability 1.0<br>GoTo Conference<br>Software Circus<br>容器技术大会<br>    <a href=\"http://atcontainer.com/\" target=\"_blank\" rel=\"external\">http://atcontainer.com/</a><br>有容云<br>    <a href=\"http://www.bagevent.com/event/176371\" target=\"_blank\" rel=\"external\">http://www.bagevent.com/event/176371</a></p>\n<p>docker vs x<br>    docker vs virtual machine<br>        link<br>            <a href=\"http://stackoverflow.com/questions/16047306/how-is-docker-different-from-a-normal-virtual-machine#\" target=\"_blank\" rel=\"external\">http://stackoverflow.com/questions/16047306/how-is-docker-different-from-a-normal-virtual-machine#</a><br>            <a href=\"http://stackoverflow.com/questions/25444099/why-docker-has-ability-to-run-different-linux-distribution\" target=\"_blank\" rel=\"external\">http://stackoverflow.com/questions/25444099/why-docker-has-ability-to-run-different-linux-distribution</a><br>            <a href=\"http://stackoverflow.com/questions/18496940/how-to-deal-with-persistent-storage-e-g-databases-in-docker\" target=\"_blank\" rel=\"external\">http://stackoverflow.com/questions/18496940/how-to-deal-with-persistent-storage-e-g-databases-in-docker</a></p>\n<p>training:<br>    <a href=\"http://edu.51cto.com/\" target=\"_blank\" rel=\"external\">http://edu.51cto.com/</a><br>    <a href=\"https://csphere.cn/training/lesson/2\" target=\"_blank\" rel=\"external\">https://csphere.cn/training/lesson/2</a><br>    <a href=\"http://git.oschina.net/dockerf/docker-practice\" target=\"_blank\" rel=\"external\">http://git.oschina.net/dockerf/docker-practice</a><br>    <a href=\"http://git.oschina.net/dockerf/docker-training\" target=\"_blank\" rel=\"external\">http://git.oschina.net/dockerf/docker-training</a><br>    mooc:<a href=\"https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS151.x+2T2016/info\" target=\"_blank\" rel=\"external\">https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS151.x+2T2016/info</a></p>\n<p>read:</p>\n<ul>\n<li>infoq: <a href=\"http://www.infoq.com/cn/dockers/\" target=\"_blank\" rel=\"external\">infoq cn</a>, <a href=\"https://www.infoq.com/docker-2\" target=\"_blank\" rel=\"external\">infoq en</a></li>\n<li>bot: <a href=\"http://www.yidianzixun.com/home?page=channel&amp;keyword=docker\" target=\"_blank\" rel=\"external\">yidian</a>, <a href=\"http://toutiao.com/tag85482990/\" target=\"_blank\" rel=\"external\">toutiao</a></li>\n<li>book<br>  Docker Cookbook<br>  Docker技术入门与实战:publish,<a href=\"http://dockerpool.com/static/books/docker_practice/index.html\" target=\"_blank\" rel=\"external\">opensource</a><br>  Docker源码分析<br>  Docker技术详解与实践 lts<br>  <a href=\"http://yuedu.baidu.com/ebook/d817967416fc700abb68fca1\" target=\"_blank\" rel=\"external\">docker入门实战</a><br>  <a href=\"http://adetante.github.io/articles/service-discovery-with-docker-1/\" target=\"_blank\" rel=\"external\">Service discovery with Docker</a></li>\n<li>tut<br>  <a href=\"http://www.alauda.cn/tutorial/\" target=\"_blank\" rel=\"external\"></a><br>  <a href=\"http://help.daocloud.io/\" target=\"_blank\" rel=\"external\"></a><br>  <a href=\"https://coreos.com/os/docs/latest/quickstart.html\" target=\"_blank\" rel=\"external\"></a>        </li>\n<li>course<br>  <a href=\"https://training.docker.com/self-paced-training\" target=\"_blank\" rel=\"external\">https://training.docker.com/self-paced-training</a><br>  <a href=\"http://www.infoq.com/cn/presentations/antgroup-financial-cloud-paas-docker-practice\" target=\"_blank\" rel=\"external\">http://www.infoq.com/cn/presentations/antgroup-financial-cloud-paas-docker-practice</a><br>languages:</li>\n<li>php<br>  <a href=\"https://github.com/schmunk42/docker-yii2-app-basic\" target=\"_blank\" rel=\"external\">https://github.com/schmunk42/docker-yii2-app-basic</a><br>  <a href=\"https://github.com/eko/docker-symfony\" target=\"_blank\" rel=\"external\">https://github.com/eko/docker-symfony</a><br>  <a href=\"https://github.com/harshjv/docker-laravel\" target=\"_blank\" rel=\"external\">https://github.com/harshjv/docker-laravel</a><br>  <a href=\"https://github.com/tkyk/docker-compose-lamp\" target=\"_blank\" rel=\"external\">https://github.com/tkyk/docker-compose-lamp</a><br>  <a href=\"https://github.com/docker-library/php\" target=\"_blank\" rel=\"external\">https://github.com/docker-library/php</a><br>  compose<br>  <a href=\"https://github.com/DockerNuts/docker-compose-php-mysql/blob/master/docker-compose.yml\" target=\"_blank\" rel=\"external\">https://github.com/DockerNuts/docker-compose-php-mysql/blob/master/docker-compose.yml</a><br>  <a href=\"https://github.com/tkyk/docker-compose-lamp\" target=\"_blank\" rel=\"external\">https://github.com/tkyk/docker-compose-lamp</a><br>  <a href=\"https://github.com/larryprice/docker-compose-example\" target=\"_blank\" rel=\"external\">https://github.com/larryprice/docker-compose-example</a></li>\n<li>py<br>  <a href=\"https://github.com/dockerfiles/django-uwsgi-nginx\" target=\"_blank\" rel=\"external\">https://github.com/dockerfiles/django-uwsgi-nginx</a><br>  <a href=\"https://github.com/mbentley/docker-django-uwsgi-nginx\" target=\"_blank\" rel=\"external\">https://github.com/mbentley/docker-django-uwsgi-nginx</a><br>  <a href=\"https://github.com/dockerfiles/django-uwsgi-nginx\" target=\"_blank\" rel=\"external\">https://github.com/dockerfiles/django-uwsgi-nginx</a><br>  django:<a href=\"https://docs.docker.com/compose/django/\" target=\"_blank\" rel=\"external\">https://docs.docker.com/compose/django/</a><br>  flask:dockercook 3.1</li>\n<li>ruby<br>  rails:<a href=\"https://docs.docker.com/compose/rails/\" target=\"_blank\" rel=\"external\">https://docs.docker.com/compose/rails/</a></li>\n<li>js<br>  <a href=\"https://github.com/b00giZm/docker-compose-nodejs-examples/tree/master/01-express-nodemon\" target=\"_blank\" rel=\"external\">https://github.com/b00giZm/docker-compose-nodejs-examples/tree/master/01-express-nodemon</a></li>\n<li>other<br>  <a href=\"https://github.com/docker/compose/blob/master/SWARM.md\" target=\"_blank\" rel=\"external\">https://github.com/docker/compose/blob/master/SWARM.md</a><br>  wordpress:<a href=\"https://docs.docker.com/compose/wordpress/\" target=\"_blank\" rel=\"external\">https://docs.docker.com/compose/wordpress/</a></li>\n</ul>\n<p>12 factor<br>    <a href=\"http://www.the12factorapp.com/\" target=\"_blank\" rel=\"external\">http://www.the12factorapp.com/</a><br>    <a href=\"https://12factor.net/\" target=\"_blank\" rel=\"external\">https://12factor.net/</a><br>    <a href=\"https://chixq.com/articles/12-factor-app/\" target=\"_blank\" rel=\"external\">https://chixq.com/articles/12-factor-app/</a></p>\n","excerpt":"","more":"<h1 id=\"resource\"><a href=\"#resource\" class=\"headerlink\" title=\"resource\"></a>resource</h1><p>official:<br><a href=\"https://www.docker.com/\">https://www.docker.com/</a><br><a href=\"https://blog.docker.com/\">https://blog.docker.com/</a><br><a href=\"https://linuxcontainers.org/\">https://linuxcontainers.org/</a></p>\n<p>awesome:<br><a href=\"https://github.com/Friz-zy/awesome-linux-containers\">https://github.com/Friz-zy/awesome-linux-containers</a></p>\n<p>github:<br><a href=\"https://github.com/docker-library\">docker-library</a><br><a href=\"https://github.com/tianon/dockerfiles\">dockerfile</a><br><a href=\"https://github.com/wsargent/docker-cheat-sheet\">docker-cheat-sheet</a><br><a href=\"https://github.com/chef-cookbooks/docker\">Chef Cookbook for Docker</a></p>\n<p>community:<br><a href=\"http://dockerpool.com/\">dockerpool</a><br><a href=\"http://dockerone.com\">dockerone</a><br><a href=\"http://devops-china.org\">devops-china</a><br><a href=\"http://bbs.ceph.org.cn/\">ceph china</a><br>cloud stack(<a href=\"http://www.cloudstack-china.org/\">http://www.cloudstack-china.org/</a>)<br>atcontainer(<a href=\"http://atcontainer.com/\">http://atcontainer.com/</a>)<br>google plus(<a href=\"https://plus.google.com/u/0/+DockerIo\">https://plus.google.com/u/0/+DockerIo</a>)<br>zhihu(<a href=\"https://www.zhihu.com/topic/19950993\">https://www.zhihu.com/topic/19950993</a>)<br>reddit<br>stackoverflow(<a href=\"http://superuser.com/questions/tagged/docker\">http://superuser.com/questions/tagged/docker</a>)</p>\n<p>organization:<br>open container project: <a href=\"http://www.opencontainers.org/\">link</a>, <a href=\"http://blog.docker.com/2015/06/open-container-project-foundation/\">doc</a><br>oci:<a href=\"https://www.opencontainers.org/\"></a>, <a href=\"https://github.com/opencontainers\"></a><br>about:尽管Docker获得广大公有云厂商的大力支持，但是目前容器技术生态中已经存在许多分支与分歧，如rkt项目。为了解决容器生态中的差异化问题，为了从根本上解决生产环境中运用Docker的风险，Google，Intel，Redhat，Microsoft，EMC，IBM，Amazon，VMware，Oracle，Pivotal，Rancher，HPE，Facebook，Twitter等IT大厂于2015年6月共同宣布成立OCI（Open Container Initiative）组织。OCI组织的目标在于建立通用的容器技术标准。除了保障与延续既有容器服务的生命周期外，还通过不断推出标准的创新的容器解决方案赋能开发者。而OCI成员企业也会秉持开放，安全，弹性等核心价值观来发展容器生态。客观而言，OCI组织的出现确立了容器技术的标准，避免容器技术被单一厂商垄断。统一技术标准后，广大企业不用担心未来新兴的容器技术不兼容Docker。<br>CNCF: Cloud Native Computing Foundation</p>\n<p>conf<br>QConf:<a href=\"http://2016.qconshanghai.com/\">http://2016.qconshanghai.com/</a>, <a href=\"http://qconferences.com/\">http://qconferences.com/</a><br>DockerCon<br>    link<br>        <a href=\"http://www.slideshare.net/Docker/presentations\">http://www.slideshare.net/Docker/presentations</a><br>        <a href=\"http://www.dockercon.com/\">http://www.dockercon.com/</a><br>    2015<br>        guidebook app<br>        <a href=\"http://europe-2015.dockercon.com/\">http://europe-2015.dockercon.com/</a><br>        <a href=\"http://dockerconeu2015.sched.org/\">http://dockerconeu2015.sched.org/</a><br>    2016<br>        <a href=\"http://2016.dockercon.com/\">http://2016.dockercon.com/</a><br>        <a href=\"https://blog.docker.com/2016/06/dockercon-general-session-day-1-and-day-2-videos/\">https://blog.docker.com/2016/06/dockercon-general-session-day-1-and-day-2-videos/</a><br>ContainerCon<br>ContainerCamp<br>Operability 1.0<br>GoTo Conference<br>Software Circus<br>容器技术大会<br>    <a href=\"http://atcontainer.com/\">http://atcontainer.com/</a><br>有容云<br>    <a href=\"http://www.bagevent.com/event/176371\">http://www.bagevent.com/event/176371</a></p>\n<p>docker vs x<br>    docker vs virtual machine<br>        link<br>            <a href=\"http://stackoverflow.com/questions/16047306/how-is-docker-different-from-a-normal-virtual-machine#\">http://stackoverflow.com/questions/16047306/how-is-docker-different-from-a-normal-virtual-machine#</a><br>            <a href=\"http://stackoverflow.com/questions/25444099/why-docker-has-ability-to-run-different-linux-distribution\">http://stackoverflow.com/questions/25444099/why-docker-has-ability-to-run-different-linux-distribution</a><br>            <a href=\"http://stackoverflow.com/questions/18496940/how-to-deal-with-persistent-storage-e-g-databases-in-docker\">http://stackoverflow.com/questions/18496940/how-to-deal-with-persistent-storage-e-g-databases-in-docker</a></p>\n<p>training:<br>    <a href=\"http://edu.51cto.com/\">http://edu.51cto.com/</a><br>    <a href=\"https://csphere.cn/training/lesson/2\">https://csphere.cn/training/lesson/2</a><br>    <a href=\"http://git.oschina.net/dockerf/docker-practice\">http://git.oschina.net/dockerf/docker-practice</a><br>    <a href=\"http://git.oschina.net/dockerf/docker-training\">http://git.oschina.net/dockerf/docker-training</a><br>    mooc:<a href=\"https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS151.x+2T2016/info\">https://courses.edx.org/courses/course-v1:LinuxFoundationX+LFS151.x+2T2016/info</a></p>\n<p>read:</p>\n<ul>\n<li>infoq: <a href=\"http://www.infoq.com/cn/dockers/\">infoq cn</a>, <a href=\"https://www.infoq.com/docker-2\">infoq en</a></li>\n<li>bot: <a href=\"http://www.yidianzixun.com/home?page=channel&amp;keyword=docker\">yidian</a>, <a href=\"http://toutiao.com/tag85482990/\">toutiao</a></li>\n<li>book<br>  Docker Cookbook<br>  Docker技术入门与实战:publish,<a href=\"http://dockerpool.com/static/books/docker_practice/index.html\">opensource</a><br>  Docker源码分析<br>  Docker技术详解与实践 lts<br>  <a href=\"http://yuedu.baidu.com/ebook/d817967416fc700abb68fca1\">docker入门实战</a><br>  <a href=\"http://adetante.github.io/articles/service-discovery-with-docker-1/\">Service discovery with Docker</a></li>\n<li>tut<br>  <a href=\"http://www.alauda.cn/tutorial/\"></a><br>  <a href=\"http://help.daocloud.io/\"></a><br>  <a href=\"https://coreos.com/os/docs/latest/quickstart.html\"></a>        </li>\n<li>course<br>  <a href=\"https://training.docker.com/self-paced-training\">https://training.docker.com/self-paced-training</a><br>  <a href=\"http://www.infoq.com/cn/presentations/antgroup-financial-cloud-paas-docker-practice\">http://www.infoq.com/cn/presentations/antgroup-financial-cloud-paas-docker-practice</a><br>languages:</li>\n<li>php<br>  <a href=\"https://github.com/schmunk42/docker-yii2-app-basic\">https://github.com/schmunk42/docker-yii2-app-basic</a><br>  <a href=\"https://github.com/eko/docker-symfony\">https://github.com/eko/docker-symfony</a><br>  <a href=\"https://github.com/harshjv/docker-laravel\">https://github.com/harshjv/docker-laravel</a><br>  <a href=\"https://github.com/tkyk/docker-compose-lamp\">https://github.com/tkyk/docker-compose-lamp</a><br>  <a href=\"https://github.com/docker-library/php\">https://github.com/docker-library/php</a><br>  compose<br>  <a href=\"https://github.com/DockerNuts/docker-compose-php-mysql/blob/master/docker-compose.yml\">https://github.com/DockerNuts/docker-compose-php-mysql/blob/master/docker-compose.yml</a><br>  <a href=\"https://github.com/tkyk/docker-compose-lamp\">https://github.com/tkyk/docker-compose-lamp</a><br>  <a href=\"https://github.com/larryprice/docker-compose-example\">https://github.com/larryprice/docker-compose-example</a></li>\n<li>py<br>  <a href=\"https://github.com/dockerfiles/django-uwsgi-nginx\">https://github.com/dockerfiles/django-uwsgi-nginx</a><br>  <a href=\"https://github.com/mbentley/docker-django-uwsgi-nginx\">https://github.com/mbentley/docker-django-uwsgi-nginx</a><br>  <a href=\"https://github.com/dockerfiles/django-uwsgi-nginx\">https://github.com/dockerfiles/django-uwsgi-nginx</a><br>  django:<a href=\"https://docs.docker.com/compose/django/\">https://docs.docker.com/compose/django/</a><br>  flask:dockercook 3.1</li>\n<li>ruby<br>  rails:<a href=\"https://docs.docker.com/compose/rails/\">https://docs.docker.com/compose/rails/</a></li>\n<li>js<br>  <a href=\"https://github.com/b00giZm/docker-compose-nodejs-examples/tree/master/01-express-nodemon\">https://github.com/b00giZm/docker-compose-nodejs-examples/tree/master/01-express-nodemon</a></li>\n<li>other<br>  <a href=\"https://github.com/docker/compose/blob/master/SWARM.md\">https://github.com/docker/compose/blob/master/SWARM.md</a><br>  wordpress:<a href=\"https://docs.docker.com/compose/wordpress/\">https://docs.docker.com/compose/wordpress/</a></li>\n</ul>\n<p>12 factor<br>    <a href=\"http://www.the12factorapp.com/\">http://www.the12factorapp.com/</a><br>    <a href=\"https://12factor.net/\">https://12factor.net/</a><br>    <a href=\"https://chixq.com/articles/12-factor-app/\">https://chixq.com/articles/12-factor-app/</a></p>\n"},{"title":"docker compose","_content":"\n# about\n\ncode: https://github.com/docker/compose\ndoc: https://docs.docker.com/compose/\nin production: https://docs.docker.com/compose/production/\n\norchestration 官方编排工具, 用于将一个多容器应用编排成一个单一应用\nFig工具的替代品: [fig](http://www.fig.sh/)可以快速搭建开发环境,通过YAML文件管理多个容器;\nFit cmd: add fig.yml; fig up\n\n# install\n\n[link](http://docs.docker.com/compose/install/)\n            \n        docker-compose --version\n\n    64bits Linux or MacOS X:\n\n            curl -L https://github.com/docker/compose/releases/download/1.1.0/docker-compose-`uname -s`-`uname -m` > /usr/local/bin/docker-compose\n            chmod +x /usr/local/bin/docker-compose\n    \n    win and other:\n\n            sudo pip install -U docker-compose\n# command\n\n- docker-compose up -d\n\n- docker exec\n    \n    docker exec -it example_web_1 bash\n\n- docker-compose stop && docker-compose rm --force\n\n- docker-compose build\n    Build or rebuild services\n\n- docker-compose help\n\n- docker-compose kill\n    Kill containers 通过发送 SIGKILL 信号来强制停止服务容器\n    支持通过参数来指定发送的信号\n        docker-compose kill -s SIGINT\n\n- docker-compose logs\n    View output from containers\n\n- docker-compose port\n    Print the public port for a port binding\n\n- docker-compose ps\n    List containers\n\n- docker-compose pull\n    Pulls service images\n\n- docker-compose rm\n    Remove stopped containers\n\n- docker-compose run\n\n    Run a one-off command 在一个服务上执行一个命令\n    docker-compose run ubuntu ping docker.com\n        将会启动一个 ubuntu 服务，执行 ping docker.com 命令\n    默认情况下，所有关联的服务将会自动被启动，除非这些服务已经在运行中。\n    该命令类似启动容器后运行指定的命令，相关卷、链接等等都将会按照期望创建。\n    两个不同点：\n        给定命令将会覆盖原有的自动运行命令；\n        不会自动创建端口，以避免冲突。\n    如果不希望自动启动关联的容器，可以使用 --no-deps 选项\n        docker-compose run --no-deps web python manage.py shell\n        将不会启动 web 容器所关联的其它容器\n\n- docker-compose scale\n\n    Set number of containers for a service 设置同一个服务运行的容器个数\n    service=num\n    docker-compose scale web=2 worker=3\n\n- docker-compose start: Start services\n\n- docker-compose stop: Stop services\n\n- docker-compose restart\n\n    Restart services\n        env variable\n\n            COMPOSE_PROJECT_NAME\n                设置通过 Compose 启动的每一个容器前添加的项目名称，默认是当前工作目录的名字。\n            COMPOSE_FILE\n                设置要使用的 docker-compose.yml 的路径。默认路径是当前工作目录。\n            DOCKER_HOST\n                设置 Docker daemon 的地址。默认使用 unix:///var/run/docker.sock，与 Docker 客户端采用的默认值一致。\n            DOCKER_TLS_VERIFY\n                如果设置不为空，则与 Docker daemon 交互通过 TLS 进行。\n            DOCKER_CERT_PATH\n                配置 TLS 通信所需要的验证（ca.pem、cert.pem 和 key.pem）文件的路径，默认是 ~/.docker 。\n\n- docker-compose up\n\n    Create and start containers\n        $ docker-compose up -d\n\n- docker-compose logs\n\n- docker-compose version\n\n    Show the Docker-Compose version information\n\n- docker-compose unpause\n\n    Unpause services\n\n- docker-compose migrate-to-labels\n\n    Recreate containers to add labels\n   ","source":"_posts/docker-compose-detail.md","raw":"---\ntitle: docker compose\ncategories:\n- docker\ntags:\n- detail\n---\n\n# about\n\ncode: https://github.com/docker/compose\ndoc: https://docs.docker.com/compose/\nin production: https://docs.docker.com/compose/production/\n\norchestration 官方编排工具, 用于将一个多容器应用编排成一个单一应用\nFig工具的替代品: [fig](http://www.fig.sh/)可以快速搭建开发环境,通过YAML文件管理多个容器;\nFit cmd: add fig.yml; fig up\n\n# install\n\n[link](http://docs.docker.com/compose/install/)\n            \n        docker-compose --version\n\n    64bits Linux or MacOS X:\n\n            curl -L https://github.com/docker/compose/releases/download/1.1.0/docker-compose-`uname -s`-`uname -m` > /usr/local/bin/docker-compose\n            chmod +x /usr/local/bin/docker-compose\n    \n    win and other:\n\n            sudo pip install -U docker-compose\n# command\n\n- docker-compose up -d\n\n- docker exec\n    \n    docker exec -it example_web_1 bash\n\n- docker-compose stop && docker-compose rm --force\n\n- docker-compose build\n    Build or rebuild services\n\n- docker-compose help\n\n- docker-compose kill\n    Kill containers 通过发送 SIGKILL 信号来强制停止服务容器\n    支持通过参数来指定发送的信号\n        docker-compose kill -s SIGINT\n\n- docker-compose logs\n    View output from containers\n\n- docker-compose port\n    Print the public port for a port binding\n\n- docker-compose ps\n    List containers\n\n- docker-compose pull\n    Pulls service images\n\n- docker-compose rm\n    Remove stopped containers\n\n- docker-compose run\n\n    Run a one-off command 在一个服务上执行一个命令\n    docker-compose run ubuntu ping docker.com\n        将会启动一个 ubuntu 服务，执行 ping docker.com 命令\n    默认情况下，所有关联的服务将会自动被启动，除非这些服务已经在运行中。\n    该命令类似启动容器后运行指定的命令，相关卷、链接等等都将会按照期望创建。\n    两个不同点：\n        给定命令将会覆盖原有的自动运行命令；\n        不会自动创建端口，以避免冲突。\n    如果不希望自动启动关联的容器，可以使用 --no-deps 选项\n        docker-compose run --no-deps web python manage.py shell\n        将不会启动 web 容器所关联的其它容器\n\n- docker-compose scale\n\n    Set number of containers for a service 设置同一个服务运行的容器个数\n    service=num\n    docker-compose scale web=2 worker=3\n\n- docker-compose start: Start services\n\n- docker-compose stop: Stop services\n\n- docker-compose restart\n\n    Restart services\n        env variable\n\n            COMPOSE_PROJECT_NAME\n                设置通过 Compose 启动的每一个容器前添加的项目名称，默认是当前工作目录的名字。\n            COMPOSE_FILE\n                设置要使用的 docker-compose.yml 的路径。默认路径是当前工作目录。\n            DOCKER_HOST\n                设置 Docker daemon 的地址。默认使用 unix:///var/run/docker.sock，与 Docker 客户端采用的默认值一致。\n            DOCKER_TLS_VERIFY\n                如果设置不为空，则与 Docker daemon 交互通过 TLS 进行。\n            DOCKER_CERT_PATH\n                配置 TLS 通信所需要的验证（ca.pem、cert.pem 和 key.pem）文件的路径，默认是 ~/.docker 。\n\n- docker-compose up\n\n    Create and start containers\n        $ docker-compose up -d\n\n- docker-compose logs\n\n- docker-compose version\n\n    Show the Docker-Compose version information\n\n- docker-compose unpause\n\n    Unpause services\n\n- docker-compose migrate-to-labels\n\n    Recreate containers to add labels\n   ","slug":"docker-compose-detail","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-02-10T15:03:07.000Z","_id":"ciypwm38h001f21sv8w2usj1k","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><p>code: <a href=\"https://github.com/docker/compose\" target=\"_blank\" rel=\"external\">https://github.com/docker/compose</a><br>doc: <a href=\"https://docs.docker.com/compose/\" target=\"_blank\" rel=\"external\">https://docs.docker.com/compose/</a><br>in production: <a href=\"https://docs.docker.com/compose/production/\" target=\"_blank\" rel=\"external\">https://docs.docker.com/compose/production/</a></p>\n<p>orchestration 官方编排工具, 用于将一个多容器应用编排成一个单一应用<br>Fig工具的替代品: <a href=\"http://www.fig.sh/\" target=\"_blank\" rel=\"external\">fig</a>可以快速搭建开发环境,通过YAML文件管理多个容器;<br>Fit cmd: add fig.yml; fig up</p>\n<h1 id=\"install\"><a href=\"#install\" class=\"headerlink\" title=\"install\"></a>install</h1><p><a href=\"http://docs.docker.com/compose/install/\" target=\"_blank\" rel=\"external\">link</a></p>\n<pre><code>    docker-compose --version\n\n64bits Linux or MacOS X:\n\n        curl -L https://github.com/docker/compose/releases/download/1.1.0/docker-compose-`uname -s`-`uname -m` &gt; /usr/local/bin/docker-compose\n        chmod +x /usr/local/bin/docker-compose\n\nwin and other:\n\n        sudo pip install -U docker-compose\n</code></pre><h1 id=\"command\"><a href=\"#command\" class=\"headerlink\" title=\"command\"></a>command</h1><ul>\n<li><p>docker-compose up -d</p>\n</li>\n<li><p>docker exec</p>\n<p>  docker exec -it example_web_1 bash</p>\n</li>\n<li><p>docker-compose stop &amp;&amp; docker-compose rm –force</p>\n</li>\n<li><p>docker-compose build<br>  Build or rebuild services</p>\n</li>\n<li><p>docker-compose help</p>\n</li>\n<li><p>docker-compose kill<br>  Kill containers 通过发送 SIGKILL 信号来强制停止服务容器<br>  支持通过参数来指定发送的信号</p>\n<pre><code>docker-compose kill -s SIGINT\n</code></pre></li>\n<li><p>docker-compose logs<br>  View output from containers</p>\n</li>\n<li><p>docker-compose port<br>  Print the public port for a port binding</p>\n</li>\n<li><p>docker-compose ps<br>  List containers</p>\n</li>\n<li><p>docker-compose pull<br>  Pulls service images</p>\n</li>\n<li><p>docker-compose rm<br>  Remove stopped containers</p>\n</li>\n<li><p>docker-compose run</p>\n<p>  Run a one-off command 在一个服务上执行一个命令<br>  docker-compose run ubuntu ping docker.com</p>\n<pre><code>将会启动一个 ubuntu 服务，执行 ping docker.com 命令\n</code></pre><p>  默认情况下，所有关联的服务将会自动被启动，除非这些服务已经在运行中。<br>  该命令类似启动容器后运行指定的命令，相关卷、链接等等都将会按照期望创建。<br>  两个不同点：</p>\n<pre><code>给定命令将会覆盖原有的自动运行命令；\n不会自动创建端口，以避免冲突。\n</code></pre><p>  如果不希望自动启动关联的容器，可以使用 –no-deps 选项</p>\n<pre><code>docker-compose run --no-deps web python manage.py shell\n将不会启动 web 容器所关联的其它容器\n</code></pre></li>\n<li><p>docker-compose scale</p>\n<p>  Set number of containers for a service 设置同一个服务运行的容器个数<br>  service=num<br>  docker-compose scale web=2 worker=3</p>\n</li>\n<li><p>docker-compose start: Start services</p>\n</li>\n<li><p>docker-compose stop: Stop services</p>\n</li>\n<li><p>docker-compose restart</p>\n<p>  Restart services</p>\n<pre><code>env variable\n\n    COMPOSE_PROJECT_NAME\n        设置通过 Compose 启动的每一个容器前添加的项目名称，默认是当前工作目录的名字。\n    COMPOSE_FILE\n        设置要使用的 docker-compose.yml 的路径。默认路径是当前工作目录。\n    DOCKER_HOST\n        设置 Docker daemon 的地址。默认使用 unix:///var/run/docker.sock，与 Docker 客户端采用的默认值一致。\n    DOCKER_TLS_VERIFY\n        如果设置不为空，则与 Docker daemon 交互通过 TLS 进行。\n    DOCKER_CERT_PATH\n        配置 TLS 通信所需要的验证（ca.pem、cert.pem 和 key.pem）文件的路径，默认是 ~/.docker 。\n</code></pre></li>\n<li><p>docker-compose up</p>\n<p>  Create and start containers</p>\n<pre><code>$ docker-compose up -d\n</code></pre></li>\n<li><p>docker-compose logs</p>\n</li>\n<li><p>docker-compose version</p>\n<p>  Show the Docker-Compose version information</p>\n</li>\n<li><p>docker-compose unpause</p>\n<p>  Unpause services</p>\n</li>\n<li><p>docker-compose migrate-to-labels</p>\n<p>  Recreate containers to add labels</p>\n</li>\n</ul>\n","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><p>code: <a href=\"https://github.com/docker/compose\">https://github.com/docker/compose</a><br>doc: <a href=\"https://docs.docker.com/compose/\">https://docs.docker.com/compose/</a><br>in production: <a href=\"https://docs.docker.com/compose/production/\">https://docs.docker.com/compose/production/</a></p>\n<p>orchestration 官方编排工具, 用于将一个多容器应用编排成一个单一应用<br>Fig工具的替代品: <a href=\"http://www.fig.sh/\">fig</a>可以快速搭建开发环境,通过YAML文件管理多个容器;<br>Fit cmd: add fig.yml; fig up</p>\n<h1 id=\"install\"><a href=\"#install\" class=\"headerlink\" title=\"install\"></a>install</h1><p><a href=\"http://docs.docker.com/compose/install/\">link</a></p>\n<pre><code>    docker-compose --version\n\n64bits Linux or MacOS X:\n\n        curl -L https://github.com/docker/compose/releases/download/1.1.0/docker-compose-`uname -s`-`uname -m` &gt; /usr/local/bin/docker-compose\n        chmod +x /usr/local/bin/docker-compose\n\nwin and other:\n\n        sudo pip install -U docker-compose\n</code></pre><h1 id=\"command\"><a href=\"#command\" class=\"headerlink\" title=\"command\"></a>command</h1><ul>\n<li><p>docker-compose up -d</p>\n</li>\n<li><p>docker exec</p>\n<p>  docker exec -it example_web_1 bash</p>\n</li>\n<li><p>docker-compose stop &amp;&amp; docker-compose rm –force</p>\n</li>\n<li><p>docker-compose build<br>  Build or rebuild services</p>\n</li>\n<li><p>docker-compose help</p>\n</li>\n<li><p>docker-compose kill<br>  Kill containers 通过发送 SIGKILL 信号来强制停止服务容器<br>  支持通过参数来指定发送的信号</p>\n<pre><code>docker-compose kill -s SIGINT\n</code></pre></li>\n<li><p>docker-compose logs<br>  View output from containers</p>\n</li>\n<li><p>docker-compose port<br>  Print the public port for a port binding</p>\n</li>\n<li><p>docker-compose ps<br>  List containers</p>\n</li>\n<li><p>docker-compose pull<br>  Pulls service images</p>\n</li>\n<li><p>docker-compose rm<br>  Remove stopped containers</p>\n</li>\n<li><p>docker-compose run</p>\n<p>  Run a one-off command 在一个服务上执行一个命令<br>  docker-compose run ubuntu ping docker.com</p>\n<pre><code>将会启动一个 ubuntu 服务，执行 ping docker.com 命令\n</code></pre><p>  默认情况下，所有关联的服务将会自动被启动，除非这些服务已经在运行中。<br>  该命令类似启动容器后运行指定的命令，相关卷、链接等等都将会按照期望创建。<br>  两个不同点：</p>\n<pre><code>给定命令将会覆盖原有的自动运行命令；\n不会自动创建端口，以避免冲突。\n</code></pre><p>  如果不希望自动启动关联的容器，可以使用 –no-deps 选项</p>\n<pre><code>docker-compose run --no-deps web python manage.py shell\n将不会启动 web 容器所关联的其它容器\n</code></pre></li>\n<li><p>docker-compose scale</p>\n<p>  Set number of containers for a service 设置同一个服务运行的容器个数<br>  service=num<br>  docker-compose scale web=2 worker=3</p>\n</li>\n<li><p>docker-compose start: Start services</p>\n</li>\n<li><p>docker-compose stop: Stop services</p>\n</li>\n<li><p>docker-compose restart</p>\n<p>  Restart services</p>\n<pre><code>env variable\n\n    COMPOSE_PROJECT_NAME\n        设置通过 Compose 启动的每一个容器前添加的项目名称，默认是当前工作目录的名字。\n    COMPOSE_FILE\n        设置要使用的 docker-compose.yml 的路径。默认路径是当前工作目录。\n    DOCKER_HOST\n        设置 Docker daemon 的地址。默认使用 unix:///var/run/docker.sock，与 Docker 客户端采用的默认值一致。\n    DOCKER_TLS_VERIFY\n        如果设置不为空，则与 Docker daemon 交互通过 TLS 进行。\n    DOCKER_CERT_PATH\n        配置 TLS 通信所需要的验证（ca.pem、cert.pem 和 key.pem）文件的路径，默认是 ~/.docker 。\n</code></pre></li>\n<li><p>docker-compose up</p>\n<p>  Create and start containers</p>\n<pre><code>$ docker-compose up -d\n</code></pre></li>\n<li><p>docker-compose logs</p>\n</li>\n<li><p>docker-compose version</p>\n<p>  Show the Docker-Compose version information</p>\n</li>\n<li><p>docker-compose unpause</p>\n<p>  Unpause services</p>\n</li>\n<li><p>docker-compose migrate-to-labels</p>\n<p>  Recreate containers to add labels</p>\n</li>\n</ul>\n"},{"title":"docker compose","_content":"\n# about\n\n- official: https://docs.docker.com/compose/compose-file/\n- django example: https://docs.docker.com/compose/django/\n- rails example: https://docs.docker.com/compose/rails/\n- machine, swarm, compose: https://blog.docker.com/2015/02/orchestrating-docker-with-machine-swarm-and-compose/\n\nThe Compose file is a YAML file defining services, networks and volumes. The default path for a Compose file is ./docker-compose.yml.\n\nA service definition contains configuration which will be applied to each container started for that service, much like passing command-line parameters to docker run. Likewise, network and volume definitions are analogous to docker network create and docker volume create.\n\nAs with docker run, options specified in the Dockerfile (e.g., CMD, EXPOSE, VOLUME, ENV) are respected by default - you don’t need to specify them again in docker-compose.yml.\n\n# service configuration reference\n\n- build\n\n# volume configuration reference\n\n# network configuration reference\n\n# versioning\n\n# variable substitution","source":"_posts/docker-compose-file-detail.md","raw":"---\ntitle: docker compose\ncategories:\n- docker\ntags:\n- detail\n---\n\n# about\n\n- official: https://docs.docker.com/compose/compose-file/\n- django example: https://docs.docker.com/compose/django/\n- rails example: https://docs.docker.com/compose/rails/\n- machine, swarm, compose: https://blog.docker.com/2015/02/orchestrating-docker-with-machine-swarm-and-compose/\n\nThe Compose file is a YAML file defining services, networks and volumes. The default path for a Compose file is ./docker-compose.yml.\n\nA service definition contains configuration which will be applied to each container started for that service, much like passing command-line parameters to docker run. Likewise, network and volume definitions are analogous to docker network create and docker volume create.\n\nAs with docker run, options specified in the Dockerfile (e.g., CMD, EXPOSE, VOLUME, ENV) are respected by default - you don’t need to specify them again in docker-compose.yml.\n\n# service configuration reference\n\n- build\n\n# volume configuration reference\n\n# network configuration reference\n\n# versioning\n\n# variable substitution","slug":"docker-compose-file-detail","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-18T07:33:14.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm38l001k21svmq9vzpvt","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><ul>\n<li>official: <a href=\"https://docs.docker.com/compose/compose-file/\" target=\"_blank\" rel=\"external\">https://docs.docker.com/compose/compose-file/</a></li>\n<li>django example: <a href=\"https://docs.docker.com/compose/django/\" target=\"_blank\" rel=\"external\">https://docs.docker.com/compose/django/</a></li>\n<li>rails example: <a href=\"https://docs.docker.com/compose/rails/\" target=\"_blank\" rel=\"external\">https://docs.docker.com/compose/rails/</a></li>\n<li>machine, swarm, compose: <a href=\"https://blog.docker.com/2015/02/orchestrating-docker-with-machine-swarm-and-compose/\" target=\"_blank\" rel=\"external\">https://blog.docker.com/2015/02/orchestrating-docker-with-machine-swarm-and-compose/</a></li>\n</ul>\n<p>The Compose file is a YAML file defining services, networks and volumes. The default path for a Compose file is ./docker-compose.yml.</p>\n<p>A service definition contains configuration which will be applied to each container started for that service, much like passing command-line parameters to docker run. Likewise, network and volume definitions are analogous to docker network create and docker volume create.</p>\n<p>As with docker run, options specified in the Dockerfile (e.g., CMD, EXPOSE, VOLUME, ENV) are respected by default - you don’t need to specify them again in docker-compose.yml.</p>\n<h1 id=\"service-configuration-reference\"><a href=\"#service-configuration-reference\" class=\"headerlink\" title=\"service configuration reference\"></a>service configuration reference</h1><ul>\n<li>build</li>\n</ul>\n<h1 id=\"volume-configuration-reference\"><a href=\"#volume-configuration-reference\" class=\"headerlink\" title=\"volume configuration reference\"></a>volume configuration reference</h1><h1 id=\"network-configuration-reference\"><a href=\"#network-configuration-reference\" class=\"headerlink\" title=\"network configuration reference\"></a>network configuration reference</h1><h1 id=\"versioning\"><a href=\"#versioning\" class=\"headerlink\" title=\"versioning\"></a>versioning</h1><h1 id=\"variable-substitution\"><a href=\"#variable-substitution\" class=\"headerlink\" title=\"variable substitution\"></a>variable substitution</h1>","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><ul>\n<li>official: <a href=\"https://docs.docker.com/compose/compose-file/\">https://docs.docker.com/compose/compose-file/</a></li>\n<li>django example: <a href=\"https://docs.docker.com/compose/django/\">https://docs.docker.com/compose/django/</a></li>\n<li>rails example: <a href=\"https://docs.docker.com/compose/rails/\">https://docs.docker.com/compose/rails/</a></li>\n<li>machine, swarm, compose: <a href=\"https://blog.docker.com/2015/02/orchestrating-docker-with-machine-swarm-and-compose/\">https://blog.docker.com/2015/02/orchestrating-docker-with-machine-swarm-and-compose/</a></li>\n</ul>\n<p>The Compose file is a YAML file defining services, networks and volumes. The default path for a Compose file is ./docker-compose.yml.</p>\n<p>A service definition contains configuration which will be applied to each container started for that service, much like passing command-line parameters to docker run. Likewise, network and volume definitions are analogous to docker network create and docker volume create.</p>\n<p>As with docker run, options specified in the Dockerfile (e.g., CMD, EXPOSE, VOLUME, ENV) are respected by default - you don’t need to specify them again in docker-compose.yml.</p>\n<h1 id=\"service-configuration-reference\"><a href=\"#service-configuration-reference\" class=\"headerlink\" title=\"service configuration reference\"></a>service configuration reference</h1><ul>\n<li>build</li>\n</ul>\n<h1 id=\"volume-configuration-reference\"><a href=\"#volume-configuration-reference\" class=\"headerlink\" title=\"volume configuration reference\"></a>volume configuration reference</h1><h1 id=\"network-configuration-reference\"><a href=\"#network-configuration-reference\" class=\"headerlink\" title=\"network configuration reference\"></a>network configuration reference</h1><h1 id=\"versioning\"><a href=\"#versioning\" class=\"headerlink\" title=\"versioning\"></a>versioning</h1><h1 id=\"variable-substitution\"><a href=\"#variable-substitution\" class=\"headerlink\" title=\"variable substitution\"></a>variable substitution</h1>"},{"_content":"    ```\n        containers:\n        web:\n         build: .\n         command: python app.py\n         ports:\n         - \"5000:5000\"\n         volumes:\n         - .:/code\n         links:\n         - redis\n         environment:\n         - PYTHONUNBUFFERED=1\n        redis:\n         image: redis:latest\n         command: redis-server --appendonly yes\n     ```\n\n     ```\n        wiki2:\n        image: 'nickstenning/mediawiki'\n        ports:\n            - \"8880:80\"\n        links:\n            - db:database\n        volumes:\n            - /data/wiki2:/data\n\n        db:\n        image: \"mysql\"\n        expose:\n            - \"3306\"\n        environment:\n            - MYSQL_ROOT_PASSWORD=defaultpass\n\n    上面的YAML文件定义了两个容器应用，第一个容器运行Python应用，并通过当前目录的Dockerfile文件构建。\n    第二个容器是从Docker Hub注册中心的Redis官方仓库中构建。links指令用来定义依赖，意思是Python应用依赖于Redis应用。\n    定义完成后，通过下面的命令来启动应用：\n\n    docker-compose up\n\n    links指令关注的是Python和Redis容器之间的依赖关系，Redis容器是最先开始构建，紧随其后的是Python容器。\n\n# Variable substitution\n            \nBoth $VARIABLE and ${VARIABLE} syntax are supported. \nExtended shell-style features, such as ${VARIABLE-default} and ${VARIABLE/foo/bar}, are not supported.\n            db:\n  image: \"postgres:${POSTGRES_VERSION}\"\n            web:\n  build: .\n  command: \"$$VAR_NOT_INTERPOLATED_BY_COMPOSE\"\n        \n# install\n\n    curl -L https://raw.githubusercontent.com/docker/compose/$(docker-compose --version | awk 'NR==1{print $NF}')/contrib/completion/bash/docker-compose > /etc/bash_completion.d/docker-compose\n        doc\n            docker-compose.yml reference\n                https://docs.docker.com/compose/yml/\n        keywords\n            image\n                指定为镜像名称或镜像 ID。如果镜像在本地不存在，Compose 将会尝试拉去这个镜像\n                image: ubuntu\n            build\n                指定 Dockerfile 所在文件夹的路径\n                Compose 将会利用它自动构建这个镜像，然后使用这个镜像\n                build: /path/to/build/dir\n            dockerfile\n            command\n                覆盖容器启动后默认执行的命令。\n                command: bundle exec thin -p 3000\n            links\n                链接到其它服务中的容器\n                使用服务名称（同时作为别名）或服务名称：服务别名 （SERVICE:ALIAS） 格式都可以\n                    links:\n - db\n - db:database\n - redis\n                使用的别名将会自动在服务容器中的 /etc/hosts 里创建,相应的环境变量也将被创建\n                    172.17.2.186  db\n172.17.2.186  database\n172.17.2.187  redis\n            external_links\n                链接到 docker-compose.yml 外部的容器\n                甚至 并非 Compose 管理的容器。参数格式跟 links 类似\n                external_links:\n - redis_1\n - project_db_1:mysql\n - project_db_1:postgresql\n            extra_hosts\n            ports\n                暴露端口信息\n                使用宿主：容器 （HOST:CONTAINER）格式或者仅仅指定容器的端口（宿主将会随机选择端口）都可以\n                ports:\n - \"3000\"\n - \"8000:8000\"\n - \"49100:22\"\n - \"127.0.0.1:8001:8001\"\n            expose\n                暴露端口，但不映射到宿主机，只被连接的服务访问\n                仅可以指定内部端口为参数\n                expose:\n - \"3000\"\n - \"8000\"\n            volumes\n                卷挂载路径设置\n                可以设置宿主机路径 （HOST:CONTAINER） 或加上访问模式 （HOST:CONTAINER:ro）\n                volumes:\n - /var/lib/mysql\n - cache/:/tmp/cache\n - ~/configs:/etc/configs/:ro\n            volumes_from\n                从另一个服务或容器挂载它的所有卷\n                volumes_from:\n - service_name\n - container_name\n            environment\n                设置环境变量。你可以使用数组或字典两种格式。\n                只给定名称的变量会自动获取它在 Compose 主机上的值，可以用来防止泄露不必要的数据\n                environment:\n  RACK_ENV: development\n  SESSION_SECRET:\n\nenvironment:\n  - RACK_ENV=development\n  - SESSION_SECRET\n            env_file\n                从文件中获取环境变量，可以为单独的文件路径或列表。\n                如果通过 docker-compose -f FILE 指定了模板文件，则 env_file 中路径会基于模板文件路径。\n                如果有变量名称与 environment 指令冲突，则以后者为准。\n                env_file: .env\n\nenv_file:\n  - ./common.env\n  - ./apps/web.env\n  - /opt/secrets.env\n                环境变量文件中每一行必须符合格式，支持 # 开头的注释行\n                # common.env: Set Rails/Rack environment\nRACK_ENV=development\n            extends\n                基于已有的服务进行扩展\n                例如我们已经有了一个 webapp 服务，模板文件为 common.yml\n                # common.yml\nwebapp:\n  build: ./webapp\n  environment:\n    - DEBUG=false\n    - SEND_EMAILS=false\n                编写一个新的 development.yml 文件，使用 common.yml 中的 webapp 服务进行扩展\n                # development.yml\nweb:\n  extends:\n    file: common.yml\n    service: webapp\n  ports:\n    - \"8000:8000\"\n  links:\n    - db\n  environment:\n    - DEBUG=true\ndb:\n  image: postgres\n                后者会自动继承 common.yml 中的 webapp 服务及相关环节变量\n            labels\n            container_name\n            log driver\n            net\n                设置网络模式\n                使用和 docker client 的 --net 参数一样的值\n                net: \"bridge\"\nnet: \"none\"\nnet: \"container:[name or id]\"\nnet: \"host\"\n            pid\n                跟主机系统共享进程命名空间\n                打开该选项的容器可以相互通过进程 ID 来访问和操作\n                pid: \"host\"\n            dns\n                配置 DNS 服务器\n                可以是一个值，也可以是一个列表\n                dns: 8.8.8.8\n- dns:\n\n  - 8.8.8.8\n  - 9.9.9.9\n            cap_add, cap_drop\n                添加或放弃容器的 Linux 能力（Capabiliity）\n                cap_add:\n  - ALL\n\n- cap_drop:\n\n  - NET_ADMIN\n  - SYS_ADMIN\n            dns_search\n                配置 DNS 搜索域\n                可以是一个值，也可以是一个列表\n                dns_search: example.com\n- dns_search:\n\n  - domain1.example.com\n  - domain2.example.com\n            devices\n            security_opt\n            working_dir, entrypoint, user, hostname, domainname, \nmac_address, mem_limit, memswap_limit, privileged, \nrestart, stdin_open, tty, cpu_shares, cpuset, \nread_only, volume_driver\n                这些都是和 docker run 支持的选项类似\n                cpu_shares: 73\n\nworking_dir: /code\nentrypoint: /code/entrypoint.sh\nuser: postgresql\n\nhostname: foo\ndomainname: foo.com\n\nmem_limit: 1000000000\nprivileged: true\n\nrestart: always\n\nstdin_open: true\ntty: true\n        keyword\n            build\n                Path to a directory containing a Dockerfile.\n                build: /path/to/build/dir\n            cap_add, cap_drop\n                Add or drop container capabilities. See man 7 capabilities for a full list.\n                cap_add:\n  - ALL\n\ncap_drop:\n  - NET_ADMIN\n  - SYS_ADMIN\n            command\n                Override the default command.\n                command: bundle exec thin -p 3000\n            cgroup_parent\n                Specify an optional parent cgroup for the container.\n                cgroup_parent: m-executor-abcd\n            container_name\n                Specify a custom container name, rather than a generated default name.\n                container_name: my-web-container\n            devices\n                List of device mappings. Uses the same format as the --device docker client create option.\n                devices:\n  - \"/dev/ttyUSB0:/dev/ttyUSB0\"\n            dns\n                Custom DNS servers. Can be a single value or a list.\n                dns: 8.8.8.8\ndns:\n  - 8.8.8.8\n  - 9.9.9.9\n            dns_search\n                Custom DNS search domains. Can be a single value or a list.\n                dns_search: example.com\ndns_search:\n  - dc1.example.com\n  - dc2.example.com\n            dockerfile\n                Alternate Dockerfile.\nCompose will use an alternate file to build with.\n                dockerfile: Dockerfile-alternate\n            env_file\n                Add environment variables from a file. Can be a single value or a list.\nIf you have specified a Compose file with docker-compose -f FILE, \npaths in env_file are relative to the directory that file is in.\nEnvironment variables specified in environment override these values.\n                env_file: .env\n\nenv_file:\n  - ./common.env\n  - ./apps/web.env\n  - /opt/secrets.env\n                Compose expects each line in an env file to be in VAR=VAL format.\n Lines beginning with # (i.e. comments) are ignored, as are blank lines.\n\n# Set Rails/Rack environment\nRACK_ENV=development\n            environment\n                Add environment variables. You can use either an array or a dictionary.\n                Any boolean values; true, false, yes no, need to be enclosed in quotes\n to ensure they are not converted to True or False by the YML parser.\n                Environment variables with only a key are resolved to their values on the machine Compose is running on, \nwhich can be helpful for secret or host-specific values.\n                environment:\n  RACK_ENV: development\n  SHOW: 'true'\n  SESSION_SECRET:\n\nenvironment:\n  - RACK_ENV=development\n  - SHOW=true\n  - SESSION_SECRET\n            expose\n                Expose ports without publishing them to the host machine - \nthey’ll only be accessible to linked services. Only the internal port can be specified.\n                expose:\n - \"3000\"\n - \"8000\"\n            extends\n                Extend another service, in the current file or another, optionally overriding configuration.\nYou can use extends on any service together with other configuration keys.\n The extends value must be a dictionary defined with a required service and an optional file key.\n                extends:\n  file: common.yml\n  service: webapp\n            external_links\n                Link to containers started outside this docker-compose.yml or even outside of Compose,\n especially for containers that provide shared or common services. \nexternal_links follow semantics similar to links when specifying both the container name and the link alias (CONTAINER:ALIAS).\n                external_links:\n - redis_1\n - project_db_1:mysql\n - project_db_1:postgresql\n            extra_hosts\n                Add hostname mappings. Use the same values as the docker client --add-host parameter.\n                extra_hosts:\n - \"somehost:162.242.195.82\"\n - \"otherhost:50.31.209.229\"\n                An entry with the ip address and hostname will be created in /etc/hosts inside containers for this service, e.g:\n\n162.242.195.82  somehost\n50.31.209.229   otherhost\n            image\n                Tag or partial image ID. Can be local or remote - \nCompose will attempt to pull if it doesn’t exist locally.\n                image: ubuntu\nimage: orchardup/postgresql\nimage: a4bc65fd\n            labels\n                Add metadata to containers using Docker labels. You can use either an array or a dictionary.\nIt’s recommended that you use reverse-DNS notation to prevent your labels from conflicting with those used by other software.\n                labels:\n  com.example.description: \"Accounting webapp\"\n  com.example.department: \"Finance\"\n  com.example.label-with-empty-value: \"\"\n\nlabels:\n  - \"com.example.description=Accounting webapp\"\n  - \"com.example.department=Finance\"\n  - \"com.example.label-with-empty-value\"\n            links\n                Link to containers in another service. \nEither specify both the service name and the link alias (SERVICE:ALIAS), \nor just the service name (which will also be used for the alias).\n                links:\n - db\n - db:database\n - redis\n                An entry with the alias’ name will be created in /etc/hosts \ninside containers for this service, e.g:\n\n172.17.2.186  db\n172.17.2.186  database\n172.17.2.187  redis\n            log_driver\n                Specify a logging driver for the service’s containers, as with the --log-driver option for docker run\n                The default value is json-file.\nNote: Only the json-file driver makes the logs available directly from docker-compose up and docker-compose logs. \nUsing any other driver will not print any logs.\n                log_driver: \"json-file\"\nlog_driver: \"syslog\"\nlog_driver: \"none\"\n            log_opt\n                Specify logging options with log_opt for the logging driver, as with the --log-opt option for docker run.\n                log_driver: \"syslog\"\nlog_opt:\n  syslog-address: \"tcp://192.168.0.42:123\"\n            net\n                Networking mode. Use the same values as the docker client --net parameter.\n                net: \"bridge\"\nnet: \"none\"\nnet: \"container:[name or id]\"\nnet: \"host\"\n            pid\n                pid: \"host\"\n                Sets the PID mode to the host PID mode. \nThis turns on sharing between container and the host oprating system the PID address space.\n Containers launched with this flag will be able to access and manipulate other containers\n in the bare-metal machine’s namespace and vise-versa.\n            ports\n                Expose ports. Either specify both ports (HOST:CONTAINER), \nor just the container port (a random host port will be chosen).\n                Note: When mapping ports in the HOST:CONTAINER format, \nyou may experience erroneous results when using a container port lower than 60, \nbecause YAML will parse numbers in the format xx:yy as sexagesimal (base 60). \nFor this reason, we recommend always explicitly specifying your port mappings as strings.\n                ports:\n - \"3000\"\n - \"3000-3005\"\n - \"8000:8000\"\n - \"9090-9091:8080-8081\"\n - \"49100:22\"\n - \"127.0.0.1:8001:8001\"\n - \"127.0.0.1:5000-5010:5000-5010\"\n            security_opt\n                Override the default labeling scheme for each container.\n                security_opt:\n    - label:user:USER\n    - label:role:ROLE\n            ulimits\n                Override the default ulimits for a container. \nYou can either specify a single limit as an integer or soft/hard limits as a mapping.\n                ulimits:\n    nproc: 65535\n    nofile:\n      soft: 20000\n      hard: 40000\n            volumes, volume_driver\n                Mount paths as volumes, optionally specifying a path on the host machine \n(HOST:CONTAINER), or an access mode (HOST:CONTAINER:ro).\n                volumes:\n - /var/lib/mysql\n - ./cache:/tmp/cache\n - ~/configs:/etc/configs/:ro\n                You can mount a relative path on the host, which will expand relative to the director\ny of the Compose configuration file being used. Relative paths should always begin with . or ...\n                If you use a volume name (instead of a volume path), you may also specify a volume_driver.\nvolume_driver: mydriver\nNote: No path expansion will be done if you have also specified a volume_driver.\n            volumes_from\n                Mount all of the volumes from another service or container, \noptionally specifying read-only access(ro) or read-write(rw).\n                volumes_from:\n - service_name\n - container_name\n - service_name:rw\n            cpu_shares, cpuset, domainname, entrypoint, hostname, \nipc, mac_address, mem_limit, memswap_limit, privileged, \nread_only, restart, stdin_open, tty, user, working_dir\n                Each of these is a single value, analogous to its docker run counterpart.\n                cpu_shares: 73\ncpuset: 0,1\n\nentrypoint: /code/entrypoint.sh\nuser: postgresql\nworking_dir: /code\n\ndomainname: foo.com\nhostname: foo\nipc: host\nmac_address: 02:42:ac:11:65:43\n\nmem_limit: 1000000000\nmemswap_limit: 2000000000\nprivileged: true\n\nrestart: always\n\nread_only: true\nstdin_open: true\ntty: true","source":"_posts/docker-compose-file-tmp.md","raw":"    ```\n        containers:\n        web:\n         build: .\n         command: python app.py\n         ports:\n         - \"5000:5000\"\n         volumes:\n         - .:/code\n         links:\n         - redis\n         environment:\n         - PYTHONUNBUFFERED=1\n        redis:\n         image: redis:latest\n         command: redis-server --appendonly yes\n     ```\n\n     ```\n        wiki2:\n        image: 'nickstenning/mediawiki'\n        ports:\n            - \"8880:80\"\n        links:\n            - db:database\n        volumes:\n            - /data/wiki2:/data\n\n        db:\n        image: \"mysql\"\n        expose:\n            - \"3306\"\n        environment:\n            - MYSQL_ROOT_PASSWORD=defaultpass\n\n    上面的YAML文件定义了两个容器应用，第一个容器运行Python应用，并通过当前目录的Dockerfile文件构建。\n    第二个容器是从Docker Hub注册中心的Redis官方仓库中构建。links指令用来定义依赖，意思是Python应用依赖于Redis应用。\n    定义完成后，通过下面的命令来启动应用：\n\n    docker-compose up\n\n    links指令关注的是Python和Redis容器之间的依赖关系，Redis容器是最先开始构建，紧随其后的是Python容器。\n\n# Variable substitution\n            \nBoth $VARIABLE and ${VARIABLE} syntax are supported. \nExtended shell-style features, such as ${VARIABLE-default} and ${VARIABLE/foo/bar}, are not supported.\n            db:\n  image: \"postgres:${POSTGRES_VERSION}\"\n            web:\n  build: .\n  command: \"$$VAR_NOT_INTERPOLATED_BY_COMPOSE\"\n        \n# install\n\n    curl -L https://raw.githubusercontent.com/docker/compose/$(docker-compose --version | awk 'NR==1{print $NF}')/contrib/completion/bash/docker-compose > /etc/bash_completion.d/docker-compose\n        doc\n            docker-compose.yml reference\n                https://docs.docker.com/compose/yml/\n        keywords\n            image\n                指定为镜像名称或镜像 ID。如果镜像在本地不存在，Compose 将会尝试拉去这个镜像\n                image: ubuntu\n            build\n                指定 Dockerfile 所在文件夹的路径\n                Compose 将会利用它自动构建这个镜像，然后使用这个镜像\n                build: /path/to/build/dir\n            dockerfile\n            command\n                覆盖容器启动后默认执行的命令。\n                command: bundle exec thin -p 3000\n            links\n                链接到其它服务中的容器\n                使用服务名称（同时作为别名）或服务名称：服务别名 （SERVICE:ALIAS） 格式都可以\n                    links:\n - db\n - db:database\n - redis\n                使用的别名将会自动在服务容器中的 /etc/hosts 里创建,相应的环境变量也将被创建\n                    172.17.2.186  db\n172.17.2.186  database\n172.17.2.187  redis\n            external_links\n                链接到 docker-compose.yml 外部的容器\n                甚至 并非 Compose 管理的容器。参数格式跟 links 类似\n                external_links:\n - redis_1\n - project_db_1:mysql\n - project_db_1:postgresql\n            extra_hosts\n            ports\n                暴露端口信息\n                使用宿主：容器 （HOST:CONTAINER）格式或者仅仅指定容器的端口（宿主将会随机选择端口）都可以\n                ports:\n - \"3000\"\n - \"8000:8000\"\n - \"49100:22\"\n - \"127.0.0.1:8001:8001\"\n            expose\n                暴露端口，但不映射到宿主机，只被连接的服务访问\n                仅可以指定内部端口为参数\n                expose:\n - \"3000\"\n - \"8000\"\n            volumes\n                卷挂载路径设置\n                可以设置宿主机路径 （HOST:CONTAINER） 或加上访问模式 （HOST:CONTAINER:ro）\n                volumes:\n - /var/lib/mysql\n - cache/:/tmp/cache\n - ~/configs:/etc/configs/:ro\n            volumes_from\n                从另一个服务或容器挂载它的所有卷\n                volumes_from:\n - service_name\n - container_name\n            environment\n                设置环境变量。你可以使用数组或字典两种格式。\n                只给定名称的变量会自动获取它在 Compose 主机上的值，可以用来防止泄露不必要的数据\n                environment:\n  RACK_ENV: development\n  SESSION_SECRET:\n\nenvironment:\n  - RACK_ENV=development\n  - SESSION_SECRET\n            env_file\n                从文件中获取环境变量，可以为单独的文件路径或列表。\n                如果通过 docker-compose -f FILE 指定了模板文件，则 env_file 中路径会基于模板文件路径。\n                如果有变量名称与 environment 指令冲突，则以后者为准。\n                env_file: .env\n\nenv_file:\n  - ./common.env\n  - ./apps/web.env\n  - /opt/secrets.env\n                环境变量文件中每一行必须符合格式，支持 # 开头的注释行\n                # common.env: Set Rails/Rack environment\nRACK_ENV=development\n            extends\n                基于已有的服务进行扩展\n                例如我们已经有了一个 webapp 服务，模板文件为 common.yml\n                # common.yml\nwebapp:\n  build: ./webapp\n  environment:\n    - DEBUG=false\n    - SEND_EMAILS=false\n                编写一个新的 development.yml 文件，使用 common.yml 中的 webapp 服务进行扩展\n                # development.yml\nweb:\n  extends:\n    file: common.yml\n    service: webapp\n  ports:\n    - \"8000:8000\"\n  links:\n    - db\n  environment:\n    - DEBUG=true\ndb:\n  image: postgres\n                后者会自动继承 common.yml 中的 webapp 服务及相关环节变量\n            labels\n            container_name\n            log driver\n            net\n                设置网络模式\n                使用和 docker client 的 --net 参数一样的值\n                net: \"bridge\"\nnet: \"none\"\nnet: \"container:[name or id]\"\nnet: \"host\"\n            pid\n                跟主机系统共享进程命名空间\n                打开该选项的容器可以相互通过进程 ID 来访问和操作\n                pid: \"host\"\n            dns\n                配置 DNS 服务器\n                可以是一个值，也可以是一个列表\n                dns: 8.8.8.8\n- dns:\n\n  - 8.8.8.8\n  - 9.9.9.9\n            cap_add, cap_drop\n                添加或放弃容器的 Linux 能力（Capabiliity）\n                cap_add:\n  - ALL\n\n- cap_drop:\n\n  - NET_ADMIN\n  - SYS_ADMIN\n            dns_search\n                配置 DNS 搜索域\n                可以是一个值，也可以是一个列表\n                dns_search: example.com\n- dns_search:\n\n  - domain1.example.com\n  - domain2.example.com\n            devices\n            security_opt\n            working_dir, entrypoint, user, hostname, domainname, \nmac_address, mem_limit, memswap_limit, privileged, \nrestart, stdin_open, tty, cpu_shares, cpuset, \nread_only, volume_driver\n                这些都是和 docker run 支持的选项类似\n                cpu_shares: 73\n\nworking_dir: /code\nentrypoint: /code/entrypoint.sh\nuser: postgresql\n\nhostname: foo\ndomainname: foo.com\n\nmem_limit: 1000000000\nprivileged: true\n\nrestart: always\n\nstdin_open: true\ntty: true\n        keyword\n            build\n                Path to a directory containing a Dockerfile.\n                build: /path/to/build/dir\n            cap_add, cap_drop\n                Add or drop container capabilities. See man 7 capabilities for a full list.\n                cap_add:\n  - ALL\n\ncap_drop:\n  - NET_ADMIN\n  - SYS_ADMIN\n            command\n                Override the default command.\n                command: bundle exec thin -p 3000\n            cgroup_parent\n                Specify an optional parent cgroup for the container.\n                cgroup_parent: m-executor-abcd\n            container_name\n                Specify a custom container name, rather than a generated default name.\n                container_name: my-web-container\n            devices\n                List of device mappings. Uses the same format as the --device docker client create option.\n                devices:\n  - \"/dev/ttyUSB0:/dev/ttyUSB0\"\n            dns\n                Custom DNS servers. Can be a single value or a list.\n                dns: 8.8.8.8\ndns:\n  - 8.8.8.8\n  - 9.9.9.9\n            dns_search\n                Custom DNS search domains. Can be a single value or a list.\n                dns_search: example.com\ndns_search:\n  - dc1.example.com\n  - dc2.example.com\n            dockerfile\n                Alternate Dockerfile.\nCompose will use an alternate file to build with.\n                dockerfile: Dockerfile-alternate\n            env_file\n                Add environment variables from a file. Can be a single value or a list.\nIf you have specified a Compose file with docker-compose -f FILE, \npaths in env_file are relative to the directory that file is in.\nEnvironment variables specified in environment override these values.\n                env_file: .env\n\nenv_file:\n  - ./common.env\n  - ./apps/web.env\n  - /opt/secrets.env\n                Compose expects each line in an env file to be in VAR=VAL format.\n Lines beginning with # (i.e. comments) are ignored, as are blank lines.\n\n# Set Rails/Rack environment\nRACK_ENV=development\n            environment\n                Add environment variables. You can use either an array or a dictionary.\n                Any boolean values; true, false, yes no, need to be enclosed in quotes\n to ensure they are not converted to True or False by the YML parser.\n                Environment variables with only a key are resolved to their values on the machine Compose is running on, \nwhich can be helpful for secret or host-specific values.\n                environment:\n  RACK_ENV: development\n  SHOW: 'true'\n  SESSION_SECRET:\n\nenvironment:\n  - RACK_ENV=development\n  - SHOW=true\n  - SESSION_SECRET\n            expose\n                Expose ports without publishing them to the host machine - \nthey’ll only be accessible to linked services. Only the internal port can be specified.\n                expose:\n - \"3000\"\n - \"8000\"\n            extends\n                Extend another service, in the current file or another, optionally overriding configuration.\nYou can use extends on any service together with other configuration keys.\n The extends value must be a dictionary defined with a required service and an optional file key.\n                extends:\n  file: common.yml\n  service: webapp\n            external_links\n                Link to containers started outside this docker-compose.yml or even outside of Compose,\n especially for containers that provide shared or common services. \nexternal_links follow semantics similar to links when specifying both the container name and the link alias (CONTAINER:ALIAS).\n                external_links:\n - redis_1\n - project_db_1:mysql\n - project_db_1:postgresql\n            extra_hosts\n                Add hostname mappings. Use the same values as the docker client --add-host parameter.\n                extra_hosts:\n - \"somehost:162.242.195.82\"\n - \"otherhost:50.31.209.229\"\n                An entry with the ip address and hostname will be created in /etc/hosts inside containers for this service, e.g:\n\n162.242.195.82  somehost\n50.31.209.229   otherhost\n            image\n                Tag or partial image ID. Can be local or remote - \nCompose will attempt to pull if it doesn’t exist locally.\n                image: ubuntu\nimage: orchardup/postgresql\nimage: a4bc65fd\n            labels\n                Add metadata to containers using Docker labels. You can use either an array or a dictionary.\nIt’s recommended that you use reverse-DNS notation to prevent your labels from conflicting with those used by other software.\n                labels:\n  com.example.description: \"Accounting webapp\"\n  com.example.department: \"Finance\"\n  com.example.label-with-empty-value: \"\"\n\nlabels:\n  - \"com.example.description=Accounting webapp\"\n  - \"com.example.department=Finance\"\n  - \"com.example.label-with-empty-value\"\n            links\n                Link to containers in another service. \nEither specify both the service name and the link alias (SERVICE:ALIAS), \nor just the service name (which will also be used for the alias).\n                links:\n - db\n - db:database\n - redis\n                An entry with the alias’ name will be created in /etc/hosts \ninside containers for this service, e.g:\n\n172.17.2.186  db\n172.17.2.186  database\n172.17.2.187  redis\n            log_driver\n                Specify a logging driver for the service’s containers, as with the --log-driver option for docker run\n                The default value is json-file.\nNote: Only the json-file driver makes the logs available directly from docker-compose up and docker-compose logs. \nUsing any other driver will not print any logs.\n                log_driver: \"json-file\"\nlog_driver: \"syslog\"\nlog_driver: \"none\"\n            log_opt\n                Specify logging options with log_opt for the logging driver, as with the --log-opt option for docker run.\n                log_driver: \"syslog\"\nlog_opt:\n  syslog-address: \"tcp://192.168.0.42:123\"\n            net\n                Networking mode. Use the same values as the docker client --net parameter.\n                net: \"bridge\"\nnet: \"none\"\nnet: \"container:[name or id]\"\nnet: \"host\"\n            pid\n                pid: \"host\"\n                Sets the PID mode to the host PID mode. \nThis turns on sharing between container and the host oprating system the PID address space.\n Containers launched with this flag will be able to access and manipulate other containers\n in the bare-metal machine’s namespace and vise-versa.\n            ports\n                Expose ports. Either specify both ports (HOST:CONTAINER), \nor just the container port (a random host port will be chosen).\n                Note: When mapping ports in the HOST:CONTAINER format, \nyou may experience erroneous results when using a container port lower than 60, \nbecause YAML will parse numbers in the format xx:yy as sexagesimal (base 60). \nFor this reason, we recommend always explicitly specifying your port mappings as strings.\n                ports:\n - \"3000\"\n - \"3000-3005\"\n - \"8000:8000\"\n - \"9090-9091:8080-8081\"\n - \"49100:22\"\n - \"127.0.0.1:8001:8001\"\n - \"127.0.0.1:5000-5010:5000-5010\"\n            security_opt\n                Override the default labeling scheme for each container.\n                security_opt:\n    - label:user:USER\n    - label:role:ROLE\n            ulimits\n                Override the default ulimits for a container. \nYou can either specify a single limit as an integer or soft/hard limits as a mapping.\n                ulimits:\n    nproc: 65535\n    nofile:\n      soft: 20000\n      hard: 40000\n            volumes, volume_driver\n                Mount paths as volumes, optionally specifying a path on the host machine \n(HOST:CONTAINER), or an access mode (HOST:CONTAINER:ro).\n                volumes:\n - /var/lib/mysql\n - ./cache:/tmp/cache\n - ~/configs:/etc/configs/:ro\n                You can mount a relative path on the host, which will expand relative to the director\ny of the Compose configuration file being used. Relative paths should always begin with . or ...\n                If you use a volume name (instead of a volume path), you may also specify a volume_driver.\nvolume_driver: mydriver\nNote: No path expansion will be done if you have also specified a volume_driver.\n            volumes_from\n                Mount all of the volumes from another service or container, \noptionally specifying read-only access(ro) or read-write(rw).\n                volumes_from:\n - service_name\n - container_name\n - service_name:rw\n            cpu_shares, cpuset, domainname, entrypoint, hostname, \nipc, mac_address, mem_limit, memswap_limit, privileged, \nread_only, restart, stdin_open, tty, user, working_dir\n                Each of these is a single value, analogous to its docker run counterpart.\n                cpu_shares: 73\ncpuset: 0,1\n\nentrypoint: /code/entrypoint.sh\nuser: postgresql\nworking_dir: /code\n\ndomainname: foo.com\nhostname: foo\nipc: host\nmac_address: 02:42:ac:11:65:43\n\nmem_limit: 1000000000\nmemswap_limit: 2000000000\nprivileged: true\n\nrestart: always\n\nread_only: true\nstdin_open: true\ntty: true","slug":"docker-compose-file-tmp","published":1,"date":"2017-01-18T07:23:48.000Z","updated":"2017-01-18T07:23:48.000Z","title":"","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm38p001n21sv7kz4kcuv","content":"<pre><code><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\">containers:</div><div class=\"line\">web:</div><div class=\"line\"> build: .</div><div class=\"line\"> command: python app.py</div><div class=\"line\"> ports:</div><div class=\"line\"> - &quot;5000:5000&quot;</div><div class=\"line\"> volumes:</div><div class=\"line\"> - .:/code</div><div class=\"line\"> links:</div><div class=\"line\"> - redis</div><div class=\"line\"> environment:</div><div class=\"line\"> - PYTHONUNBUFFERED=1</div><div class=\"line\">redis:</div><div class=\"line\"> image: redis:latest</div><div class=\"line\"> command: redis-server --appendonly yes</div></pre></td></tr></table></figure>\n\n ```\n    wiki2:\n    image: &apos;nickstenning/mediawiki&apos;\n    ports:\n        - &quot;8880:80&quot;\n    links:\n        - db:database\n    volumes:\n        - /data/wiki2:/data\n\n    db:\n    image: &quot;mysql&quot;\n    expose:\n        - &quot;3306&quot;\n    environment:\n        - MYSQL_ROOT_PASSWORD=defaultpass\n\n上面的YAML文件定义了两个容器应用，第一个容器运行Python应用，并通过当前目录的Dockerfile文件构建。\n第二个容器是从Docker Hub注册中心的Redis官方仓库中构建。links指令用来定义依赖，意思是Python应用依赖于Redis应用。\n定义完成后，通过下面的命令来启动应用：\n\ndocker-compose up\n\nlinks指令关注的是Python和Redis容器之间的依赖关系，Redis容器是最先开始构建，紧随其后的是Python容器。\n</code></pre><h1 id=\"Variable-substitution\"><a href=\"#Variable-substitution\" class=\"headerlink\" title=\"Variable substitution\"></a>Variable substitution</h1><p>Both $VARIABLE and ${VARIABLE} syntax are supported.<br>Extended shell-style features, such as ${VARIABLE-default} and ${VARIABLE/foo/bar}, are not supported.<br>            db:<br>  image: “postgres:${POSTGRES_VERSION}”<br>            web:<br>  build: .<br>  command: “$$VAR_NOT_INTERPOLATED_BY_COMPOSE”</p>\n<h1 id=\"install\"><a href=\"#install\" class=\"headerlink\" title=\"install\"></a>install</h1><pre><code>curl -L https://raw.githubusercontent.com/docker/compose/$(docker-compose --version | awk &apos;NR==1{print $NF}&apos;)/contrib/completion/bash/docker-compose &gt; /etc/bash_completion.d/docker-compose\n    doc\n        docker-compose.yml reference\n            https://docs.docker.com/compose/yml/\n    keywords\n        image\n            指定为镜像名称或镜像 ID。如果镜像在本地不存在，Compose 将会尝试拉去这个镜像\n            image: ubuntu\n        build\n            指定 Dockerfile 所在文件夹的路径\n            Compose 将会利用它自动构建这个镜像，然后使用这个镜像\n            build: /path/to/build/dir\n        dockerfile\n        command\n            覆盖容器启动后默认执行的命令。\n            command: bundle exec thin -p 3000\n        links\n            链接到其它服务中的容器\n            使用服务名称（同时作为别名）或服务名称：服务别名 （SERVICE:ALIAS） 格式都可以\n                links:\n</code></pre><ul>\n<li>db</li>\n<li>db:database</li>\n<li>redis<pre><code>使用的别名将会自动在服务容器中的 /etc/hosts 里创建,相应的环境变量也将被创建\n    172.17.2.186  db\n</code></pre>172.17.2.186  database<br>172.17.2.187  redis<pre><code>external_links\n    链接到 docker-compose.yml 外部的容器\n    甚至 并非 Compose 管理的容器。参数格式跟 links 类似\n    external_links:\n</code></pre></li>\n<li>redis_1</li>\n<li>project_db_1:mysql</li>\n<li>project_db_1:postgresql<pre><code>extra_hosts\nports\n    暴露端口信息\n    使用宿主：容器 （HOST:CONTAINER）格式或者仅仅指定容器的端口（宿主将会随机选择端口）都可以\n    ports:\n</code></pre></li>\n<li>“3000”</li>\n<li>“8000:8000”</li>\n<li>“49100:22”</li>\n<li>“127.0.0.1:8001:8001”<pre><code>expose\n    暴露端口，但不映射到宿主机，只被连接的服务访问\n    仅可以指定内部端口为参数\n    expose:\n</code></pre></li>\n<li>“3000”</li>\n<li>“8000”<pre><code>volumes\n    卷挂载路径设置\n    可以设置宿主机路径 （HOST:CONTAINER） 或加上访问模式 （HOST:CONTAINER:ro）\n    volumes:\n</code></pre></li>\n<li>/var/lib/mysql</li>\n<li>cache/:/tmp/cache</li>\n<li>~/configs:/etc/configs/:ro<pre><code>volumes_from\n    从另一个服务或容器挂载它的所有卷\n    volumes_from:\n</code></pre></li>\n<li>service_name</li>\n<li>container_name<pre><code>environment\n    设置环境变量。你可以使用数组或字典两种格式。\n    只给定名称的变量会自动获取它在 Compose 主机上的值，可以用来防止泄露不必要的数据\n    environment:\n</code></pre>RACK_ENV: development<br>SESSION_SECRET:</li>\n</ul>\n<p>environment:</p>\n<ul>\n<li>RACK_ENV=development</li>\n<li>SESSION_SECRET<pre><code>env_file\n    从文件中获取环境变量，可以为单独的文件路径或列表。\n    如果通过 docker-compose -f FILE 指定了模板文件，则 env_file 中路径会基于模板文件路径。\n    如果有变量名称与 environment 指令冲突，则以后者为准。\n    env_file: .env\n</code></pre></li>\n</ul>\n<p>env_file:</p>\n<ul>\n<li>./common.env</li>\n<li>./apps/web.env</li>\n<li><p>/opt/secrets.env</p>\n<pre><code>环境变量文件中每一行必须符合格式，支持 # 开头的注释行\n# common.env: Set Rails/Rack environment\n</code></pre><p>RACK_ENV=development</p>\n<pre><code>extends\n    基于已有的服务进行扩展\n    例如我们已经有了一个 webapp 服务，模板文件为 common.yml\n    # common.yml\n</code></pre><p>webapp:<br>build: ./webapp<br>environment:</p>\n<ul>\n<li>DEBUG=false</li>\n<li>SEND_EMAILS=false<pre><code>编写一个新的 development.yml 文件，使用 common.yml 中的 webapp 服务进行扩展\n# development.yml\n</code></pre>web:<br>extends:<br>file: common.yml<br>service: webapp<br>ports:</li>\n<li>“8000:8000”<br>links:</li>\n<li>db<br>environment:</li>\n<li>DEBUG=true<br>db:<br>image: postgres<pre><code>    后者会自动继承 common.yml 中的 webapp 服务及相关环节变量\nlabels\ncontainer_name\nlog driver\nnet\n    设置网络模式\n    使用和 docker client 的 --net 参数一样的值\n    net: &quot;bridge&quot;\n</code></pre>net: “none”<br>net: “container:[name or id]”<br>net: “host”<pre><code>pid\n    跟主机系统共享进程命名空间\n    打开该选项的容器可以相互通过进程 ID 来访问和操作\n    pid: &quot;host&quot;\ndns\n    配置 DNS 服务器\n    可以是一个值，也可以是一个列表\n    dns: 8.8.8.8\n</code></pre></li>\n<li>dns:</li>\n</ul>\n</li>\n<li><p>8.8.8.8</p>\n</li>\n<li>9.9.9.9<pre><code>cap_add, cap_drop\n    添加或放弃容器的 Linux 能力（Capabiliity）\n    cap_add:\n</code></pre></li>\n<li>ALL</li>\n</ul>\n<ul>\n<li><p>cap_drop:</p>\n<ul>\n<li>NET_ADMIN</li>\n<li>SYS_ADMIN<pre><code>dns_search\n    配置 DNS 搜索域\n    可以是一个值，也可以是一个列表\n    dns_search: example.com\n</code></pre></li>\n</ul>\n</li>\n<li><p>dns_search:</p>\n<ul>\n<li>domain1.example.com</li>\n<li>domain2.example.com<pre><code>devices\nsecurity_opt\nworking_dir, entrypoint, user, hostname, domainname, \n</code></pre>mac_address, mem_limit, memswap_limit, privileged,<br>restart, stdin_open, tty, cpu_shares, cpuset,<br>read_only, volume_driver<pre><code>这些都是和 docker run 支持的选项类似\ncpu_shares: 73\n</code></pre></li>\n</ul>\n</li>\n</ul>\n<p>working_dir: /code<br>entrypoint: /code/entrypoint.sh<br>user: postgresql</p>\n<p>hostname: foo<br>domainname: foo.com</p>\n<p>mem_limit: 1000000000<br>privileged: true</p>\n<p>restart: always</p>\n<p>stdin_open: true<br>tty: true<br>        keyword<br>            build<br>                Path to a directory containing a Dockerfile.<br>                build: /path/to/build/dir<br>            cap_add, cap_drop<br>                Add or drop container capabilities. See man 7 capabilities for a full list.<br>                cap_add:</p>\n<ul>\n<li>ALL</li>\n</ul>\n<p>cap_drop:</p>\n<ul>\n<li>NET_ADMIN</li>\n<li>SYS_ADMIN<pre><code>command\n    Override the default command.\n    command: bundle exec thin -p 3000\ncgroup_parent\n    Specify an optional parent cgroup for the container.\n    cgroup_parent: m-executor-abcd\ncontainer_name\n    Specify a custom container name, rather than a generated default name.\n    container_name: my-web-container\ndevices\n    List of device mappings. Uses the same format as the --device docker client create option.\n    devices:\n</code></pre></li>\n<li>“/dev/ttyUSB0:/dev/ttyUSB0”<pre><code>dns\n    Custom DNS servers. Can be a single value or a list.\n    dns: 8.8.8.8\n</code></pre>dns:</li>\n<li>8.8.8.8</li>\n<li>9.9.9.9<pre><code>dns_search\n    Custom DNS search domains. Can be a single value or a list.\n    dns_search: example.com\n</code></pre>dns_search:</li>\n<li>dc1.example.com</li>\n<li>dc2.example.com<pre><code>dockerfile\n    Alternate Dockerfile.\n</code></pre>Compose will use an alternate file to build with.<pre><code>    dockerfile: Dockerfile-alternate\nenv_file\n    Add environment variables from a file. Can be a single value or a list.\n</code></pre>If you have specified a Compose file with docker-compose -f FILE,<br>paths in env_file are relative to the directory that file is in.<br>Environment variables specified in environment override these values.<pre><code>env_file: .env\n</code></pre></li>\n</ul>\n<p>env_file:</p>\n<ul>\n<li>./common.env</li>\n<li>./apps/web.env</li>\n<li>/opt/secrets.env<pre><code>Compose expects each line in an env file to be in VAR=VAL format.\n</code></pre>Lines beginning with # (i.e. comments) are ignored, as are blank lines.</li>\n</ul>\n<h1 id=\"Set-Rails-Rack-environment\"><a href=\"#Set-Rails-Rack-environment\" class=\"headerlink\" title=\"Set Rails/Rack environment\"></a>Set Rails/Rack environment</h1><p>RACK_ENV=development<br>            environment<br>                Add environment variables. You can use either an array or a dictionary.<br>                Any boolean values; true, false, yes no, need to be enclosed in quotes<br> to ensure they are not converted to True or False by the YML parser.<br>                Environment variables with only a key are resolved to their values on the machine Compose is running on,<br>which can be helpful for secret or host-specific values.<br>                environment:<br>  RACK_ENV: development<br>  SHOW: ‘true’<br>  SESSION_SECRET:</p>\n<p>environment:</p>\n<ul>\n<li>RACK_ENV=development</li>\n<li>SHOW=true</li>\n<li>SESSION_SECRET<pre><code>expose\n    Expose ports without publishing them to the host machine - \n</code></pre>they’ll only be accessible to linked services. Only the internal port can be specified.<pre><code>expose:\n</code></pre><ul>\n<li>“3000”</li>\n<li>“8000”<pre><code>extends\n    Extend another service, in the current file or another, optionally overriding configuration.\n</code></pre>You can use extends on any service together with other configuration keys.<br>The extends value must be a dictionary defined with a required service and an optional file key.<pre><code>extends:\n</code></pre>file: common.yml<br>service: webapp<pre><code>external_links\n    Link to containers started outside this docker-compose.yml or even outside of Compose,\n</code></pre>especially for containers that provide shared or common services.<br>external_links follow semantics similar to links when specifying both the container name and the link alias (CONTAINER:ALIAS).<pre><code>external_links:\n</code></pre></li>\n<li>redis_1</li>\n<li>project_db_1:mysql</li>\n<li>project_db_1:postgresql<pre><code>extra_hosts\n    Add hostname mappings. Use the same values as the docker client --add-host parameter.\n    extra_hosts:\n</code></pre></li>\n<li>“somehost:162.242.195.82”</li>\n<li>“otherhost:50.31.209.229”<pre><code>An entry with the ip address and hostname will be created in /etc/hosts inside containers for this service, e.g:\n</code></pre></li>\n</ul>\n</li>\n</ul>\n<p>162.242.195.82  somehost<br>50.31.209.229   otherhost<br>            image<br>                Tag or partial image ID. Can be local or remote -<br>Compose will attempt to pull if it doesn’t exist locally.<br>                image: ubuntu<br>image: orchardup/postgresql<br>image: a4bc65fd<br>            labels<br>                Add metadata to containers using Docker labels. You can use either an array or a dictionary.<br>It’s recommended that you use reverse-DNS notation to prevent your labels from conflicting with those used by other software.<br>                labels:<br>  com.example.description: “Accounting webapp”<br>  com.example.department: “Finance”<br>  com.example.label-with-empty-value: “”</p>\n<p>labels:</p>\n<ul>\n<li>“com.example.description=Accounting webapp”</li>\n<li>“com.example.department=Finance”</li>\n<li>“com.example.label-with-empty-value”<pre><code>links\n    Link to containers in another service. \n</code></pre>Either specify both the service name and the link alias (SERVICE:ALIAS),<br>or just the service name (which will also be used for the alias).<pre><code>links:\n</code></pre><ul>\n<li>db</li>\n<li>db:database</li>\n<li>redis<pre><code>An entry with the alias’ name will be created in /etc/hosts \n</code></pre>inside containers for this service, e.g:</li>\n</ul>\n</li>\n</ul>\n<p>172.17.2.186  db<br>172.17.2.186  database<br>172.17.2.187  redis<br>            log_driver<br>                Specify a logging driver for the service’s containers, as with the –log-driver option for docker run<br>                The default value is json-file.<br>Note: Only the json-file driver makes the logs available directly from docker-compose up and docker-compose logs.<br>Using any other driver will not print any logs.<br>                log_driver: “json-file”<br>log_driver: “syslog”<br>log_driver: “none”<br>            log_opt<br>                Specify logging options with log_opt for the logging driver, as with the –log-opt option for docker run.<br>                log_driver: “syslog”<br>log_opt:<br>  syslog-address: “tcp://192.168.0.42:123”<br>            net<br>                Networking mode. Use the same values as the docker client –net parameter.<br>                net: “bridge”<br>net: “none”<br>net: “container:[name or id]”<br>net: “host”<br>            pid<br>                pid: “host”<br>                Sets the PID mode to the host PID mode.<br>This turns on sharing between container and the host oprating system the PID address space.<br> Containers launched with this flag will be able to access and manipulate other containers<br> in the bare-metal machine’s namespace and vise-versa.<br>            ports<br>                Expose ports. Either specify both ports (HOST:CONTAINER),<br>or just the container port (a random host port will be chosen).<br>                Note: When mapping ports in the HOST:CONTAINER format,<br>you may experience erroneous results when using a container port lower than 60,<br>because YAML will parse numbers in the format xx:yy as sexagesimal (base 60).<br>For this reason, we recommend always explicitly specifying your port mappings as strings.<br>                ports:</p>\n<ul>\n<li>“3000”</li>\n<li>“3000-3005”</li>\n<li>“8000:8000”</li>\n<li>“9090-9091:8080-8081”</li>\n<li>“49100:22”</li>\n<li>“127.0.0.1:8001:8001”</li>\n<li>“127.0.0.1:5000-5010:5000-5010”<pre><code>security_opt\n    Override the default labeling scheme for each container.\n    security_opt:\n</code></pre><ul>\n<li>label:user:USER</li>\n<li>label:role:ROLE<pre><code>ulimits\n    Override the default ulimits for a container. \n</code></pre>You can either specify a single limit as an integer or soft/hard limits as a mapping.<pre><code>ulimits:\n</code></pre>nproc: 65535<br>nofile:<br>soft: 20000<br>hard: 40000<pre><code>volumes, volume_driver\n    Mount paths as volumes, optionally specifying a path on the host machine \n</code></pre>(HOST:CONTAINER), or an access mode (HOST:CONTAINER:ro).<pre><code>volumes:\n</code></pre></li>\n</ul>\n</li>\n<li>/var/lib/mysql</li>\n<li>./cache:/tmp/cache</li>\n<li>~/configs:/etc/configs/:ro<pre><code>You can mount a relative path on the host, which will expand relative to the director\n</code></pre>y of the Compose configuration file being used. Relative paths should always begin with . or …<pre><code>If you use a volume name (instead of a volume path), you may also specify a volume_driver.\n</code></pre>volume_driver: mydriver<br>Note: No path expansion will be done if you have also specified a volume_driver.<pre><code>volumes_from\n    Mount all of the volumes from another service or container, \n</code></pre>optionally specifying read-only access(ro) or read-write(rw).<pre><code>volumes_from:\n</code></pre></li>\n<li>service_name</li>\n<li>container_name</li>\n<li>service_name:rw<pre><code>cpu_shares, cpuset, domainname, entrypoint, hostname, \n</code></pre>ipc, mac_address, mem_limit, memswap_limit, privileged,<br>read_only, restart, stdin_open, tty, user, working_dir<pre><code>Each of these is a single value, analogous to its docker run counterpart.\ncpu_shares: 73\n</code></pre>cpuset: 0,1</li>\n</ul>\n<p>entrypoint: /code/entrypoint.sh<br>user: postgresql<br>working_dir: /code</p>\n<p>domainname: foo.com<br>hostname: foo<br>ipc: host<br>mac_address: 02:42:ac:11:65:43</p>\n<p>mem_limit: 1000000000<br>memswap_limit: 2000000000<br>privileged: true</p>\n<p>restart: always</p>\n<p>read_only: true<br>stdin_open: true<br>tty: true</p>\n","excerpt":"","more":"<pre><code><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\">containers:</div><div class=\"line\">web:</div><div class=\"line\"> build: .</div><div class=\"line\"> command: python app.py</div><div class=\"line\"> ports:</div><div class=\"line\"> - &quot;5000:5000&quot;</div><div class=\"line\"> volumes:</div><div class=\"line\"> - .:/code</div><div class=\"line\"> links:</div><div class=\"line\"> - redis</div><div class=\"line\"> environment:</div><div class=\"line\"> - PYTHONUNBUFFERED=1</div><div class=\"line\">redis:</div><div class=\"line\"> image: redis:latest</div><div class=\"line\"> command: redis-server --appendonly yes</div></pre></td></tr></table></figure>\n\n ```\n    wiki2:\n    image: &apos;nickstenning/mediawiki&apos;\n    ports:\n        - &quot;8880:80&quot;\n    links:\n        - db:database\n    volumes:\n        - /data/wiki2:/data\n\n    db:\n    image: &quot;mysql&quot;\n    expose:\n        - &quot;3306&quot;\n    environment:\n        - MYSQL_ROOT_PASSWORD=defaultpass\n\n上面的YAML文件定义了两个容器应用，第一个容器运行Python应用，并通过当前目录的Dockerfile文件构建。\n第二个容器是从Docker Hub注册中心的Redis官方仓库中构建。links指令用来定义依赖，意思是Python应用依赖于Redis应用。\n定义完成后，通过下面的命令来启动应用：\n\ndocker-compose up\n\nlinks指令关注的是Python和Redis容器之间的依赖关系，Redis容器是最先开始构建，紧随其后的是Python容器。\n</code></pre><h1 id=\"Variable-substitution\"><a href=\"#Variable-substitution\" class=\"headerlink\" title=\"Variable substitution\"></a>Variable substitution</h1><p>Both $VARIABLE and ${VARIABLE} syntax are supported.<br>Extended shell-style features, such as ${VARIABLE-default} and ${VARIABLE/foo/bar}, are not supported.<br>            db:<br>  image: “postgres:${POSTGRES_VERSION}”<br>            web:<br>  build: .<br>  command: “$$VAR_NOT_INTERPOLATED_BY_COMPOSE”</p>\n<h1 id=\"install\"><a href=\"#install\" class=\"headerlink\" title=\"install\"></a>install</h1><pre><code>curl -L https://raw.githubusercontent.com/docker/compose/$(docker-compose --version | awk &apos;NR==1{print $NF}&apos;)/contrib/completion/bash/docker-compose &gt; /etc/bash_completion.d/docker-compose\n    doc\n        docker-compose.yml reference\n            https://docs.docker.com/compose/yml/\n    keywords\n        image\n            指定为镜像名称或镜像 ID。如果镜像在本地不存在，Compose 将会尝试拉去这个镜像\n            image: ubuntu\n        build\n            指定 Dockerfile 所在文件夹的路径\n            Compose 将会利用它自动构建这个镜像，然后使用这个镜像\n            build: /path/to/build/dir\n        dockerfile\n        command\n            覆盖容器启动后默认执行的命令。\n            command: bundle exec thin -p 3000\n        links\n            链接到其它服务中的容器\n            使用服务名称（同时作为别名）或服务名称：服务别名 （SERVICE:ALIAS） 格式都可以\n                links:\n</code></pre><ul>\n<li>db</li>\n<li>db:database</li>\n<li>redis<pre><code>使用的别名将会自动在服务容器中的 /etc/hosts 里创建,相应的环境变量也将被创建\n    172.17.2.186  db\n</code></pre>172.17.2.186  database<br>172.17.2.187  redis<pre><code>external_links\n    链接到 docker-compose.yml 外部的容器\n    甚至 并非 Compose 管理的容器。参数格式跟 links 类似\n    external_links:\n</code></pre></li>\n<li>redis_1</li>\n<li>project_db_1:mysql</li>\n<li>project_db_1:postgresql<pre><code>extra_hosts\nports\n    暴露端口信息\n    使用宿主：容器 （HOST:CONTAINER）格式或者仅仅指定容器的端口（宿主将会随机选择端口）都可以\n    ports:\n</code></pre></li>\n<li>“3000”</li>\n<li>“8000:8000”</li>\n<li>“49100:22”</li>\n<li>“127.0.0.1:8001:8001”<pre><code>expose\n    暴露端口，但不映射到宿主机，只被连接的服务访问\n    仅可以指定内部端口为参数\n    expose:\n</code></pre></li>\n<li>“3000”</li>\n<li>“8000”<pre><code>volumes\n    卷挂载路径设置\n    可以设置宿主机路径 （HOST:CONTAINER） 或加上访问模式 （HOST:CONTAINER:ro）\n    volumes:\n</code></pre></li>\n<li>/var/lib/mysql</li>\n<li>cache/:/tmp/cache</li>\n<li>~/configs:/etc/configs/:ro<pre><code>volumes_from\n    从另一个服务或容器挂载它的所有卷\n    volumes_from:\n</code></pre></li>\n<li>service_name</li>\n<li>container_name<pre><code>environment\n    设置环境变量。你可以使用数组或字典两种格式。\n    只给定名称的变量会自动获取它在 Compose 主机上的值，可以用来防止泄露不必要的数据\n    environment:\n</code></pre>RACK_ENV: development<br>SESSION_SECRET:</li>\n</ul>\n<p>environment:</p>\n<ul>\n<li>RACK_ENV=development</li>\n<li>SESSION_SECRET<pre><code>env_file\n    从文件中获取环境变量，可以为单独的文件路径或列表。\n    如果通过 docker-compose -f FILE 指定了模板文件，则 env_file 中路径会基于模板文件路径。\n    如果有变量名称与 environment 指令冲突，则以后者为准。\n    env_file: .env\n</code></pre></li>\n</ul>\n<p>env_file:</p>\n<ul>\n<li>./common.env</li>\n<li>./apps/web.env</li>\n<li><p>/opt/secrets.env</p>\n<pre><code>环境变量文件中每一行必须符合格式，支持 # 开头的注释行\n# common.env: Set Rails/Rack environment\n</code></pre><p>RACK_ENV=development</p>\n<pre><code>extends\n    基于已有的服务进行扩展\n    例如我们已经有了一个 webapp 服务，模板文件为 common.yml\n    # common.yml\n</code></pre><p>webapp:<br>build: ./webapp<br>environment:</p>\n<ul>\n<li>DEBUG=false</li>\n<li>SEND_EMAILS=false<pre><code>编写一个新的 development.yml 文件，使用 common.yml 中的 webapp 服务进行扩展\n# development.yml\n</code></pre>web:<br>extends:<br>file: common.yml<br>service: webapp<br>ports:</li>\n<li>“8000:8000”<br>links:</li>\n<li>db<br>environment:</li>\n<li>DEBUG=true<br>db:<br>image: postgres<pre><code>    后者会自动继承 common.yml 中的 webapp 服务及相关环节变量\nlabels\ncontainer_name\nlog driver\nnet\n    设置网络模式\n    使用和 docker client 的 --net 参数一样的值\n    net: &quot;bridge&quot;\n</code></pre>net: “none”<br>net: “container:[name or id]”<br>net: “host”<pre><code>pid\n    跟主机系统共享进程命名空间\n    打开该选项的容器可以相互通过进程 ID 来访问和操作\n    pid: &quot;host&quot;\ndns\n    配置 DNS 服务器\n    可以是一个值，也可以是一个列表\n    dns: 8.8.8.8\n</code></pre></li>\n<li>dns:</li>\n</ul>\n</li>\n<li><p>8.8.8.8</p>\n</li>\n<li>9.9.9.9<pre><code>cap_add, cap_drop\n    添加或放弃容器的 Linux 能力（Capabiliity）\n    cap_add:\n</code></pre></li>\n<li>ALL</li>\n</ul>\n<ul>\n<li><p>cap_drop:</p>\n<ul>\n<li>NET_ADMIN</li>\n<li>SYS_ADMIN<pre><code>dns_search\n    配置 DNS 搜索域\n    可以是一个值，也可以是一个列表\n    dns_search: example.com\n</code></pre></li>\n</ul>\n</li>\n<li><p>dns_search:</p>\n<ul>\n<li>domain1.example.com</li>\n<li>domain2.example.com<pre><code>devices\nsecurity_opt\nworking_dir, entrypoint, user, hostname, domainname, \n</code></pre>mac_address, mem_limit, memswap_limit, privileged,<br>restart, stdin_open, tty, cpu_shares, cpuset,<br>read_only, volume_driver<pre><code>这些都是和 docker run 支持的选项类似\ncpu_shares: 73\n</code></pre></li>\n</ul>\n</li>\n</ul>\n<p>working_dir: /code<br>entrypoint: /code/entrypoint.sh<br>user: postgresql</p>\n<p>hostname: foo<br>domainname: foo.com</p>\n<p>mem_limit: 1000000000<br>privileged: true</p>\n<p>restart: always</p>\n<p>stdin_open: true<br>tty: true<br>        keyword<br>            build<br>                Path to a directory containing a Dockerfile.<br>                build: /path/to/build/dir<br>            cap_add, cap_drop<br>                Add or drop container capabilities. See man 7 capabilities for a full list.<br>                cap_add:</p>\n<ul>\n<li>ALL</li>\n</ul>\n<p>cap_drop:</p>\n<ul>\n<li>NET_ADMIN</li>\n<li>SYS_ADMIN<pre><code>command\n    Override the default command.\n    command: bundle exec thin -p 3000\ncgroup_parent\n    Specify an optional parent cgroup for the container.\n    cgroup_parent: m-executor-abcd\ncontainer_name\n    Specify a custom container name, rather than a generated default name.\n    container_name: my-web-container\ndevices\n    List of device mappings. Uses the same format as the --device docker client create option.\n    devices:\n</code></pre></li>\n<li>“/dev/ttyUSB0:/dev/ttyUSB0”<pre><code>dns\n    Custom DNS servers. Can be a single value or a list.\n    dns: 8.8.8.8\n</code></pre>dns:</li>\n<li>8.8.8.8</li>\n<li>9.9.9.9<pre><code>dns_search\n    Custom DNS search domains. Can be a single value or a list.\n    dns_search: example.com\n</code></pre>dns_search:</li>\n<li>dc1.example.com</li>\n<li>dc2.example.com<pre><code>dockerfile\n    Alternate Dockerfile.\n</code></pre>Compose will use an alternate file to build with.<pre><code>    dockerfile: Dockerfile-alternate\nenv_file\n    Add environment variables from a file. Can be a single value or a list.\n</code></pre>If you have specified a Compose file with docker-compose -f FILE,<br>paths in env_file are relative to the directory that file is in.<br>Environment variables specified in environment override these values.<pre><code>env_file: .env\n</code></pre></li>\n</ul>\n<p>env_file:</p>\n<ul>\n<li>./common.env</li>\n<li>./apps/web.env</li>\n<li>/opt/secrets.env<pre><code>Compose expects each line in an env file to be in VAR=VAL format.\n</code></pre>Lines beginning with # (i.e. comments) are ignored, as are blank lines.</li>\n</ul>\n<h1 id=\"Set-Rails-Rack-environment\"><a href=\"#Set-Rails-Rack-environment\" class=\"headerlink\" title=\"Set Rails/Rack environment\"></a>Set Rails/Rack environment</h1><p>RACK_ENV=development<br>            environment<br>                Add environment variables. You can use either an array or a dictionary.<br>                Any boolean values; true, false, yes no, need to be enclosed in quotes<br> to ensure they are not converted to True or False by the YML parser.<br>                Environment variables with only a key are resolved to their values on the machine Compose is running on,<br>which can be helpful for secret or host-specific values.<br>                environment:<br>  RACK_ENV: development<br>  SHOW: ‘true’<br>  SESSION_SECRET:</p>\n<p>environment:</p>\n<ul>\n<li>RACK_ENV=development</li>\n<li>SHOW=true</li>\n<li>SESSION_SECRET<pre><code>expose\n    Expose ports without publishing them to the host machine - \n</code></pre>they’ll only be accessible to linked services. Only the internal port can be specified.<pre><code>expose:\n</code></pre><ul>\n<li>“3000”</li>\n<li>“8000”<pre><code>extends\n    Extend another service, in the current file or another, optionally overriding configuration.\n</code></pre>You can use extends on any service together with other configuration keys.<br>The extends value must be a dictionary defined with a required service and an optional file key.<pre><code>extends:\n</code></pre>file: common.yml<br>service: webapp<pre><code>external_links\n    Link to containers started outside this docker-compose.yml or even outside of Compose,\n</code></pre>especially for containers that provide shared or common services.<br>external_links follow semantics similar to links when specifying both the container name and the link alias (CONTAINER:ALIAS).<pre><code>external_links:\n</code></pre></li>\n<li>redis_1</li>\n<li>project_db_1:mysql</li>\n<li>project_db_1:postgresql<pre><code>extra_hosts\n    Add hostname mappings. Use the same values as the docker client --add-host parameter.\n    extra_hosts:\n</code></pre></li>\n<li>“somehost:162.242.195.82”</li>\n<li>“otherhost:50.31.209.229”<pre><code>An entry with the ip address and hostname will be created in /etc/hosts inside containers for this service, e.g:\n</code></pre></li>\n</ul>\n</li>\n</ul>\n<p>162.242.195.82  somehost<br>50.31.209.229   otherhost<br>            image<br>                Tag or partial image ID. Can be local or remote -<br>Compose will attempt to pull if it doesn’t exist locally.<br>                image: ubuntu<br>image: orchardup/postgresql<br>image: a4bc65fd<br>            labels<br>                Add metadata to containers using Docker labels. You can use either an array or a dictionary.<br>It’s recommended that you use reverse-DNS notation to prevent your labels from conflicting with those used by other software.<br>                labels:<br>  com.example.description: “Accounting webapp”<br>  com.example.department: “Finance”<br>  com.example.label-with-empty-value: “”</p>\n<p>labels:</p>\n<ul>\n<li>“com.example.description=Accounting webapp”</li>\n<li>“com.example.department=Finance”</li>\n<li>“com.example.label-with-empty-value”<pre><code>links\n    Link to containers in another service. \n</code></pre>Either specify both the service name and the link alias (SERVICE:ALIAS),<br>or just the service name (which will also be used for the alias).<pre><code>links:\n</code></pre><ul>\n<li>db</li>\n<li>db:database</li>\n<li>redis<pre><code>An entry with the alias’ name will be created in /etc/hosts \n</code></pre>inside containers for this service, e.g:</li>\n</ul>\n</li>\n</ul>\n<p>172.17.2.186  db<br>172.17.2.186  database<br>172.17.2.187  redis<br>            log_driver<br>                Specify a logging driver for the service’s containers, as with the –log-driver option for docker run<br>                The default value is json-file.<br>Note: Only the json-file driver makes the logs available directly from docker-compose up and docker-compose logs.<br>Using any other driver will not print any logs.<br>                log_driver: “json-file”<br>log_driver: “syslog”<br>log_driver: “none”<br>            log_opt<br>                Specify logging options with log_opt for the logging driver, as with the –log-opt option for docker run.<br>                log_driver: “syslog”<br>log_opt:<br>  syslog-address: “tcp://192.168.0.42:123”<br>            net<br>                Networking mode. Use the same values as the docker client –net parameter.<br>                net: “bridge”<br>net: “none”<br>net: “container:[name or id]”<br>net: “host”<br>            pid<br>                pid: “host”<br>                Sets the PID mode to the host PID mode.<br>This turns on sharing between container and the host oprating system the PID address space.<br> Containers launched with this flag will be able to access and manipulate other containers<br> in the bare-metal machine’s namespace and vise-versa.<br>            ports<br>                Expose ports. Either specify both ports (HOST:CONTAINER),<br>or just the container port (a random host port will be chosen).<br>                Note: When mapping ports in the HOST:CONTAINER format,<br>you may experience erroneous results when using a container port lower than 60,<br>because YAML will parse numbers in the format xx:yy as sexagesimal (base 60).<br>For this reason, we recommend always explicitly specifying your port mappings as strings.<br>                ports:</p>\n<ul>\n<li>“3000”</li>\n<li>“3000-3005”</li>\n<li>“8000:8000”</li>\n<li>“9090-9091:8080-8081”</li>\n<li>“49100:22”</li>\n<li>“127.0.0.1:8001:8001”</li>\n<li>“127.0.0.1:5000-5010:5000-5010”<pre><code>security_opt\n    Override the default labeling scheme for each container.\n    security_opt:\n</code></pre><ul>\n<li>label:user:USER</li>\n<li>label:role:ROLE<pre><code>ulimits\n    Override the default ulimits for a container. \n</code></pre>You can either specify a single limit as an integer or soft/hard limits as a mapping.<pre><code>ulimits:\n</code></pre>nproc: 65535<br>nofile:<br>soft: 20000<br>hard: 40000<pre><code>volumes, volume_driver\n    Mount paths as volumes, optionally specifying a path on the host machine \n</code></pre>(HOST:CONTAINER), or an access mode (HOST:CONTAINER:ro).<pre><code>volumes:\n</code></pre></li>\n</ul>\n</li>\n<li>/var/lib/mysql</li>\n<li>./cache:/tmp/cache</li>\n<li>~/configs:/etc/configs/:ro<pre><code>You can mount a relative path on the host, which will expand relative to the director\n</code></pre>y of the Compose configuration file being used. Relative paths should always begin with . or …<pre><code>If you use a volume name (instead of a volume path), you may also specify a volume_driver.\n</code></pre>volume_driver: mydriver<br>Note: No path expansion will be done if you have also specified a volume_driver.<pre><code>volumes_from\n    Mount all of the volumes from another service or container, \n</code></pre>optionally specifying read-only access(ro) or read-write(rw).<pre><code>volumes_from:\n</code></pre></li>\n<li>service_name</li>\n<li>container_name</li>\n<li>service_name:rw<pre><code>cpu_shares, cpuset, domainname, entrypoint, hostname, \n</code></pre>ipc, mac_address, mem_limit, memswap_limit, privileged,<br>read_only, restart, stdin_open, tty, user, working_dir<pre><code>Each of these is a single value, analogous to its docker run counterpart.\ncpu_shares: 73\n</code></pre>cpuset: 0,1</li>\n</ul>\n<p>entrypoint: /code/entrypoint.sh<br>user: postgresql<br>working_dir: /code</p>\n<p>domainname: foo.com<br>hostname: foo<br>ipc: host<br>mac_address: 02:42:ac:11:65:43</p>\n<p>mem_limit: 1000000000<br>memswap_limit: 2000000000<br>privileged: true</p>\n<p>restart: always</p>\n<p>read_only: true<br>stdin_open: true<br>tty: true</p>\n"},{"title":"docker core","_content":"\n# about docker\n\n\n# basic concept\n\n- [dockerfile]()\n- [image]()\n- [yaml]()\n- [file system]()\n\n- [dockerhub]()\n\n# basic component\n\n- [docker engine]()\n- [docker compose]()\n- [docker swarm]()\n\n# basic topic\n\n- [storage]()\n- [network]()\n- [security]()","source":"_posts/docker-core.md","raw":"---\ntitle: docker core\ncategories:\n- docker\ntags:\n- core\n---\n\n# about docker\n\n\n# basic concept\n\n- [dockerfile]()\n- [image]()\n- [yaml]()\n- [file system]()\n\n- [dockerhub]()\n\n# basic component\n\n- [docker engine]()\n- [docker compose]()\n- [docker swarm]()\n\n# basic topic\n\n- [storage]()\n- [network]()\n- [security]()","slug":"docker-core","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-15T06:28:13.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm38t001s21svt0sqa4iq","content":"<h1 id=\"about-docker\"><a href=\"#about-docker\" class=\"headerlink\" title=\"about docker\"></a>about docker</h1><h1 id=\"basic-concept\"><a href=\"#basic-concept\" class=\"headerlink\" title=\"basic concept\"></a>basic concept</h1><ul>\n<li><a href=\"\">dockerfile</a></li>\n<li><a href=\"\">image</a></li>\n<li><a href=\"\">yaml</a></li>\n<li><p><a href=\"\">file system</a></p>\n</li>\n<li><p><a href=\"\">dockerhub</a></p>\n</li>\n</ul>\n<h1 id=\"basic-component\"><a href=\"#basic-component\" class=\"headerlink\" title=\"basic component\"></a>basic component</h1><ul>\n<li><a href=\"\">docker engine</a></li>\n<li><a href=\"\">docker compose</a></li>\n<li><a href=\"\">docker swarm</a></li>\n</ul>\n<h1 id=\"basic-topic\"><a href=\"#basic-topic\" class=\"headerlink\" title=\"basic topic\"></a>basic topic</h1><ul>\n<li><a href=\"\">storage</a></li>\n<li><a href=\"\">network</a></li>\n<li><a href=\"\">security</a></li>\n</ul>\n","excerpt":"","more":"<h1 id=\"about-docker\"><a href=\"#about-docker\" class=\"headerlink\" title=\"about docker\"></a>about docker</h1><h1 id=\"basic-concept\"><a href=\"#basic-concept\" class=\"headerlink\" title=\"basic concept\"></a>basic concept</h1><ul>\n<li><a href=\"\">dockerfile</a></li>\n<li><a href=\"\">image</a></li>\n<li><a href=\"\">yaml</a></li>\n<li><p><a href=\"\">file system</a></p>\n</li>\n<li><p><a href=\"\">dockerhub</a></p>\n</li>\n</ul>\n<h1 id=\"basic-component\"><a href=\"#basic-component\" class=\"headerlink\" title=\"basic component\"></a>basic component</h1><ul>\n<li><a href=\"\">docker engine</a></li>\n<li><a href=\"\">docker compose</a></li>\n<li><a href=\"\">docker swarm</a></li>\n</ul>\n<h1 id=\"basic-topic\"><a href=\"#basic-topic\" class=\"headerlink\" title=\"basic topic\"></a>basic topic</h1><ul>\n<li><a href=\"\">storage</a></li>\n<li><a href=\"\">network</a></li>\n<li><a href=\"\">security</a></li>\n</ul>\n"},{"title":"docker cookbook","_content":"\n# docker cookbook","source":"_posts/docker-cookbook.md","raw":"---\ntitle: docker cookbook\ncategories:\n- docker\ntags:\n- cookbook\n---\n\n# docker cookbook","slug":"docker-cookbook","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-13T10:51:59.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm38y001u21svs7mji43n","content":"<h1 id=\"docker-cookbook\"><a href=\"#docker-cookbook\" class=\"headerlink\" title=\"docker cookbook\"></a>docker cookbook</h1>","excerpt":"","more":"<h1 id=\"docker-cookbook\"><a href=\"#docker-cookbook\" class=\"headerlink\" title=\"docker cookbook\"></a>docker cookbook</h1>"},{"title":"devops portal","_content":"\n# about\n\n# awesome\nhttps://github.com/AcalephStorage/awesome-devops\nhttps://github.com/joubertredrat/awesome-devops","source":"_posts/devops--portal.md","raw":"---\ntitle: devops portal\ncategories:\n- devops\ntags:\n- portal\n---\n\n# about\n\n# awesome\nhttps://github.com/AcalephStorage/awesome-devops\nhttps://github.com/joubertredrat/awesome-devops","slug":"devops--portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-02-27T03:08:56.000Z","_id":"ciypwm391001y21svngtxbmr2","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><h1 id=\"awesome\"><a href=\"#awesome\" class=\"headerlink\" title=\"awesome\"></a>awesome</h1><p><a href=\"https://github.com/AcalephStorage/awesome-devops\" target=\"_blank\" rel=\"external\">https://github.com/AcalephStorage/awesome-devops</a><br><a href=\"https://github.com/joubertredrat/awesome-devops\" target=\"_blank\" rel=\"external\">https://github.com/joubertredrat/awesome-devops</a></p>\n","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><h1 id=\"awesome\"><a href=\"#awesome\" class=\"headerlink\" title=\"awesome\"></a>awesome</h1><p><a href=\"https://github.com/AcalephStorage/awesome-devops\">https://github.com/AcalephStorage/awesome-devops</a><br><a href=\"https://github.com/joubertredrat/awesome-devops\">https://github.com/joubertredrat/awesome-devops</a></p>\n"},{"title":"dockerfile","_content":"\n# about dockerfile\n\n\n# dockerfile command\n\n- point to a Dockerfile anywhere in your file system\n    \n        docker build -f /path/to/a/Dockerfile .\n\n- specify a repository and tag at which to save the new image if the build succeeds\n    \n        docker build -t shykes/myapp .\n\n# dockerfile keyword\n\n- FROM: base image\n\n- MAINTAINER\n\n        MAINTAINER ag \"allengaller@gmail.com\"\n\n- USER: set user\n\n        USER root\n\n- RUN: run system cmd\n\n        RUN apt-get update\n        RUN [\"apt-get\", \"update\"]\n        RUN apt-get install -y nginx\n        RUN touch test.txt && echo \"abc\" >> abc.txt\n\n- EXPOSE: expose port\n\n- ADD\n\n    The ADD instruction copies new files, directories or remote file URLs from <src> and adds them to the filesystem of the container at the path <dest>.\n    \n    pattern: ADD <src>... <dest>; ADD [\"<src>\",... \"<dest>\"]\n    add folder: ADD /webapp /opt/webapp\n    add file: ADD abc.txt /opt/\n    add network file: ADD https://www.baidu.com/img/bd_logo1.png /opt/\n\n- ENV: set env variable\n\n        ENV WEBAPP_PORT = 9090\n\n- WORKDIR: set working directory\n    \n    The WORKDIR instruction sets the working directory for any RUN, CMD, ENTRYPOINT, COPY and ADD instructions that follow it in the Dockerfile.The WORKDIR instruction can resolve environment variables previously set using ENV. You can only use environment variables explicitly set in the Dockerfile. For example:\n\n        ENV DIRPATH /path\n        WORKDIR $DIRPATH/$DIRNAME\n        RUN pwd\n\n    The output of the final pwd command in this Dockerfile would be /path/$DIRNAME.It can be used multiple times in the one Dockerfile. If a relative path is provided, it will be relative to the path of the previous WORKDIR instruction. For example:\n\n        WORKDIR /a\n        WORKDIR b\n        WORKDIR c\n        RUN pwd\n\n    The output of the final pwd command in this Dockerfile would be /a/b/c.\n    \n        WORKDIR /opt/\n\n- ENTRYPOINT: set boot command, append parameter to boot cmd\n    \n    An ENTRYPOINT allows you to configure a container that will run as an executable.For example, the following will start nginx with its default content, listening on port 80:\n            \n        docker run -i -t --rm -p 80:80 nginx\n    \n        ENTRYPOINT [\"ls\"]\n        ENTRYPOINT [\"ls\"]\n        CMD [\"-l\", \"-a\"]\n\n- CMD: set boot parameter\n    \n        CMD [\"ls\", \"-a\", \"-l\"]\n        CMD ls -l -a\n\n- VOLUME: set volume\n    \n        VOLUME [\"/data\", \"/var/www\"]\n\n- ONBUILD: trigger for child image\n    \n        ONBUILD ADD . /app/src\n        ONBUILD RUN echo \"on build excuted\" >> onbuild.txt\n\n- ARG\n\n- STOPSIGNAL\n\n# best practice [link](https://docs.docker.com/engine/articles/dockerfile_best-practices/)\n\n- Containers should be ephemeral\n\n    The container produced by the image your Dockerfile defines should be as ephemeral as possible.By “ephemeral,” we mean that it can be stopped and destroyed and a new one built and put in place with an absolute minimum of set-up and configuration.\n\n- Use a .dockerignore file\n\n    In most cases, it’s best to put each Dockerfile in an empty directory. Then, add to that directory only the files needed for building the Dockerfile. To increase the build’s performance, you can exclude files and directories by adding a .dockerignore file to that directory as well. This file supports exclusion patterns similar to .gitignore files. For information on creating one, see the .dockerignore file.\n    \n- Avoid installing unnecessary packages\n\n    In order to reduce complexity, dependencies, file sizes, and build times, you should avoid installing extra or unnecessary packages just because they might be “nice to have.” For example, you don’t need to include a text editor in a database image.\n\n- Run only one process per container\n\n    In almost all cases, you should only run a single process in a single container. Decoupling applications into multiple containers makes it much easier to scale horizontally and reuse containers. If that service depends on another service, make use of container linking.\n\n- Minimize the number of layers\n\n    You need to find the balance between readability (and thus long-term maintainability) of the Dockerfile and minimizing the number of layers it uses. Be strategic and cautious about the number of layers you use.\n\n- Sort multi-line arguments\n\n    Here’s an example from the buildpack-deps image:\n\n    RUN apt-get update && apt-get install -y \\\n      bzr \\\n      cvs \\\n      git \\\n      mercurial \\\n      subversion\n\n- Build cache\n\n# .dockerignore\n\n    */temp*\n    */*/temp*\n    temp?\n","source":"_posts/docker-dockerfile-detail.md","raw":"---\ntitle: dockerfile\ncategories:\n- docker\ntags:\n- detail\n---\n\n# about dockerfile\n\n\n# dockerfile command\n\n- point to a Dockerfile anywhere in your file system\n    \n        docker build -f /path/to/a/Dockerfile .\n\n- specify a repository and tag at which to save the new image if the build succeeds\n    \n        docker build -t shykes/myapp .\n\n# dockerfile keyword\n\n- FROM: base image\n\n- MAINTAINER\n\n        MAINTAINER ag \"allengaller@gmail.com\"\n\n- USER: set user\n\n        USER root\n\n- RUN: run system cmd\n\n        RUN apt-get update\n        RUN [\"apt-get\", \"update\"]\n        RUN apt-get install -y nginx\n        RUN touch test.txt && echo \"abc\" >> abc.txt\n\n- EXPOSE: expose port\n\n- ADD\n\n    The ADD instruction copies new files, directories or remote file URLs from <src> and adds them to the filesystem of the container at the path <dest>.\n    \n    pattern: ADD <src>... <dest>; ADD [\"<src>\",... \"<dest>\"]\n    add folder: ADD /webapp /opt/webapp\n    add file: ADD abc.txt /opt/\n    add network file: ADD https://www.baidu.com/img/bd_logo1.png /opt/\n\n- ENV: set env variable\n\n        ENV WEBAPP_PORT = 9090\n\n- WORKDIR: set working directory\n    \n    The WORKDIR instruction sets the working directory for any RUN, CMD, ENTRYPOINT, COPY and ADD instructions that follow it in the Dockerfile.The WORKDIR instruction can resolve environment variables previously set using ENV. You can only use environment variables explicitly set in the Dockerfile. For example:\n\n        ENV DIRPATH /path\n        WORKDIR $DIRPATH/$DIRNAME\n        RUN pwd\n\n    The output of the final pwd command in this Dockerfile would be /path/$DIRNAME.It can be used multiple times in the one Dockerfile. If a relative path is provided, it will be relative to the path of the previous WORKDIR instruction. For example:\n\n        WORKDIR /a\n        WORKDIR b\n        WORKDIR c\n        RUN pwd\n\n    The output of the final pwd command in this Dockerfile would be /a/b/c.\n    \n        WORKDIR /opt/\n\n- ENTRYPOINT: set boot command, append parameter to boot cmd\n    \n    An ENTRYPOINT allows you to configure a container that will run as an executable.For example, the following will start nginx with its default content, listening on port 80:\n            \n        docker run -i -t --rm -p 80:80 nginx\n    \n        ENTRYPOINT [\"ls\"]\n        ENTRYPOINT [\"ls\"]\n        CMD [\"-l\", \"-a\"]\n\n- CMD: set boot parameter\n    \n        CMD [\"ls\", \"-a\", \"-l\"]\n        CMD ls -l -a\n\n- VOLUME: set volume\n    \n        VOLUME [\"/data\", \"/var/www\"]\n\n- ONBUILD: trigger for child image\n    \n        ONBUILD ADD . /app/src\n        ONBUILD RUN echo \"on build excuted\" >> onbuild.txt\n\n- ARG\n\n- STOPSIGNAL\n\n# best practice [link](https://docs.docker.com/engine/articles/dockerfile_best-practices/)\n\n- Containers should be ephemeral\n\n    The container produced by the image your Dockerfile defines should be as ephemeral as possible.By “ephemeral,” we mean that it can be stopped and destroyed and a new one built and put in place with an absolute minimum of set-up and configuration.\n\n- Use a .dockerignore file\n\n    In most cases, it’s best to put each Dockerfile in an empty directory. Then, add to that directory only the files needed for building the Dockerfile. To increase the build’s performance, you can exclude files and directories by adding a .dockerignore file to that directory as well. This file supports exclusion patterns similar to .gitignore files. For information on creating one, see the .dockerignore file.\n    \n- Avoid installing unnecessary packages\n\n    In order to reduce complexity, dependencies, file sizes, and build times, you should avoid installing extra or unnecessary packages just because they might be “nice to have.” For example, you don’t need to include a text editor in a database image.\n\n- Run only one process per container\n\n    In almost all cases, you should only run a single process in a single container. Decoupling applications into multiple containers makes it much easier to scale horizontally and reuse containers. If that service depends on another service, make use of container linking.\n\n- Minimize the number of layers\n\n    You need to find the balance between readability (and thus long-term maintainability) of the Dockerfile and minimizing the number of layers it uses. Be strategic and cautious about the number of layers you use.\n\n- Sort multi-line arguments\n\n    Here’s an example from the buildpack-deps image:\n\n    RUN apt-get update && apt-get install -y \\\n      bzr \\\n      cvs \\\n      git \\\n      mercurial \\\n      subversion\n\n- Build cache\n\n# .dockerignore\n\n    */temp*\n    */*/temp*\n    temp?\n","slug":"docker-dockerfile-detail","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-13T11:46:41.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm394002121svo7zb0zb3","content":"<h1 id=\"about-dockerfile\"><a href=\"#about-dockerfile\" class=\"headerlink\" title=\"about dockerfile\"></a>about dockerfile</h1><h1 id=\"dockerfile-command\"><a href=\"#dockerfile-command\" class=\"headerlink\" title=\"dockerfile command\"></a>dockerfile command</h1><ul>\n<li><p>point to a Dockerfile anywhere in your file system</p>\n<pre><code>docker build -f /path/to/a/Dockerfile .\n</code></pre></li>\n<li><p>specify a repository and tag at which to save the new image if the build succeeds</p>\n<pre><code>docker build -t shykes/myapp .\n</code></pre></li>\n</ul>\n<h1 id=\"dockerfile-keyword\"><a href=\"#dockerfile-keyword\" class=\"headerlink\" title=\"dockerfile keyword\"></a>dockerfile keyword</h1><ul>\n<li><p>FROM: base image</p>\n</li>\n<li><p>MAINTAINER</p>\n<pre><code>MAINTAINER ag &quot;allengaller@gmail.com&quot;\n</code></pre></li>\n<li><p>USER: set user</p>\n<pre><code>USER root\n</code></pre></li>\n<li><p>RUN: run system cmd</p>\n<pre><code>RUN apt-get update\nRUN [&quot;apt-get&quot;, &quot;update&quot;]\nRUN apt-get install -y nginx\nRUN touch test.txt &amp;&amp; echo &quot;abc&quot; &gt;&gt; abc.txt\n</code></pre></li>\n<li><p>EXPOSE: expose port</p>\n</li>\n<li><p>ADD</p>\n<p>  The ADD instruction copies new files, directories or remote file URLs from <src> and adds them to the filesystem of the container at the path <dest>.</dest></src></p>\n<p>  pattern: ADD <src>… <dest>; ADD [“<src>“,… “<dest>“]<br>  add folder: ADD /webapp /opt/webapp<br>  add file: ADD abc.txt /opt/<br>  add network file: ADD <a href=\"https://www.baidu.com/img/bd_logo1.png\" target=\"_blank\" rel=\"external\">https://www.baidu.com/img/bd_logo1.png</a> /opt/</dest></src></dest></src></p>\n</li>\n<li><p>ENV: set env variable</p>\n<pre><code>ENV WEBAPP_PORT = 9090\n</code></pre></li>\n<li><p>WORKDIR: set working directory</p>\n<p>  The WORKDIR instruction sets the working directory for any RUN, CMD, ENTRYPOINT, COPY and ADD instructions that follow it in the Dockerfile.The WORKDIR instruction can resolve environment variables previously set using ENV. You can only use environment variables explicitly set in the Dockerfile. For example:</p>\n<pre><code>ENV DIRPATH /path\nWORKDIR $DIRPATH/$DIRNAME\nRUN pwd\n</code></pre><p>  The output of the final pwd command in this Dockerfile would be /path/$DIRNAME.It can be used multiple times in the one Dockerfile. If a relative path is provided, it will be relative to the path of the previous WORKDIR instruction. For example:</p>\n<pre><code>WORKDIR /a\nWORKDIR b\nWORKDIR c\nRUN pwd\n</code></pre><p>  The output of the final pwd command in this Dockerfile would be /a/b/c.</p>\n<pre><code>WORKDIR /opt/\n</code></pre></li>\n<li><p>ENTRYPOINT: set boot command, append parameter to boot cmd</p>\n<p>  An ENTRYPOINT allows you to configure a container that will run as an executable.For example, the following will start nginx with its default content, listening on port 80:</p>\n<pre><code>docker run -i -t --rm -p 80:80 nginx\n\nENTRYPOINT [&quot;ls&quot;]\nENTRYPOINT [&quot;ls&quot;]\nCMD [&quot;-l&quot;, &quot;-a&quot;]\n</code></pre></li>\n<li><p>CMD: set boot parameter</p>\n<pre><code>CMD [&quot;ls&quot;, &quot;-a&quot;, &quot;-l&quot;]\nCMD ls -l -a\n</code></pre></li>\n<li><p>VOLUME: set volume</p>\n<pre><code>VOLUME [&quot;/data&quot;, &quot;/var/www&quot;]\n</code></pre></li>\n<li><p>ONBUILD: trigger for child image</p>\n<pre><code>ONBUILD ADD . /app/src\nONBUILD RUN echo &quot;on build excuted&quot; &gt;&gt; onbuild.txt\n</code></pre></li>\n<li><p>ARG</p>\n</li>\n<li><p>STOPSIGNAL</p>\n</li>\n</ul>\n<h1 id=\"best-practice-link\"><a href=\"#best-practice-link\" class=\"headerlink\" title=\"best practice link\"></a>best practice <a href=\"https://docs.docker.com/engine/articles/dockerfile_best-practices/\" target=\"_blank\" rel=\"external\">link</a></h1><ul>\n<li><p>Containers should be ephemeral</p>\n<p>  The container produced by the image your Dockerfile defines should be as ephemeral as possible.By “ephemeral,” we mean that it can be stopped and destroyed and a new one built and put in place with an absolute minimum of set-up and configuration.</p>\n</li>\n<li><p>Use a .dockerignore file</p>\n<p>  In most cases, it’s best to put each Dockerfile in an empty directory. Then, add to that directory only the files needed for building the Dockerfile. To increase the build’s performance, you can exclude files and directories by adding a .dockerignore file to that directory as well. This file supports exclusion patterns similar to .gitignore files. For information on creating one, see the .dockerignore file.</p>\n</li>\n<li><p>Avoid installing unnecessary packages</p>\n<p>  In order to reduce complexity, dependencies, file sizes, and build times, you should avoid installing extra or unnecessary packages just because they might be “nice to have.” For example, you don’t need to include a text editor in a database image.</p>\n</li>\n<li><p>Run only one process per container</p>\n<p>  In almost all cases, you should only run a single process in a single container. Decoupling applications into multiple containers makes it much easier to scale horizontally and reuse containers. If that service depends on another service, make use of container linking.</p>\n</li>\n<li><p>Minimize the number of layers</p>\n<p>  You need to find the balance between readability (and thus long-term maintainability) of the Dockerfile and minimizing the number of layers it uses. Be strategic and cautious about the number of layers you use.</p>\n</li>\n<li><p>Sort multi-line arguments</p>\n<p>  Here’s an example from the buildpack-deps image:</p>\n<p>  RUN apt-get update &amp;&amp; apt-get install -y \\</p>\n<pre><code>bzr \\\ncvs \\\ngit \\\nmercurial \\\nsubversion\n</code></pre></li>\n<li><p>Build cache</p>\n</li>\n</ul>\n<h1 id=\"dockerignore\"><a href=\"#dockerignore\" class=\"headerlink\" title=\".dockerignore\"></a>.dockerignore</h1><pre><code>*/temp*\n*/*/temp*\ntemp?\n</code></pre>","excerpt":"","more":"<h1 id=\"about-dockerfile\"><a href=\"#about-dockerfile\" class=\"headerlink\" title=\"about dockerfile\"></a>about dockerfile</h1><h1 id=\"dockerfile-command\"><a href=\"#dockerfile-command\" class=\"headerlink\" title=\"dockerfile command\"></a>dockerfile command</h1><ul>\n<li><p>point to a Dockerfile anywhere in your file system</p>\n<pre><code>docker build -f /path/to/a/Dockerfile .\n</code></pre></li>\n<li><p>specify a repository and tag at which to save the new image if the build succeeds</p>\n<pre><code>docker build -t shykes/myapp .\n</code></pre></li>\n</ul>\n<h1 id=\"dockerfile-keyword\"><a href=\"#dockerfile-keyword\" class=\"headerlink\" title=\"dockerfile keyword\"></a>dockerfile keyword</h1><ul>\n<li><p>FROM: base image</p>\n</li>\n<li><p>MAINTAINER</p>\n<pre><code>MAINTAINER ag &quot;allengaller@gmail.com&quot;\n</code></pre></li>\n<li><p>USER: set user</p>\n<pre><code>USER root\n</code></pre></li>\n<li><p>RUN: run system cmd</p>\n<pre><code>RUN apt-get update\nRUN [&quot;apt-get&quot;, &quot;update&quot;]\nRUN apt-get install -y nginx\nRUN touch test.txt &amp;&amp; echo &quot;abc&quot; &gt;&gt; abc.txt\n</code></pre></li>\n<li><p>EXPOSE: expose port</p>\n</li>\n<li><p>ADD</p>\n<p>  The ADD instruction copies new files, directories or remote file URLs from <src> and adds them to the filesystem of the container at the path <dest>.</p>\n<p>  pattern: ADD <src>… <dest>; ADD [“<src>“,… “<dest>“]<br>  add folder: ADD /webapp /opt/webapp<br>  add file: ADD abc.txt /opt/<br>  add network file: ADD <a href=\"https://www.baidu.com/img/bd_logo1.png\">https://www.baidu.com/img/bd_logo1.png</a> /opt/</p>\n</li>\n<li><p>ENV: set env variable</p>\n<pre><code>ENV WEBAPP_PORT = 9090\n</code></pre></li>\n<li><p>WORKDIR: set working directory</p>\n<p>  The WORKDIR instruction sets the working directory for any RUN, CMD, ENTRYPOINT, COPY and ADD instructions that follow it in the Dockerfile.The WORKDIR instruction can resolve environment variables previously set using ENV. You can only use environment variables explicitly set in the Dockerfile. For example:</p>\n<pre><code>ENV DIRPATH /path\nWORKDIR $DIRPATH/$DIRNAME\nRUN pwd\n</code></pre><p>  The output of the final pwd command in this Dockerfile would be /path/$DIRNAME.It can be used multiple times in the one Dockerfile. If a relative path is provided, it will be relative to the path of the previous WORKDIR instruction. For example:</p>\n<pre><code>WORKDIR /a\nWORKDIR b\nWORKDIR c\nRUN pwd\n</code></pre><p>  The output of the final pwd command in this Dockerfile would be /a/b/c.</p>\n<pre><code>WORKDIR /opt/\n</code></pre></li>\n<li><p>ENTRYPOINT: set boot command, append parameter to boot cmd</p>\n<p>  An ENTRYPOINT allows you to configure a container that will run as an executable.For example, the following will start nginx with its default content, listening on port 80:</p>\n<pre><code>docker run -i -t --rm -p 80:80 nginx\n\nENTRYPOINT [&quot;ls&quot;]\nENTRYPOINT [&quot;ls&quot;]\nCMD [&quot;-l&quot;, &quot;-a&quot;]\n</code></pre></li>\n<li><p>CMD: set boot parameter</p>\n<pre><code>CMD [&quot;ls&quot;, &quot;-a&quot;, &quot;-l&quot;]\nCMD ls -l -a\n</code></pre></li>\n<li><p>VOLUME: set volume</p>\n<pre><code>VOLUME [&quot;/data&quot;, &quot;/var/www&quot;]\n</code></pre></li>\n<li><p>ONBUILD: trigger for child image</p>\n<pre><code>ONBUILD ADD . /app/src\nONBUILD RUN echo &quot;on build excuted&quot; &gt;&gt; onbuild.txt\n</code></pre></li>\n<li><p>ARG</p>\n</li>\n<li><p>STOPSIGNAL</p>\n</li>\n</ul>\n<h1 id=\"best-practice-link\"><a href=\"#best-practice-link\" class=\"headerlink\" title=\"best practice link\"></a>best practice <a href=\"https://docs.docker.com/engine/articles/dockerfile_best-practices/\">link</a></h1><ul>\n<li><p>Containers should be ephemeral</p>\n<p>  The container produced by the image your Dockerfile defines should be as ephemeral as possible.By “ephemeral,” we mean that it can be stopped and destroyed and a new one built and put in place with an absolute minimum of set-up and configuration.</p>\n</li>\n<li><p>Use a .dockerignore file</p>\n<p>  In most cases, it’s best to put each Dockerfile in an empty directory. Then, add to that directory only the files needed for building the Dockerfile. To increase the build’s performance, you can exclude files and directories by adding a .dockerignore file to that directory as well. This file supports exclusion patterns similar to .gitignore files. For information on creating one, see the .dockerignore file.</p>\n</li>\n<li><p>Avoid installing unnecessary packages</p>\n<p>  In order to reduce complexity, dependencies, file sizes, and build times, you should avoid installing extra or unnecessary packages just because they might be “nice to have.” For example, you don’t need to include a text editor in a database image.</p>\n</li>\n<li><p>Run only one process per container</p>\n<p>  In almost all cases, you should only run a single process in a single container. Decoupling applications into multiple containers makes it much easier to scale horizontally and reuse containers. If that service depends on another service, make use of container linking.</p>\n</li>\n<li><p>Minimize the number of layers</p>\n<p>  You need to find the balance between readability (and thus long-term maintainability) of the Dockerfile and minimizing the number of layers it uses. Be strategic and cautious about the number of layers you use.</p>\n</li>\n<li><p>Sort multi-line arguments</p>\n<p>  Here’s an example from the buildpack-deps image:</p>\n<p>  RUN apt-get update &amp;&amp; apt-get install -y \\</p>\n<pre><code>bzr \\\ncvs \\\ngit \\\nmercurial \\\nsubversion\n</code></pre></li>\n<li><p>Build cache</p>\n</li>\n</ul>\n<h1 id=\"dockerignore\"><a href=\"#dockerignore\" class=\"headerlink\" title=\".dockerignore\"></a>.dockerignore</h1><pre><code>*/temp*\n*/*/temp*\ntemp?\n</code></pre>"},{"title":"dockerhub","_content":"\n# dockerhub\n\n## about dockerhub [link](https://hub.docker.com)\n\n- types:\n\n        official image\n        user image\n\n## command\n\n- docker login\n\n        comfig: cat ~/.dockercfg\n\n- build: Automated Build/Trusted Build\n    \n- registry\n\n        docker pull registry\n        docker run -p 5000:5000 -d -i -t registry\n        docker commit 3ie9djk 127.0.0.1:5000/my_image:v1\n            [registry_host: registry_port\\image_name:image_tag]\n        docker push 127.0.0.1:5000/my_image:v1","source":"_posts/docker-dockerhub-detail.md","raw":"---\ntitle: dockerhub\ncategories:\n- docker\ntags:\n- detail\n---\n\n# dockerhub\n\n## about dockerhub [link](https://hub.docker.com)\n\n- types:\n\n        official image\n        user image\n\n## command\n\n- docker login\n\n        comfig: cat ~/.dockercfg\n\n- build: Automated Build/Trusted Build\n    \n- registry\n\n        docker pull registry\n        docker run -p 5000:5000 -d -i -t registry\n        docker commit 3ie9djk 127.0.0.1:5000/my_image:v1\n            [registry_host: registry_port\\image_name:image_tag]\n        docker push 127.0.0.1:5000/my_image:v1","slug":"docker-dockerhub-detail","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-13T11:46:34.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm396002521svmkmo7uob","content":"<h1 id=\"dockerhub\"><a href=\"#dockerhub\" class=\"headerlink\" title=\"dockerhub\"></a>dockerhub</h1><h2 id=\"about-dockerhub-link\"><a href=\"#about-dockerhub-link\" class=\"headerlink\" title=\"about dockerhub link\"></a>about dockerhub <a href=\"https://hub.docker.com\" target=\"_blank\" rel=\"external\">link</a></h2><ul>\n<li><p>types:</p>\n<pre><code>official image\nuser image\n</code></pre></li>\n</ul>\n<h2 id=\"command\"><a href=\"#command\" class=\"headerlink\" title=\"command\"></a>command</h2><ul>\n<li><p>docker login</p>\n<pre><code>comfig: cat ~/.dockercfg\n</code></pre></li>\n<li><p>build: Automated Build/Trusted Build</p>\n</li>\n<li><p>registry</p>\n<pre><code>docker pull registry\ndocker run -p 5000:5000 -d -i -t registry\ndocker commit 3ie9djk 127.0.0.1:5000/my_image:v1\n    [registry_host: registry_port\\image_name:image_tag]\ndocker push 127.0.0.1:5000/my_image:v1\n</code></pre></li>\n</ul>\n","excerpt":"","more":"<h1 id=\"dockerhub\"><a href=\"#dockerhub\" class=\"headerlink\" title=\"dockerhub\"></a>dockerhub</h1><h2 id=\"about-dockerhub-link\"><a href=\"#about-dockerhub-link\" class=\"headerlink\" title=\"about dockerhub link\"></a>about dockerhub <a href=\"https://hub.docker.com\">link</a></h2><ul>\n<li><p>types:</p>\n<pre><code>official image\nuser image\n</code></pre></li>\n</ul>\n<h2 id=\"command\"><a href=\"#command\" class=\"headerlink\" title=\"command\"></a>command</h2><ul>\n<li><p>docker login</p>\n<pre><code>comfig: cat ~/.dockercfg\n</code></pre></li>\n<li><p>build: Automated Build/Trusted Build</p>\n</li>\n<li><p>registry</p>\n<pre><code>docker pull registry\ndocker run -p 5000:5000 -d -i -t registry\ndocker commit 3ie9djk 127.0.0.1:5000/my_image:v1\n    [registry_host: registry_port\\image_name:image_tag]\ndocker push 127.0.0.1:5000/my_image:v1\n</code></pre></li>\n</ul>\n"},{"title":"docker engine","_content":"\n## about\nsource code: https://github.com/docker/docker\ninstallation: https://www.docker.com/products/overview\n\n## docker engine command\n\n- tips\n\n    Delete all containers:\n        \n        docker rm $(docker ps -a -q)\n\n    Delete all images:\n        \n        docker rmi $(docker images -q)\n\n- env\n\n    - info\n    \n    - version\n\n- life-cycle\n\n    - create:  (ini: stop)\n\n            --restart: check for exit code then restart container; always or on-failure; on-failure:5 restart 5 times max\n\n            sudo docker run --restart=always --name docker_restart -d ubuntu /bin/sh -c \"while true;do echo hello world;sleep 1;done\"\n\n    - exec: exec cmd insid container\n\n            sudo docker exec -d daemon_dave touch /etc/new_config_file\n            sudo docker exec -t -i daemon_dave /bin/bash\n\n    - kill: send SIGKILL signal to container process\n    \n    - pause\n\n    - restart\n\n    - rm: cannot remove a running container; docker stop or kill first or docker rm -f bad_ubuntu\n        \n        -q: list only container ids;\n        delete all container at once:\n            \n            docker rm `docker ps -a -q`\n\n    - run: [reference](https://docs.docker.com/engine/reference/run/);(ini: run); \n\n        equals: docker create & docker start\n\n        2 types of container\n        - interactive\n            -i: STDIN\n            -t: open terminal\n            exit?: docker stop or kill;exit\n\n                sudo docker run -i -t --name=inspect_shell ubuntu /bin/bash\n                inspect_shell: container name\n                base image: ubuntu\n                command: /bin/bash\n                file system: image+writable layer\n                network: virtual network interface bridge to host & set a IP\n        \n        - daemon: -d\n            exit?: docker stop or kill\n            \n                sudo docker run --name daemon_while -d ubuntu /bin/sh -c \"while true; do echo hello world; sleep 1; done\"\n            \n                return token\n            \n                docker ps\n\n    - start: start existing container\n            \n            sudo docker start inspect_shell or cid\n\n    - stop: works for both interactive and daemon container;send SIGTERM signal to container process\n\n            sudo docker stop daemon_while\n            sudo docker stop s39c938dj34489d\n        \n    - unpause\n\n- registry\n\n    - login\n    \n    - logout\n    \n    - pull\n    \n    - push\n    \n    - search\n\n- image\n\n    - build\n    \n    - images\n    \n    - import:             \n\n            cat my_container.rar | sudo docker import - imported:container\n            repository: imported, tag: container\n            docker import url res:tag\n\n    - load\n    \n    - rmi\n    \n    - save\n    \n    - tag\n    \n    - commit\n\n- container\n\n    - attach: attach terminal to interactive container\n    \n    - export\n\n            sudo docker run -i -t --nam=inspect_import ubuntu /bin/bash\n            #... do something\n            sudo docker export inspect_import > my_container.tar\n\n    - inspect: check out the configuration\n\n            sudo docker inspect daemon_dave\n\n        -f or --format:\n\n            sudo docker inspect --format='{{ .State.Running }}' daemon_dave\n    \n    - port\n    \n    - ps: checkout existing container\n\n        -a: all\n            Exited(0): exit\n        -l: latest container\n        -n=x: latest x container\n\n    - rename\n    \n    - stats\n    \n    - top: check out UID PID PPID...\n        \n            sudo docker run -d --name=\"daemon_top\" ubuntu /bin/bash -c 'while true;do sleep 1;done'\n        \n        2 process:\n        \n            sudo docker top daemon_top\n\n    - wait\n    \n    - cp\n    \n    - diff\n\n- sys log\n\n    - events\n    \n    - history\n    \n    - logs\n\n        -f: realtime\n        --tail=x: last x line\n                \n            sudo docker logs -f --tail=5 -t daemon_logs\n\n- other\n\n    - docker daemon: [link](https://docs.docker.com/engine/reference/commandline/daemon/), A self-sufficient runtime for linux containers.\n            \n        The Docker daemon can listen for Docker Remote API requests via three different types of Socket: unix, tcp, and fd.\n        By default, a unix domain socket (or IPC socket) is created at /var/run/docker.sock, requiring either root permission, or docker group membership.","source":"_posts/docker-engine-detail.md","raw":"---\ntitle: docker engine\ncategories:\n- docker\ntags:\n- detail\n---\n\n## about\nsource code: https://github.com/docker/docker\ninstallation: https://www.docker.com/products/overview\n\n## docker engine command\n\n- tips\n\n    Delete all containers:\n        \n        docker rm $(docker ps -a -q)\n\n    Delete all images:\n        \n        docker rmi $(docker images -q)\n\n- env\n\n    - info\n    \n    - version\n\n- life-cycle\n\n    - create:  (ini: stop)\n\n            --restart: check for exit code then restart container; always or on-failure; on-failure:5 restart 5 times max\n\n            sudo docker run --restart=always --name docker_restart -d ubuntu /bin/sh -c \"while true;do echo hello world;sleep 1;done\"\n\n    - exec: exec cmd insid container\n\n            sudo docker exec -d daemon_dave touch /etc/new_config_file\n            sudo docker exec -t -i daemon_dave /bin/bash\n\n    - kill: send SIGKILL signal to container process\n    \n    - pause\n\n    - restart\n\n    - rm: cannot remove a running container; docker stop or kill first or docker rm -f bad_ubuntu\n        \n        -q: list only container ids;\n        delete all container at once:\n            \n            docker rm `docker ps -a -q`\n\n    - run: [reference](https://docs.docker.com/engine/reference/run/);(ini: run); \n\n        equals: docker create & docker start\n\n        2 types of container\n        - interactive\n            -i: STDIN\n            -t: open terminal\n            exit?: docker stop or kill;exit\n\n                sudo docker run -i -t --name=inspect_shell ubuntu /bin/bash\n                inspect_shell: container name\n                base image: ubuntu\n                command: /bin/bash\n                file system: image+writable layer\n                network: virtual network interface bridge to host & set a IP\n        \n        - daemon: -d\n            exit?: docker stop or kill\n            \n                sudo docker run --name daemon_while -d ubuntu /bin/sh -c \"while true; do echo hello world; sleep 1; done\"\n            \n                return token\n            \n                docker ps\n\n    - start: start existing container\n            \n            sudo docker start inspect_shell or cid\n\n    - stop: works for both interactive and daemon container;send SIGTERM signal to container process\n\n            sudo docker stop daemon_while\n            sudo docker stop s39c938dj34489d\n        \n    - unpause\n\n- registry\n\n    - login\n    \n    - logout\n    \n    - pull\n    \n    - push\n    \n    - search\n\n- image\n\n    - build\n    \n    - images\n    \n    - import:             \n\n            cat my_container.rar | sudo docker import - imported:container\n            repository: imported, tag: container\n            docker import url res:tag\n\n    - load\n    \n    - rmi\n    \n    - save\n    \n    - tag\n    \n    - commit\n\n- container\n\n    - attach: attach terminal to interactive container\n    \n    - export\n\n            sudo docker run -i -t --nam=inspect_import ubuntu /bin/bash\n            #... do something\n            sudo docker export inspect_import > my_container.tar\n\n    - inspect: check out the configuration\n\n            sudo docker inspect daemon_dave\n\n        -f or --format:\n\n            sudo docker inspect --format='{{ .State.Running }}' daemon_dave\n    \n    - port\n    \n    - ps: checkout existing container\n\n        -a: all\n            Exited(0): exit\n        -l: latest container\n        -n=x: latest x container\n\n    - rename\n    \n    - stats\n    \n    - top: check out UID PID PPID...\n        \n            sudo docker run -d --name=\"daemon_top\" ubuntu /bin/bash -c 'while true;do sleep 1;done'\n        \n        2 process:\n        \n            sudo docker top daemon_top\n\n    - wait\n    \n    - cp\n    \n    - diff\n\n- sys log\n\n    - events\n    \n    - history\n    \n    - logs\n\n        -f: realtime\n        --tail=x: last x line\n                \n            sudo docker logs -f --tail=5 -t daemon_logs\n\n- other\n\n    - docker daemon: [link](https://docs.docker.com/engine/reference/commandline/daemon/), A self-sufficient runtime for linux containers.\n            \n        The Docker daemon can listen for Docker Remote API requests via three different types of Socket: unix, tcp, and fd.\n        By default, a unix domain socket (or IPC socket) is created at /var/run/docker.sock, requiring either root permission, or docker group membership.","slug":"docker-engine-detail","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-02-10T05:34:03.000Z","_id":"ciypwm39a002821svvdj89we5","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h2><p>source code: <a href=\"https://github.com/docker/docker\" target=\"_blank\" rel=\"external\">https://github.com/docker/docker</a><br>installation: <a href=\"https://www.docker.com/products/overview\" target=\"_blank\" rel=\"external\">https://www.docker.com/products/overview</a></p>\n<h2 id=\"docker-engine-command\"><a href=\"#docker-engine-command\" class=\"headerlink\" title=\"docker engine command\"></a>docker engine command</h2><ul>\n<li><p>tips</p>\n<p>  Delete all containers:</p>\n<pre><code>docker rm $(docker ps -a -q)\n</code></pre><p>  Delete all images:</p>\n<pre><code>docker rmi $(docker images -q)\n</code></pre></li>\n<li><p>env</p>\n<ul>\n<li><p>info</p>\n</li>\n<li><p>version</p>\n</li>\n</ul>\n</li>\n<li><p>life-cycle</p>\n<ul>\n<li><p>create:  (ini: stop)</p>\n<pre><code>--restart: check for exit code then restart container; always or on-failure; on-failure:5 restart 5 times max\n\nsudo docker run --restart=always --name docker_restart -d ubuntu /bin/sh -c &quot;while true;do echo hello world;sleep 1;done&quot;\n</code></pre></li>\n<li><p>exec: exec cmd insid container</p>\n<pre><code>sudo docker exec -d daemon_dave touch /etc/new_config_file\nsudo docker exec -t -i daemon_dave /bin/bash\n</code></pre></li>\n<li><p>kill: send SIGKILL signal to container process</p>\n</li>\n<li><p>pause</p>\n</li>\n<li><p>restart</p>\n</li>\n<li><p>rm: cannot remove a running container; docker stop or kill first or docker rm -f bad_ubuntu</p>\n<p>  -q: list only container ids;<br>  delete all container at once:</p>\n<pre><code>docker rm `docker ps -a -q`\n</code></pre></li>\n<li><p>run: <a href=\"https://docs.docker.com/engine/reference/run/\" target=\"_blank\" rel=\"external\">reference</a>;(ini: run); </p>\n<p>  equals: docker create &amp; docker start</p>\n<p>  2 types of container</p>\n<ul>\n<li><p>interactive<br>  -i: STDIN<br>  -t: open terminal<br>  exit?: docker stop or kill;exit</p>\n<pre><code>sudo docker run -i -t --name=inspect_shell ubuntu /bin/bash\ninspect_shell: container name\nbase image: ubuntu\ncommand: /bin/bash\nfile system: image+writable layer\nnetwork: virtual network interface bridge to host &amp; set a IP\n</code></pre></li>\n<li><p>daemon: -d<br>  exit?: docker stop or kill</p>\n<pre><code>sudo docker run --name daemon_while -d ubuntu /bin/sh -c &quot;while true; do echo hello world; sleep 1; done&quot;\n\nreturn token\n\ndocker ps\n</code></pre></li>\n</ul>\n</li>\n<li><p>start: start existing container</p>\n<pre><code>sudo docker start inspect_shell or cid\n</code></pre></li>\n<li><p>stop: works for both interactive and daemon container;send SIGTERM signal to container process</p>\n<pre><code>sudo docker stop daemon_while\nsudo docker stop s39c938dj34489d\n</code></pre></li>\n<li><p>unpause</p>\n</li>\n</ul>\n</li>\n<li><p>registry</p>\n<ul>\n<li><p>login</p>\n</li>\n<li><p>logout</p>\n</li>\n<li><p>pull</p>\n</li>\n<li><p>push</p>\n</li>\n<li><p>search</p>\n</li>\n</ul>\n</li>\n<li><p>image</p>\n<ul>\n<li><p>build</p>\n</li>\n<li><p>images</p>\n</li>\n<li><p>import:             </p>\n<pre><code>cat my_container.rar | sudo docker import - imported:container\nrepository: imported, tag: container\ndocker import url res:tag\n</code></pre></li>\n<li><p>load</p>\n</li>\n<li><p>rmi</p>\n</li>\n<li><p>save</p>\n</li>\n<li><p>tag</p>\n</li>\n<li><p>commit</p>\n</li>\n</ul>\n</li>\n<li><p>container</p>\n<ul>\n<li><p>attach: attach terminal to interactive container</p>\n</li>\n<li><p>export</p>\n<pre><code>sudo docker run -i -t --nam=inspect_import ubuntu /bin/bash\n#... do something\nsudo docker export inspect_import &gt; my_container.tar\n</code></pre></li>\n<li><p>inspect: check out the configuration</p>\n<pre><code>sudo docker inspect daemon_dave\n</code></pre><p>  -f or –format:</p>\n<pre><code>sudo docker inspect --format=&apos;{{ .State.Running }}&apos; daemon_dave\n</code></pre></li>\n<li><p>port</p>\n</li>\n<li><p>ps: checkout existing container</p>\n<p>  -a: all</p>\n<pre><code>Exited(0): exit\n</code></pre><p>  -l: latest container<br>  -n=x: latest x container</p>\n</li>\n<li><p>rename</p>\n</li>\n<li><p>stats</p>\n</li>\n<li><p>top: check out UID PID PPID…</p>\n<pre><code>sudo docker run -d --name=&quot;daemon_top&quot; ubuntu /bin/bash -c &apos;while true;do sleep 1;done&apos;\n</code></pre><p>  2 process:</p>\n<pre><code>sudo docker top daemon_top\n</code></pre></li>\n<li><p>wait</p>\n</li>\n<li><p>cp</p>\n</li>\n<li><p>diff</p>\n</li>\n</ul>\n</li>\n<li><p>sys log</p>\n<ul>\n<li><p>events</p>\n</li>\n<li><p>history</p>\n</li>\n<li><p>logs</p>\n<p>  -f: realtime<br>  –tail=x: last x line</p>\n<pre><code>sudo docker logs -f --tail=5 -t daemon_logs\n</code></pre></li>\n</ul>\n</li>\n<li><p>other</p>\n<ul>\n<li><p>docker daemon: <a href=\"https://docs.docker.com/engine/reference/commandline/daemon/\" target=\"_blank\" rel=\"external\">link</a>, A self-sufficient runtime for linux containers.</p>\n<p>  The Docker daemon can listen for Docker Remote API requests via three different types of Socket: unix, tcp, and fd.<br>  By default, a unix domain socket (or IPC socket) is created at /var/run/docker.sock, requiring either root permission, or docker group membership.</p>\n</li>\n</ul>\n</li>\n</ul>\n","excerpt":"","more":"<h2 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h2><p>source code: <a href=\"https://github.com/docker/docker\">https://github.com/docker/docker</a><br>installation: <a href=\"https://www.docker.com/products/overview\">https://www.docker.com/products/overview</a></p>\n<h2 id=\"docker-engine-command\"><a href=\"#docker-engine-command\" class=\"headerlink\" title=\"docker engine command\"></a>docker engine command</h2><ul>\n<li><p>tips</p>\n<p>  Delete all containers:</p>\n<pre><code>docker rm $(docker ps -a -q)\n</code></pre><p>  Delete all images:</p>\n<pre><code>docker rmi $(docker images -q)\n</code></pre></li>\n<li><p>env</p>\n<ul>\n<li><p>info</p>\n</li>\n<li><p>version</p>\n</li>\n</ul>\n</li>\n<li><p>life-cycle</p>\n<ul>\n<li><p>create:  (ini: stop)</p>\n<pre><code>--restart: check for exit code then restart container; always or on-failure; on-failure:5 restart 5 times max\n\nsudo docker run --restart=always --name docker_restart -d ubuntu /bin/sh -c &quot;while true;do echo hello world;sleep 1;done&quot;\n</code></pre></li>\n<li><p>exec: exec cmd insid container</p>\n<pre><code>sudo docker exec -d daemon_dave touch /etc/new_config_file\nsudo docker exec -t -i daemon_dave /bin/bash\n</code></pre></li>\n<li><p>kill: send SIGKILL signal to container process</p>\n</li>\n<li><p>pause</p>\n</li>\n<li><p>restart</p>\n</li>\n<li><p>rm: cannot remove a running container; docker stop or kill first or docker rm -f bad_ubuntu</p>\n<p>  -q: list only container ids;<br>  delete all container at once:</p>\n<pre><code>docker rm `docker ps -a -q`\n</code></pre></li>\n<li><p>run: <a href=\"https://docs.docker.com/engine/reference/run/\">reference</a>;(ini: run); </p>\n<p>  equals: docker create &amp; docker start</p>\n<p>  2 types of container</p>\n<ul>\n<li><p>interactive<br>  -i: STDIN<br>  -t: open terminal<br>  exit?: docker stop or kill;exit</p>\n<pre><code>sudo docker run -i -t --name=inspect_shell ubuntu /bin/bash\ninspect_shell: container name\nbase image: ubuntu\ncommand: /bin/bash\nfile system: image+writable layer\nnetwork: virtual network interface bridge to host &amp; set a IP\n</code></pre></li>\n<li><p>daemon: -d<br>  exit?: docker stop or kill</p>\n<pre><code>sudo docker run --name daemon_while -d ubuntu /bin/sh -c &quot;while true; do echo hello world; sleep 1; done&quot;\n\nreturn token\n\ndocker ps\n</code></pre></li>\n</ul>\n</li>\n<li><p>start: start existing container</p>\n<pre><code>sudo docker start inspect_shell or cid\n</code></pre></li>\n<li><p>stop: works for both interactive and daemon container;send SIGTERM signal to container process</p>\n<pre><code>sudo docker stop daemon_while\nsudo docker stop s39c938dj34489d\n</code></pre></li>\n<li><p>unpause</p>\n</li>\n</ul>\n</li>\n<li><p>registry</p>\n<ul>\n<li><p>login</p>\n</li>\n<li><p>logout</p>\n</li>\n<li><p>pull</p>\n</li>\n<li><p>push</p>\n</li>\n<li><p>search</p>\n</li>\n</ul>\n</li>\n<li><p>image</p>\n<ul>\n<li><p>build</p>\n</li>\n<li><p>images</p>\n</li>\n<li><p>import:             </p>\n<pre><code>cat my_container.rar | sudo docker import - imported:container\nrepository: imported, tag: container\ndocker import url res:tag\n</code></pre></li>\n<li><p>load</p>\n</li>\n<li><p>rmi</p>\n</li>\n<li><p>save</p>\n</li>\n<li><p>tag</p>\n</li>\n<li><p>commit</p>\n</li>\n</ul>\n</li>\n<li><p>container</p>\n<ul>\n<li><p>attach: attach terminal to interactive container</p>\n</li>\n<li><p>export</p>\n<pre><code>sudo docker run -i -t --nam=inspect_import ubuntu /bin/bash\n#... do something\nsudo docker export inspect_import &gt; my_container.tar\n</code></pre></li>\n<li><p>inspect: check out the configuration</p>\n<pre><code>sudo docker inspect daemon_dave\n</code></pre><p>  -f or –format:</p>\n<pre><code>sudo docker inspect --format=&apos;{{ .State.Running }}&apos; daemon_dave\n</code></pre></li>\n<li><p>port</p>\n</li>\n<li><p>ps: checkout existing container</p>\n<p>  -a: all</p>\n<pre><code>Exited(0): exit\n</code></pre><p>  -l: latest container<br>  -n=x: latest x container</p>\n</li>\n<li><p>rename</p>\n</li>\n<li><p>stats</p>\n</li>\n<li><p>top: check out UID PID PPID…</p>\n<pre><code>sudo docker run -d --name=&quot;daemon_top&quot; ubuntu /bin/bash -c &apos;while true;do sleep 1;done&apos;\n</code></pre><p>  2 process:</p>\n<pre><code>sudo docker top daemon_top\n</code></pre></li>\n<li><p>wait</p>\n</li>\n<li><p>cp</p>\n</li>\n<li><p>diff</p>\n</li>\n</ul>\n</li>\n<li><p>sys log</p>\n<ul>\n<li><p>events</p>\n</li>\n<li><p>history</p>\n</li>\n<li><p>logs</p>\n<p>  -f: realtime<br>  –tail=x: last x line</p>\n<pre><code>sudo docker logs -f --tail=5 -t daemon_logs\n</code></pre></li>\n</ul>\n</li>\n<li><p>other</p>\n<ul>\n<li><p>docker daemon: <a href=\"https://docs.docker.com/engine/reference/commandline/daemon/\">link</a>, A self-sufficient runtime for linux containers.</p>\n<p>  The Docker daemon can listen for Docker Remote API requests via three different types of Socket: unix, tcp, and fd.<br>  By default, a unix domain socket (or IPC socket) is created at /var/run/docker.sock, requiring either root permission, or docker group membership.</p>\n</li>\n</ul>\n</li>\n</ul>\n"},{"title":"docker filesystem","_content":"\n# about\n\n\n# AuFS\n\n- about\n    \n    http://aufs.sourceforge.net/\n    \n    layered file system\n    \n    AuFS is a layered file system, so you can have a read only part, and a write part, and merge those together. So you could have the common parts of the operating system as read only, which are shared amongst all of your containers, and then give each container its own mount for writing.So let's say you have a container image that is 1GB in size. If you wanted to use a Full VM, you would need to have 1GB times x number of VMs you want. With LXC and AuFS you can share the bulk of the 1GB and if you have 1000 containers you still might only have a little over 1GB of space for the containers OS, assuming they are all running the same OS image.\n\n# ceph\n\n- about\n\n    一个 Linux PB 级分布式文件系统\n    Ceph is a distributed object store and file system designed to provide excellent performance, reliability and scalability.\n\n- feature\n\n    - 以时间复杂度为O(1)的方式提供消息持久化能力，即使对TB级以上数据也能保证常数时间复杂度的访问性能。\n    - 高吞吐率。即使在非常廉价的商用机器上也能做到单机支持每秒100K条以上消息的传输。\n    - 支持Kafka Server间的消息分区，及分布式消费，同时保证每个Partition内的消息顺序传输。\n    - 同时支持离线数据处理和实时数据处理。\n    - Scale out：支持在线水平扩展。\n\n- resource\n\n    link： http://ceph.com/\n    doc： http://docs.openfans.org/ceph\n\n- core\n\n- components\n    \n    cluster monitors\n    clients\n    metadata server cluster\n    object storage cluster\n\n# overlayfs\n\n- link： https://www.kernel.org/doc/Documentation/filesystems/overlayfs.txt","source":"_posts/docker-filesystem-core.md","raw":"---\ntitle: docker filesystem\ncategories:\n- docker\ntags:\n- core\n- filesystem\n---\n\n# about\n\n\n# AuFS\n\n- about\n    \n    http://aufs.sourceforge.net/\n    \n    layered file system\n    \n    AuFS is a layered file system, so you can have a read only part, and a write part, and merge those together. So you could have the common parts of the operating system as read only, which are shared amongst all of your containers, and then give each container its own mount for writing.So let's say you have a container image that is 1GB in size. If you wanted to use a Full VM, you would need to have 1GB times x number of VMs you want. With LXC and AuFS you can share the bulk of the 1GB and if you have 1000 containers you still might only have a little over 1GB of space for the containers OS, assuming they are all running the same OS image.\n\n# ceph\n\n- about\n\n    一个 Linux PB 级分布式文件系统\n    Ceph is a distributed object store and file system designed to provide excellent performance, reliability and scalability.\n\n- feature\n\n    - 以时间复杂度为O(1)的方式提供消息持久化能力，即使对TB级以上数据也能保证常数时间复杂度的访问性能。\n    - 高吞吐率。即使在非常廉价的商用机器上也能做到单机支持每秒100K条以上消息的传输。\n    - 支持Kafka Server间的消息分区，及分布式消费，同时保证每个Partition内的消息顺序传输。\n    - 同时支持离线数据处理和实时数据处理。\n    - Scale out：支持在线水平扩展。\n\n- resource\n\n    link： http://ceph.com/\n    doc： http://docs.openfans.org/ceph\n\n- core\n\n- components\n    \n    cluster monitors\n    clients\n    metadata server cluster\n    object storage cluster\n\n# overlayfs\n\n- link： https://www.kernel.org/doc/Documentation/filesystems/overlayfs.txt","slug":"docker-filesystem-core","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-15T07:30:20.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm39c002b21svtsnp2crr","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><h1 id=\"AuFS\"><a href=\"#AuFS\" class=\"headerlink\" title=\"AuFS\"></a>AuFS</h1><ul>\n<li><p>about</p>\n<p>  <a href=\"http://aufs.sourceforge.net/\" target=\"_blank\" rel=\"external\">http://aufs.sourceforge.net/</a></p>\n<p>  layered file system</p>\n<p>  AuFS is a layered file system, so you can have a read only part, and a write part, and merge those together. So you could have the common parts of the operating system as read only, which are shared amongst all of your containers, and then give each container its own mount for writing.So let’s say you have a container image that is 1GB in size. If you wanted to use a Full VM, you would need to have 1GB times x number of VMs you want. With LXC and AuFS you can share the bulk of the 1GB and if you have 1000 containers you still might only have a little over 1GB of space for the containers OS, assuming they are all running the same OS image.</p>\n</li>\n</ul>\n<h1 id=\"ceph\"><a href=\"#ceph\" class=\"headerlink\" title=\"ceph\"></a>ceph</h1><ul>\n<li><p>about</p>\n<p>  一个 Linux PB 级分布式文件系统<br>  Ceph is a distributed object store and file system designed to provide excellent performance, reliability and scalability.</p>\n</li>\n<li><p>feature</p>\n<ul>\n<li>以时间复杂度为O(1)的方式提供消息持久化能力，即使对TB级以上数据也能保证常数时间复杂度的访问性能。</li>\n<li>高吞吐率。即使在非常廉价的商用机器上也能做到单机支持每秒100K条以上消息的传输。</li>\n<li>支持Kafka Server间的消息分区，及分布式消费，同时保证每个Partition内的消息顺序传输。</li>\n<li>同时支持离线数据处理和实时数据处理。</li>\n<li>Scale out：支持在线水平扩展。</li>\n</ul>\n</li>\n<li><p>resource</p>\n<p>  link： <a href=\"http://ceph.com/\" target=\"_blank\" rel=\"external\">http://ceph.com/</a><br>  doc： <a href=\"http://docs.openfans.org/ceph\" target=\"_blank\" rel=\"external\">http://docs.openfans.org/ceph</a></p>\n</li>\n<li><p>core</p>\n</li>\n<li><p>components</p>\n<p>  cluster monitors<br>  clients<br>  metadata server cluster<br>  object storage cluster</p>\n</li>\n</ul>\n<h1 id=\"overlayfs\"><a href=\"#overlayfs\" class=\"headerlink\" title=\"overlayfs\"></a>overlayfs</h1><ul>\n<li>link： <a href=\"https://www.kernel.org/doc/Documentation/filesystems/overlayfs.txt\" target=\"_blank\" rel=\"external\">https://www.kernel.org/doc/Documentation/filesystems/overlayfs.txt</a></li>\n</ul>\n","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><h1 id=\"AuFS\"><a href=\"#AuFS\" class=\"headerlink\" title=\"AuFS\"></a>AuFS</h1><ul>\n<li><p>about</p>\n<p>  <a href=\"http://aufs.sourceforge.net/\">http://aufs.sourceforge.net/</a></p>\n<p>  layered file system</p>\n<p>  AuFS is a layered file system, so you can have a read only part, and a write part, and merge those together. So you could have the common parts of the operating system as read only, which are shared amongst all of your containers, and then give each container its own mount for writing.So let’s say you have a container image that is 1GB in size. If you wanted to use a Full VM, you would need to have 1GB times x number of VMs you want. With LXC and AuFS you can share the bulk of the 1GB and if you have 1000 containers you still might only have a little over 1GB of space for the containers OS, assuming they are all running the same OS image.</p>\n</li>\n</ul>\n<h1 id=\"ceph\"><a href=\"#ceph\" class=\"headerlink\" title=\"ceph\"></a>ceph</h1><ul>\n<li><p>about</p>\n<p>  一个 Linux PB 级分布式文件系统<br>  Ceph is a distributed object store and file system designed to provide excellent performance, reliability and scalability.</p>\n</li>\n<li><p>feature</p>\n<ul>\n<li>以时间复杂度为O(1)的方式提供消息持久化能力，即使对TB级以上数据也能保证常数时间复杂度的访问性能。</li>\n<li>高吞吐率。即使在非常廉价的商用机器上也能做到单机支持每秒100K条以上消息的传输。</li>\n<li>支持Kafka Server间的消息分区，及分布式消费，同时保证每个Partition内的消息顺序传输。</li>\n<li>同时支持离线数据处理和实时数据处理。</li>\n<li>Scale out：支持在线水平扩展。</li>\n</ul>\n</li>\n<li><p>resource</p>\n<p>  link： <a href=\"http://ceph.com/\">http://ceph.com/</a><br>  doc： <a href=\"http://docs.openfans.org/ceph\">http://docs.openfans.org/ceph</a></p>\n</li>\n<li><p>core</p>\n</li>\n<li><p>components</p>\n<p>  cluster monitors<br>  clients<br>  metadata server cluster<br>  object storage cluster</p>\n</li>\n</ul>\n<h1 id=\"overlayfs\"><a href=\"#overlayfs\" class=\"headerlink\" title=\"overlayfs\"></a>overlayfs</h1><ul>\n<li>link： <a href=\"https://www.kernel.org/doc/Documentation/filesystems/overlayfs.txt\">https://www.kernel.org/doc/Documentation/filesystems/overlayfs.txt</a></li>\n</ul>\n"},{"title":"docker image","_content":"\n# about docker image\n\n- standard: [Docker Image Specification](https://github.com/docker/docker/blob/master/image/spec/v1.md)\n\n- layer\n\n        r & w layer-container\n        add nginx-image2\n        add nginx-image1\n        ubuntu-base image\n        kernel-bootfs\n\n- duplication while writing 写时复制机制\n\n# docker image command\n        \n- docker pull\n\n- docker run\n\n- docker images: check out\n    \n        docker images ububtu\n\n- docker inspect\n\n        docker inspect ubuntu\n\n- docker search: AUTOMATED-automatic build\n\n- docker rmi: delete image\n\n        docker rmi c03k349dfjn2\n    \n    -f if some container depends on this image:\n\n        docker rmi -f ubuntu\n    \n    delete all:\n        \n        docker rm $(docker ps -a -q)\n\n- docker commit: one way to create local image, the other way is dockerfile\n    commit changes to user image\n    \n        sudo docker run -t -i ubuntu\n        apt-get install sqlite3\n        echo 'test docker commit' >> hellodocker\n        exit\n        sudo docker commit -m=\"message\" --author=\"ag\" CONTAINERID ag/sqlite3:v1\n        sudo docker run -t -i ag/sqlite3:v1\n        cat hellodocker\n        sqlite3 -version\n\n- docker build: build image with dockerfile\n    \n    -rm=false: do not delete the tmp image while building\n\n    -t: set namespace, repo name, tag\n\n        sudo docker build -t ag/test:v1\n\n- docker tag\n\n        sudo docker tag ag/test:v1 ag/test:v2\n        (v1 and v2 will have the same image id)\n    \n    build with github:\n    \n        sudo docker build -t ag/test:v1 git://github.com/ag/dockerfile.git\n\n- docker save\n\n- docker load\n\n- docker diff\n    \n        docker diff container","source":"_posts/docker-image-detail.md","raw":"---\ntitle: docker image\ncategories:\n- docker\ntags:\n- detail\n---\n\n# about docker image\n\n- standard: [Docker Image Specification](https://github.com/docker/docker/blob/master/image/spec/v1.md)\n\n- layer\n\n        r & w layer-container\n        add nginx-image2\n        add nginx-image1\n        ubuntu-base image\n        kernel-bootfs\n\n- duplication while writing 写时复制机制\n\n# docker image command\n        \n- docker pull\n\n- docker run\n\n- docker images: check out\n    \n        docker images ububtu\n\n- docker inspect\n\n        docker inspect ubuntu\n\n- docker search: AUTOMATED-automatic build\n\n- docker rmi: delete image\n\n        docker rmi c03k349dfjn2\n    \n    -f if some container depends on this image:\n\n        docker rmi -f ubuntu\n    \n    delete all:\n        \n        docker rm $(docker ps -a -q)\n\n- docker commit: one way to create local image, the other way is dockerfile\n    commit changes to user image\n    \n        sudo docker run -t -i ubuntu\n        apt-get install sqlite3\n        echo 'test docker commit' >> hellodocker\n        exit\n        sudo docker commit -m=\"message\" --author=\"ag\" CONTAINERID ag/sqlite3:v1\n        sudo docker run -t -i ag/sqlite3:v1\n        cat hellodocker\n        sqlite3 -version\n\n- docker build: build image with dockerfile\n    \n    -rm=false: do not delete the tmp image while building\n\n    -t: set namespace, repo name, tag\n\n        sudo docker build -t ag/test:v1\n\n- docker tag\n\n        sudo docker tag ag/test:v1 ag/test:v2\n        (v1 and v2 will have the same image id)\n    \n    build with github:\n    \n        sudo docker build -t ag/test:v1 git://github.com/ag/dockerfile.git\n\n- docker save\n\n- docker load\n\n- docker diff\n    \n        docker diff container","slug":"docker-image-detail","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-13T11:50:32.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm39e002f21svqd89nzvh","content":"<h1 id=\"about-docker-image\"><a href=\"#about-docker-image\" class=\"headerlink\" title=\"about docker image\"></a>about docker image</h1><ul>\n<li><p>standard: <a href=\"https://github.com/docker/docker/blob/master/image/spec/v1.md\" target=\"_blank\" rel=\"external\">Docker Image Specification</a></p>\n</li>\n<li><p>layer</p>\n<pre><code>r &amp; w layer-container\nadd nginx-image2\nadd nginx-image1\nubuntu-base image\nkernel-bootfs\n</code></pre></li>\n<li><p>duplication while writing 写时复制机制</p>\n</li>\n</ul>\n<h1 id=\"docker-image-command\"><a href=\"#docker-image-command\" class=\"headerlink\" title=\"docker image command\"></a>docker image command</h1><ul>\n<li><p>docker pull</p>\n</li>\n<li><p>docker run</p>\n</li>\n<li><p>docker images: check out</p>\n<pre><code>docker images ububtu\n</code></pre></li>\n<li><p>docker inspect</p>\n<pre><code>docker inspect ubuntu\n</code></pre></li>\n<li><p>docker search: AUTOMATED-automatic build</p>\n</li>\n<li><p>docker rmi: delete image</p>\n<pre><code>docker rmi c03k349dfjn2\n</code></pre><p>  -f if some container depends on this image:</p>\n<pre><code>docker rmi -f ubuntu\n</code></pre><p>  delete all:</p>\n<pre><code>docker rm $(docker ps -a -q)\n</code></pre></li>\n<li><p>docker commit: one way to create local image, the other way is dockerfile<br>  commit changes to user image</p>\n<pre><code>sudo docker run -t -i ubuntu\napt-get install sqlite3\necho &apos;test docker commit&apos; &gt;&gt; hellodocker\nexit\nsudo docker commit -m=&quot;message&quot; --author=&quot;ag&quot; CONTAINERID ag/sqlite3:v1\nsudo docker run -t -i ag/sqlite3:v1\ncat hellodocker\nsqlite3 -version\n</code></pre></li>\n<li><p>docker build: build image with dockerfile</p>\n<p>  -rm=false: do not delete the tmp image while building</p>\n<p>  -t: set namespace, repo name, tag</p>\n<pre><code>sudo docker build -t ag/test:v1\n</code></pre></li>\n<li><p>docker tag</p>\n<pre><code>sudo docker tag ag/test:v1 ag/test:v2\n(v1 and v2 will have the same image id)\n</code></pre><p>  build with github:</p>\n<pre><code>sudo docker build -t ag/test:v1 git://github.com/ag/dockerfile.git\n</code></pre></li>\n<li><p>docker save</p>\n</li>\n<li><p>docker load</p>\n</li>\n<li><p>docker diff</p>\n<pre><code>docker diff container\n</code></pre></li>\n</ul>\n","excerpt":"","more":"<h1 id=\"about-docker-image\"><a href=\"#about-docker-image\" class=\"headerlink\" title=\"about docker image\"></a>about docker image</h1><ul>\n<li><p>standard: <a href=\"https://github.com/docker/docker/blob/master/image/spec/v1.md\">Docker Image Specification</a></p>\n</li>\n<li><p>layer</p>\n<pre><code>r &amp; w layer-container\nadd nginx-image2\nadd nginx-image1\nubuntu-base image\nkernel-bootfs\n</code></pre></li>\n<li><p>duplication while writing 写时复制机制</p>\n</li>\n</ul>\n<h1 id=\"docker-image-command\"><a href=\"#docker-image-command\" class=\"headerlink\" title=\"docker image command\"></a>docker image command</h1><ul>\n<li><p>docker pull</p>\n</li>\n<li><p>docker run</p>\n</li>\n<li><p>docker images: check out</p>\n<pre><code>docker images ububtu\n</code></pre></li>\n<li><p>docker inspect</p>\n<pre><code>docker inspect ubuntu\n</code></pre></li>\n<li><p>docker search: AUTOMATED-automatic build</p>\n</li>\n<li><p>docker rmi: delete image</p>\n<pre><code>docker rmi c03k349dfjn2\n</code></pre><p>  -f if some container depends on this image:</p>\n<pre><code>docker rmi -f ubuntu\n</code></pre><p>  delete all:</p>\n<pre><code>docker rm $(docker ps -a -q)\n</code></pre></li>\n<li><p>docker commit: one way to create local image, the other way is dockerfile<br>  commit changes to user image</p>\n<pre><code>sudo docker run -t -i ubuntu\napt-get install sqlite3\necho &apos;test docker commit&apos; &gt;&gt; hellodocker\nexit\nsudo docker commit -m=&quot;message&quot; --author=&quot;ag&quot; CONTAINERID ag/sqlite3:v1\nsudo docker run -t -i ag/sqlite3:v1\ncat hellodocker\nsqlite3 -version\n</code></pre></li>\n<li><p>docker build: build image with dockerfile</p>\n<p>  -rm=false: do not delete the tmp image while building</p>\n<p>  -t: set namespace, repo name, tag</p>\n<pre><code>sudo docker build -t ag/test:v1\n</code></pre></li>\n<li><p>docker tag</p>\n<pre><code>sudo docker tag ag/test:v1 ag/test:v2\n(v1 and v2 will have the same image id)\n</code></pre><p>  build with github:</p>\n<pre><code>sudo docker build -t ag/test:v1 git://github.com/ag/dockerfile.git\n</code></pre></li>\n<li><p>docker save</p>\n</li>\n<li><p>docker load</p>\n</li>\n<li><p>docker diff</p>\n<pre><code>docker diff container\n</code></pre></li>\n</ul>\n"},{"title":"k8s core","_content":"\n#  core","source":"_posts/docker-k8s-core.md","raw":"---\ntitle: k8s core\ncategories:\n- docker\ntags:\n- core\n---\n\n#  core","slug":"docker-k8s-core","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:32:11.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm39i002i21sv9msto5pk","content":"<h1 id=\"core\"><a href=\"#core\" class=\"headerlink\" title=\"core\"></a>core</h1>","excerpt":"","more":"<h1 id=\"core\"><a href=\"#core\" class=\"headerlink\" title=\"core\"></a>core</h1>"},{"title":"docker machine","_content":"\n# about\ncode: https://github.com/docker/machine\ndoc: https://docs.docker.com/machine/overview/\n\n# faq\n\n- What’s the difference between Docker Engine and Docker Machine?\n\nWhen people say “Docker” they typically mean Docker Engine, the client-server application made up of the Docker daemon, a REST API that specifies interfaces for interacting with the daemon, and a command line interface (CLI) client that talks to the daemon (through the REST API wrapper). Docker Engine accepts docker commands from the CLI, such as docker run <image>, docker ps to list running containers, docker images to list images, and so on.","source":"_posts/docker-machine-detail.md","raw":"---\ntitle: docker machine\ncategories:\n- docker\ntags:\n- detail\n---\n\n# about\ncode: https://github.com/docker/machine\ndoc: https://docs.docker.com/machine/overview/\n\n# faq\n\n- What’s the difference between Docker Engine and Docker Machine?\n\nWhen people say “Docker” they typically mean Docker Engine, the client-server application made up of the Docker daemon, a REST API that specifies interfaces for interacting with the daemon, and a command line interface (CLI) client that talks to the daemon (through the REST API wrapper). Docker Engine accepts docker commands from the CLI, such as docker run <image>, docker ps to list running containers, docker images to list images, and so on.","slug":"docker-machine-detail","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-02-10T05:48:40.000Z","_id":"ciypwm39m002n21svzvub4bz1","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><p>code: <a href=\"https://github.com/docker/machine\" target=\"_blank\" rel=\"external\">https://github.com/docker/machine</a><br>doc: <a href=\"https://docs.docker.com/machine/overview/\" target=\"_blank\" rel=\"external\">https://docs.docker.com/machine/overview/</a></p>\n<h1 id=\"faq\"><a href=\"#faq\" class=\"headerlink\" title=\"faq\"></a>faq</h1><ul>\n<li>What’s the difference between Docker Engine and Docker Machine?</li>\n</ul>\n<p>When people say “Docker” they typically mean Docker Engine, the client-server application made up of the Docker daemon, a REST API that specifies interfaces for interacting with the daemon, and a command line interface (CLI) client that talks to the daemon (through the REST API wrapper). Docker Engine accepts docker commands from the CLI, such as docker run <image>, docker ps to list running containers, docker images to list images, and so on.</image></p>\n","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><p>code: <a href=\"https://github.com/docker/machine\">https://github.com/docker/machine</a><br>doc: <a href=\"https://docs.docker.com/machine/overview/\">https://docs.docker.com/machine/overview/</a></p>\n<h1 id=\"faq\"><a href=\"#faq\" class=\"headerlink\" title=\"faq\"></a>faq</h1><ul>\n<li>What’s the difference between Docker Engine and Docker Machine?</li>\n</ul>\n<p>When people say “Docker” they typically mean Docker Engine, the client-server application made up of the Docker daemon, a REST API that specifies interfaces for interacting with the daemon, and a command line interface (CLI) client that talks to the daemon (through the REST API wrapper). Docker Engine accepts docker commands from the CLI, such as docker run <image>, docker ps to list running containers, docker images to list images, and so on.</p>\n"},{"title":"mesos core","_content":"\n#  core\n\nhttps://github.com/Dataman-Cloud/Mesos-CN/blob/master/SUMMARY.md","source":"_posts/docker-mesos-core.md","raw":"---\ntitle: mesos core\ncategories:\n- docker\ntags:\n- core\n---\n\n#  core\n\nhttps://github.com/Dataman-Cloud/Mesos-CN/blob/master/SUMMARY.md","slug":"docker-mesos-core","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-02-10T06:16:56.000Z","_id":"ciypwm39p002q21svs69ki7jr","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"core\"><a href=\"#core\" class=\"headerlink\" title=\"core\"></a>core</h1><p><a href=\"https://github.com/Dataman-Cloud/Mesos-CN/blob/master/SUMMARY.md\" target=\"_blank\" rel=\"external\">https://github.com/Dataman-Cloud/Mesos-CN/blob/master/SUMMARY.md</a></p>\n","excerpt":"","more":"<h1 id=\"core\"><a href=\"#core\" class=\"headerlink\" title=\"core\"></a>core</h1><p><a href=\"https://github.com/Dataman-Cloud/Mesos-CN/blob/master/SUMMARY.md\">https://github.com/Dataman-Cloud/Mesos-CN/blob/master/SUMMARY.md</a></p>\n"},{"title":"docker security","_content":"\n# about\n\n# cgroups\n\n# capability\n\n# notary\nhttps://github.com/docker/notary","source":"_posts/docker-security-core.md","raw":"---\ntitle: docker security\ncategories:\n- docker\ntags:\n- core\n- security\n---\n\n# about\n\n# cgroups\n\n# capability\n\n# notary\nhttps://github.com/docker/notary","slug":"docker-security-core","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-15T05:26:24.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm39s002u21svylq4icy5","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><h1 id=\"cgroups\"><a href=\"#cgroups\" class=\"headerlink\" title=\"cgroups\"></a>cgroups</h1><h1 id=\"capability\"><a href=\"#capability\" class=\"headerlink\" title=\"capability\"></a>capability</h1><h1 id=\"notary\"><a href=\"#notary\" class=\"headerlink\" title=\"notary\"></a>notary</h1><p><a href=\"https://github.com/docker/notary\" target=\"_blank\" rel=\"external\">https://github.com/docker/notary</a></p>\n","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><h1 id=\"cgroups\"><a href=\"#cgroups\" class=\"headerlink\" title=\"cgroups\"></a>cgroups</h1><h1 id=\"capability\"><a href=\"#capability\" class=\"headerlink\" title=\"capability\"></a>capability</h1><h1 id=\"notary\"><a href=\"#notary\" class=\"headerlink\" title=\"notary\"></a>notary</h1><p><a href=\"https://github.com/docker/notary\">https://github.com/docker/notary</a></p>\n"},{"title":"docker network","_content":"\n# about\n\n- docker daemon ini process(docker -d)\n\n    [/var/lib/docker|116d5cd4] +job init_networkdriver()\n    [/var/lib/docker|116d5cd4.init_networkdriver()] creating new bridge for docker0\n    [/var/lib/docker|116d5cd4.init_networkdriver()] getting iface addr\n    [/var/lib/docker|116d5cd4] -job init_networkdriver() = OK (0)\n\n- default mode\n    bridge\n        docker0\n\n- expose port\n\n    -P: randomly expose a port between 49000-49900\n            sudo docker run -d -P traning/webapp python app.py\n    -p:\n            ip:hostPort:containerPost | ip::containerPort | hostPort:containerPort\n\n- check out network setting\n\n        sudo docker inspect --format '{{.NetworkSettings}}' CID\n\n- container link\n    \n    about: docker0 bridge; iptables\n\n    --link name:alias\n\n            sudo docker -d --name dbdata training/postgres\n            sudo docker run -d -P --name web --link dbdata:db training/webapp python app.py\n            sudo docker inspect web\n        \n        Links: /dbdata:/web/db\n\n        how web container use dbdata:\n            \n            - env variable\n                sudo docker run --rm --name web2 --link dbdata:webdb training/webapp env\n                <name>_PORT_<port>_<protocol>_ADDR/PORT/PROTO\n            \n            - /etc/hosts\n\n    - ambassador\n        about: 代理连接\n        connect redis client and server via 2 ambassador\n                sudo docker run -d --name redis ag/redis\n                sudo docker run -d --name redis ag/redis\n\n# docker network design\n\nhttps://blog.docker.com/2016/03/docker-networking-design-philosophy/\n\n# cnm design\n\nhttps://github.com/docker/libnetwork/blob/master/docs/design.md\n\n# docker network command\n\n- docker network ls\n\n- docker network create\n\n- docker network connect\n\n- docker network disconnect\n\n- docker network inspect\n\n- docker network rm\n\n# docker network api\n\n- network driver api\n\n- IPAM api\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","source":"_posts/docker-network-core.md","raw":"---\ntitle: docker network\ncategories:\n- docker\ntags:\n- core\n- network\n---\n\n# about\n\n- docker daemon ini process(docker -d)\n\n    [/var/lib/docker|116d5cd4] +job init_networkdriver()\n    [/var/lib/docker|116d5cd4.init_networkdriver()] creating new bridge for docker0\n    [/var/lib/docker|116d5cd4.init_networkdriver()] getting iface addr\n    [/var/lib/docker|116d5cd4] -job init_networkdriver() = OK (0)\n\n- default mode\n    bridge\n        docker0\n\n- expose port\n\n    -P: randomly expose a port between 49000-49900\n            sudo docker run -d -P traning/webapp python app.py\n    -p:\n            ip:hostPort:containerPost | ip::containerPort | hostPort:containerPort\n\n- check out network setting\n\n        sudo docker inspect --format '{{.NetworkSettings}}' CID\n\n- container link\n    \n    about: docker0 bridge; iptables\n\n    --link name:alias\n\n            sudo docker -d --name dbdata training/postgres\n            sudo docker run -d -P --name web --link dbdata:db training/webapp python app.py\n            sudo docker inspect web\n        \n        Links: /dbdata:/web/db\n\n        how web container use dbdata:\n            \n            - env variable\n                sudo docker run --rm --name web2 --link dbdata:webdb training/webapp env\n                <name>_PORT_<port>_<protocol>_ADDR/PORT/PROTO\n            \n            - /etc/hosts\n\n    - ambassador\n        about: 代理连接\n        connect redis client and server via 2 ambassador\n                sudo docker run -d --name redis ag/redis\n                sudo docker run -d --name redis ag/redis\n\n# docker network design\n\nhttps://blog.docker.com/2016/03/docker-networking-design-philosophy/\n\n# cnm design\n\nhttps://github.com/docker/libnetwork/blob/master/docs/design.md\n\n# docker network command\n\n- docker network ls\n\n- docker network create\n\n- docker network connect\n\n- docker network disconnect\n\n- docker network inspect\n\n- docker network rm\n\n# docker network api\n\n- network driver api\n\n- IPAM api\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","slug":"docker-network-core","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-15T05:25:33.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm39x002x21svqqrk3m16","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><ul>\n<li><p>docker daemon ini process(docker -d)</p>\n<p>  [/var/lib/docker|116d5cd4] +job init_networkdriver()<br>  [/var/lib/docker|116d5cd4.init_networkdriver()] creating new bridge for docker0<br>  [/var/lib/docker|116d5cd4.init_networkdriver()] getting iface addr<br>  [/var/lib/docker|116d5cd4] -job init_networkdriver() = OK (0)</p>\n</li>\n<li><p>default mode<br>  bridge</p>\n<pre><code>docker0\n</code></pre></li>\n<li><p>expose port</p>\n<p>  -P: randomly expose a port between 49000-49900</p>\n<pre><code>sudo docker run -d -P traning/webapp python app.py\n</code></pre><p>  -p:</p>\n<pre><code>ip:hostPort:containerPost | ip::containerPort | hostPort:containerPort\n</code></pre></li>\n<li><p>check out network setting</p>\n<pre><code>sudo docker inspect --format &apos;{{.NetworkSettings}}&apos; CID\n</code></pre></li>\n<li><p>container link</p>\n<p>  about: docker0 bridge; iptables</p>\n<p>  –link name:alias</p>\n<pre><code>    sudo docker -d --name dbdata training/postgres\n    sudo docker run -d -P --name web --link dbdata:db training/webapp python app.py\n    sudo docker inspect web\n\nLinks: /dbdata:/web/db\n\nhow web container use dbdata:\n\n    - env variable\n        sudo docker run --rm --name web2 --link dbdata:webdb training/webapp env\n        &lt;name&gt;_PORT_&lt;port&gt;_&lt;protocol&gt;_ADDR/PORT/PROTO\n\n    - /etc/hosts\n</code></pre><ul>\n<li>ambassador<br>  about: 代理连接<br>  connect redis client and server via 2 ambassador<pre><code>sudo docker run -d --name redis ag/redis\nsudo docker run -d --name redis ag/redis\n</code></pre></li>\n</ul>\n</li>\n</ul>\n<h1 id=\"docker-network-design\"><a href=\"#docker-network-design\" class=\"headerlink\" title=\"docker network design\"></a>docker network design</h1><p><a href=\"https://blog.docker.com/2016/03/docker-networking-design-philosophy/\" target=\"_blank\" rel=\"external\">https://blog.docker.com/2016/03/docker-networking-design-philosophy/</a></p>\n<h1 id=\"cnm-design\"><a href=\"#cnm-design\" class=\"headerlink\" title=\"cnm design\"></a>cnm design</h1><p><a href=\"https://github.com/docker/libnetwork/blob/master/docs/design.md\" target=\"_blank\" rel=\"external\">https://github.com/docker/libnetwork/blob/master/docs/design.md</a></p>\n<h1 id=\"docker-network-command\"><a href=\"#docker-network-command\" class=\"headerlink\" title=\"docker network command\"></a>docker network command</h1><ul>\n<li><p>docker network ls</p>\n</li>\n<li><p>docker network create</p>\n</li>\n<li><p>docker network connect</p>\n</li>\n<li><p>docker network disconnect</p>\n</li>\n<li><p>docker network inspect</p>\n</li>\n<li><p>docker network rm</p>\n</li>\n</ul>\n<h1 id=\"docker-network-api\"><a href=\"#docker-network-api\" class=\"headerlink\" title=\"docker network api\"></a>docker network api</h1><ul>\n<li><p>network driver api</p>\n</li>\n<li><p>IPAM api</p>\n</li>\n</ul>\n","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><ul>\n<li><p>docker daemon ini process(docker -d)</p>\n<p>  [/var/lib/docker|116d5cd4] +job init_networkdriver()<br>  [/var/lib/docker|116d5cd4.init_networkdriver()] creating new bridge for docker0<br>  [/var/lib/docker|116d5cd4.init_networkdriver()] getting iface addr<br>  [/var/lib/docker|116d5cd4] -job init_networkdriver() = OK (0)</p>\n</li>\n<li><p>default mode<br>  bridge</p>\n<pre><code>docker0\n</code></pre></li>\n<li><p>expose port</p>\n<p>  -P: randomly expose a port between 49000-49900</p>\n<pre><code>sudo docker run -d -P traning/webapp python app.py\n</code></pre><p>  -p:</p>\n<pre><code>ip:hostPort:containerPost | ip::containerPort | hostPort:containerPort\n</code></pre></li>\n<li><p>check out network setting</p>\n<pre><code>sudo docker inspect --format &apos;{{.NetworkSettings}}&apos; CID\n</code></pre></li>\n<li><p>container link</p>\n<p>  about: docker0 bridge; iptables</p>\n<p>  –link name:alias</p>\n<pre><code>    sudo docker -d --name dbdata training/postgres\n    sudo docker run -d -P --name web --link dbdata:db training/webapp python app.py\n    sudo docker inspect web\n\nLinks: /dbdata:/web/db\n\nhow web container use dbdata:\n\n    - env variable\n        sudo docker run --rm --name web2 --link dbdata:webdb training/webapp env\n        &lt;name&gt;_PORT_&lt;port&gt;_&lt;protocol&gt;_ADDR/PORT/PROTO\n\n    - /etc/hosts\n</code></pre><ul>\n<li>ambassador<br>  about: 代理连接<br>  connect redis client and server via 2 ambassador<pre><code>sudo docker run -d --name redis ag/redis\nsudo docker run -d --name redis ag/redis\n</code></pre></li>\n</ul>\n</li>\n</ul>\n<h1 id=\"docker-network-design\"><a href=\"#docker-network-design\" class=\"headerlink\" title=\"docker network design\"></a>docker network design</h1><p><a href=\"https://blog.docker.com/2016/03/docker-networking-design-philosophy/\">https://blog.docker.com/2016/03/docker-networking-design-philosophy/</a></p>\n<h1 id=\"cnm-design\"><a href=\"#cnm-design\" class=\"headerlink\" title=\"cnm design\"></a>cnm design</h1><p><a href=\"https://github.com/docker/libnetwork/blob/master/docs/design.md\">https://github.com/docker/libnetwork/blob/master/docs/design.md</a></p>\n<h1 id=\"docker-network-command\"><a href=\"#docker-network-command\" class=\"headerlink\" title=\"docker network command\"></a>docker network command</h1><ul>\n<li><p>docker network ls</p>\n</li>\n<li><p>docker network create</p>\n</li>\n<li><p>docker network connect</p>\n</li>\n<li><p>docker network disconnect</p>\n</li>\n<li><p>docker network inspect</p>\n</li>\n<li><p>docker network rm</p>\n</li>\n</ul>\n<h1 id=\"docker-network-api\"><a href=\"#docker-network-api\" class=\"headerlink\" title=\"docker network api\"></a>docker network api</h1><ul>\n<li><p>network driver api</p>\n</li>\n<li><p>IPAM api</p>\n</li>\n</ul>\n"},{"title":"docker storage","_content":"\n# about\n\n- data volumes 数据卷\n   \n    - create\n        \n        using dockerfile:\n            \n                VOLUME /var/lib/postgresql\n        \n        docker run -v:\n\n                docker run -d -P -v /webapp training/webapp python app.py\n                docker inspect my_data\n                docker inspect --format {{.Volums}} my_data\n    \n    - mount file\n        \n            $ sudo docker run --rm -it -v ~/.bash_history:/.bash_history ubuntu /bin/bash\n    \n    - mount folder\n        \n            $ sudo docker run -d -P --name web -v /src/webapp:/opt/webapp training/webapp python app.py\n    \n    - mount local directory\n        \n            sudo docker run -d -P --name webapp -v `pwd`:/webapp:ro training/webapp python app.py\n\n- data volume containers 数据卷容器\n    \n    - tips:\n\n            $ sudo docker run -it -v /dbdata --name dbdata training/postgres\n            sudo docker run -d --volumes-from=dbdata --name db1 training/postgres\n            sudo docker run -d --name db2 --volumes-from=dbdata training/postgres\n            sudo docker run -d --name db2 --volumes-from=db1 training/postgres\n            docker rm -v db3\n\n    - migration\n        \n        backup:\n\n                $ sudo docker run --volumes-from dbdata -v $(pwd):/backup --name worker ubuntu tar cvf /backup/backup.tar /dbdata\n                should use sudo \n            \n        restore:\n        \n                $ sudo docker run -v /dbdata --name dbdata2 ubuntu /bin/bash\n\n# link\n[Manage data in containers](https://docs.docker.com/engine/userguide/containers/dockervolumes/)\n\n# docker storage command\n\n- docker volume create\n\n- docker volume inspect\n\n- docker volume ls\n\n- docker volume rm","source":"_posts/docker-storage-core.md","raw":"---\ntitle: docker storage\ncategories:\n- docker\ntags:\n- core\n- storage\n---\n\n# about\n\n- data volumes 数据卷\n   \n    - create\n        \n        using dockerfile:\n            \n                VOLUME /var/lib/postgresql\n        \n        docker run -v:\n\n                docker run -d -P -v /webapp training/webapp python app.py\n                docker inspect my_data\n                docker inspect --format {{.Volums}} my_data\n    \n    - mount file\n        \n            $ sudo docker run --rm -it -v ~/.bash_history:/.bash_history ubuntu /bin/bash\n    \n    - mount folder\n        \n            $ sudo docker run -d -P --name web -v /src/webapp:/opt/webapp training/webapp python app.py\n    \n    - mount local directory\n        \n            sudo docker run -d -P --name webapp -v `pwd`:/webapp:ro training/webapp python app.py\n\n- data volume containers 数据卷容器\n    \n    - tips:\n\n            $ sudo docker run -it -v /dbdata --name dbdata training/postgres\n            sudo docker run -d --volumes-from=dbdata --name db1 training/postgres\n            sudo docker run -d --name db2 --volumes-from=dbdata training/postgres\n            sudo docker run -d --name db2 --volumes-from=db1 training/postgres\n            docker rm -v db3\n\n    - migration\n        \n        backup:\n\n                $ sudo docker run --volumes-from dbdata -v $(pwd):/backup --name worker ubuntu tar cvf /backup/backup.tar /dbdata\n                should use sudo \n            \n        restore:\n        \n                $ sudo docker run -v /dbdata --name dbdata2 ubuntu /bin/bash\n\n# link\n[Manage data in containers](https://docs.docker.com/engine/userguide/containers/dockervolumes/)\n\n# docker storage command\n\n- docker volume create\n\n- docker volume inspect\n\n- docker volume ls\n\n- docker volume rm","slug":"docker-storage-core","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-15T06:26:57.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm3a2003221svf7toen7b","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><ul>\n<li><p>data volumes 数据卷</p>\n<ul>\n<li><p>create</p>\n<p>  using dockerfile:</p>\n<pre><code>VOLUME /var/lib/postgresql\n</code></pre><p>  docker run -v:</p>\n<pre><code>docker run -d -P -v /webapp training/webapp python app.py\ndocker inspect my_data\ndocker inspect --format {{.Volums}} my_data\n</code></pre></li>\n<li><p>mount file</p>\n<pre><code>$ sudo docker run --rm -it -v ~/.bash_history:/.bash_history ubuntu /bin/bash\n</code></pre></li>\n<li><p>mount folder</p>\n<pre><code>$ sudo docker run -d -P --name web -v /src/webapp:/opt/webapp training/webapp python app.py\n</code></pre></li>\n<li><p>mount local directory</p>\n<pre><code>sudo docker run -d -P --name webapp -v `pwd`:/webapp:ro training/webapp python app.py\n</code></pre></li>\n</ul>\n</li>\n<li><p>data volume containers 数据卷容器</p>\n<ul>\n<li><p>tips:</p>\n<pre><code>$ sudo docker run -it -v /dbdata --name dbdata training/postgres\nsudo docker run -d --volumes-from=dbdata --name db1 training/postgres\nsudo docker run -d --name db2 --volumes-from=dbdata training/postgres\nsudo docker run -d --name db2 --volumes-from=db1 training/postgres\ndocker rm -v db3\n</code></pre></li>\n<li><p>migration</p>\n<p>  backup:</p>\n<pre><code>$ sudo docker run --volumes-from dbdata -v $(pwd):/backup --name worker ubuntu tar cvf /backup/backup.tar /dbdata\nshould use sudo \n</code></pre><p>  restore:</p>\n<pre><code>$ sudo docker run -v /dbdata --name dbdata2 ubuntu /bin/bash\n</code></pre></li>\n</ul>\n</li>\n</ul>\n<h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><p><a href=\"https://docs.docker.com/engine/userguide/containers/dockervolumes/\" target=\"_blank\" rel=\"external\">Manage data in containers</a></p>\n<h1 id=\"docker-storage-command\"><a href=\"#docker-storage-command\" class=\"headerlink\" title=\"docker storage command\"></a>docker storage command</h1><ul>\n<li><p>docker volume create</p>\n</li>\n<li><p>docker volume inspect</p>\n</li>\n<li><p>docker volume ls</p>\n</li>\n<li><p>docker volume rm</p>\n</li>\n</ul>\n","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><ul>\n<li><p>data volumes 数据卷</p>\n<ul>\n<li><p>create</p>\n<p>  using dockerfile:</p>\n<pre><code>VOLUME /var/lib/postgresql\n</code></pre><p>  docker run -v:</p>\n<pre><code>docker run -d -P -v /webapp training/webapp python app.py\ndocker inspect my_data\ndocker inspect --format {{.Volums}} my_data\n</code></pre></li>\n<li><p>mount file</p>\n<pre><code>$ sudo docker run --rm -it -v ~/.bash_history:/.bash_history ubuntu /bin/bash\n</code></pre></li>\n<li><p>mount folder</p>\n<pre><code>$ sudo docker run -d -P --name web -v /src/webapp:/opt/webapp training/webapp python app.py\n</code></pre></li>\n<li><p>mount local directory</p>\n<pre><code>sudo docker run -d -P --name webapp -v `pwd`:/webapp:ro training/webapp python app.py\n</code></pre></li>\n</ul>\n</li>\n<li><p>data volume containers 数据卷容器</p>\n<ul>\n<li><p>tips:</p>\n<pre><code>$ sudo docker run -it -v /dbdata --name dbdata training/postgres\nsudo docker run -d --volumes-from=dbdata --name db1 training/postgres\nsudo docker run -d --name db2 --volumes-from=dbdata training/postgres\nsudo docker run -d --name db2 --volumes-from=db1 training/postgres\ndocker rm -v db3\n</code></pre></li>\n<li><p>migration</p>\n<p>  backup:</p>\n<pre><code>$ sudo docker run --volumes-from dbdata -v $(pwd):/backup --name worker ubuntu tar cvf /backup/backup.tar /dbdata\nshould use sudo \n</code></pre><p>  restore:</p>\n<pre><code>$ sudo docker run -v /dbdata --name dbdata2 ubuntu /bin/bash\n</code></pre></li>\n</ul>\n</li>\n</ul>\n<h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><p><a href=\"https://docs.docker.com/engine/userguide/containers/dockervolumes/\">Manage data in containers</a></p>\n<h1 id=\"docker-storage-command\"><a href=\"#docker-storage-command\" class=\"headerlink\" title=\"docker storage command\"></a>docker storage command</h1><ul>\n<li><p>docker volume create</p>\n</li>\n<li><p>docker volume inspect</p>\n</li>\n<li><p>docker volume ls</p>\n</li>\n<li><p>docker volume rm</p>\n</li>\n</ul>\n"},{"title":"docker swarm portal","_content":"\n# about \n\nsource code: https://github.com/docker/swarmkit\ndoc: https://docs.docker.com/swarm/overview/\n\n# swarm next (swarm mode)\n\n- reference: http://mt.sohu.com/20160818/n464799101.shtml\n\nSwarmNext（Swarm 模式）是对原有Docker Swarm的巨大改善。在Docker中有服务对象将使缩放、滚动更新、服务发现、负载均衡和路由网等功能更容易实现，这也使得Swarm能够赶上Kubernetes之类的某些功能。Docker已经在1.12版中支持SwarmNext和Swarm，使得已经部署了Swarm的生产用户不会受到部分升级的影响。SwarmNext并不具有所有的功能，包括与Compose和存储插件的集成。不久这一点将被添加到SwarmNext。从长远看，我认为Swarm将会过时，而SwarmNext将会成为在Swarm中进行编排的唯一模式。Swarmkit作为一个开源项目，允许对Swarmkit进行独立开发，允许任何为分布应用开发编排系统的人将其作为一个独立模块进行使用。\n\n# swarm \n\n- link: https://docs.docker.com/engine/swarm/\n- tut: https://docs.docker.com/engine/swarm/swarm-tutorial/\n- cli: https://docs.docker.com/engine/swarm/#swarm-mode-cli-commands\n\n# swarmkit\n","source":"_posts/docker-swarm-portal.md","raw":"---\ntitle: docker swarm portal\ncategories:\n- docker\ntags:\n- detail\n---\n\n# about \n\nsource code: https://github.com/docker/swarmkit\ndoc: https://docs.docker.com/swarm/overview/\n\n# swarm next (swarm mode)\n\n- reference: http://mt.sohu.com/20160818/n464799101.shtml\n\nSwarmNext（Swarm 模式）是对原有Docker Swarm的巨大改善。在Docker中有服务对象将使缩放、滚动更新、服务发现、负载均衡和路由网等功能更容易实现，这也使得Swarm能够赶上Kubernetes之类的某些功能。Docker已经在1.12版中支持SwarmNext和Swarm，使得已经部署了Swarm的生产用户不会受到部分升级的影响。SwarmNext并不具有所有的功能，包括与Compose和存储插件的集成。不久这一点将被添加到SwarmNext。从长远看，我认为Swarm将会过时，而SwarmNext将会成为在Swarm中进行编排的唯一模式。Swarmkit作为一个开源项目，允许对Swarmkit进行独立开发，允许任何为分布应用开发编排系统的人将其作为一个独立模块进行使用。\n\n# swarm \n\n- link: https://docs.docker.com/engine/swarm/\n- tut: https://docs.docker.com/engine/swarm/swarm-tutorial/\n- cli: https://docs.docker.com/engine/swarm/#swarm-mode-cli-commands\n\n# swarmkit\n","slug":"docker-swarm-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-02-10T05:40:36.000Z","_id":"ciypwm3ae003521svyblhs7lq","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><p>source code: <a href=\"https://github.com/docker/swarmkit\" target=\"_blank\" rel=\"external\">https://github.com/docker/swarmkit</a><br>doc: <a href=\"https://docs.docker.com/swarm/overview/\" target=\"_blank\" rel=\"external\">https://docs.docker.com/swarm/overview/</a></p>\n<h1 id=\"swarm-next-swarm-mode\"><a href=\"#swarm-next-swarm-mode\" class=\"headerlink\" title=\"swarm next (swarm mode)\"></a>swarm next (swarm mode)</h1><ul>\n<li>reference: <a href=\"http://mt.sohu.com/20160818/n464799101.shtml\" target=\"_blank\" rel=\"external\">http://mt.sohu.com/20160818/n464799101.shtml</a></li>\n</ul>\n<p>SwarmNext（Swarm 模式）是对原有Docker Swarm的巨大改善。在Docker中有服务对象将使缩放、滚动更新、服务发现、负载均衡和路由网等功能更容易实现，这也使得Swarm能够赶上Kubernetes之类的某些功能。Docker已经在1.12版中支持SwarmNext和Swarm，使得已经部署了Swarm的生产用户不会受到部分升级的影响。SwarmNext并不具有所有的功能，包括与Compose和存储插件的集成。不久这一点将被添加到SwarmNext。从长远看，我认为Swarm将会过时，而SwarmNext将会成为在Swarm中进行编排的唯一模式。Swarmkit作为一个开源项目，允许对Swarmkit进行独立开发，允许任何为分布应用开发编排系统的人将其作为一个独立模块进行使用。</p>\n<h1 id=\"swarm\"><a href=\"#swarm\" class=\"headerlink\" title=\"swarm\"></a>swarm</h1><ul>\n<li>link: <a href=\"https://docs.docker.com/engine/swarm/\" target=\"_blank\" rel=\"external\">https://docs.docker.com/engine/swarm/</a></li>\n<li>tut: <a href=\"https://docs.docker.com/engine/swarm/swarm-tutorial/\" target=\"_blank\" rel=\"external\">https://docs.docker.com/engine/swarm/swarm-tutorial/</a></li>\n<li>cli: <a href=\"https://docs.docker.com/engine/swarm/#swarm-mode-cli-commands\" target=\"_blank\" rel=\"external\">https://docs.docker.com/engine/swarm/#swarm-mode-cli-commands</a></li>\n</ul>\n<h1 id=\"swarmkit\"><a href=\"#swarmkit\" class=\"headerlink\" title=\"swarmkit\"></a>swarmkit</h1>","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><p>source code: <a href=\"https://github.com/docker/swarmkit\">https://github.com/docker/swarmkit</a><br>doc: <a href=\"https://docs.docker.com/swarm/overview/\">https://docs.docker.com/swarm/overview/</a></p>\n<h1 id=\"swarm-next-swarm-mode\"><a href=\"#swarm-next-swarm-mode\" class=\"headerlink\" title=\"swarm next (swarm mode)\"></a>swarm next (swarm mode)</h1><ul>\n<li>reference: <a href=\"http://mt.sohu.com/20160818/n464799101.shtml\">http://mt.sohu.com/20160818/n464799101.shtml</a></li>\n</ul>\n<p>SwarmNext（Swarm 模式）是对原有Docker Swarm的巨大改善。在Docker中有服务对象将使缩放、滚动更新、服务发现、负载均衡和路由网等功能更容易实现，这也使得Swarm能够赶上Kubernetes之类的某些功能。Docker已经在1.12版中支持SwarmNext和Swarm，使得已经部署了Swarm的生产用户不会受到部分升级的影响。SwarmNext并不具有所有的功能，包括与Compose和存储插件的集成。不久这一点将被添加到SwarmNext。从长远看，我认为Swarm将会过时，而SwarmNext将会成为在Swarm中进行编排的唯一模式。Swarmkit作为一个开源项目，允许对Swarmkit进行独立开发，允许任何为分布应用开发编排系统的人将其作为一个独立模块进行使用。</p>\n<h1 id=\"swarm\"><a href=\"#swarm\" class=\"headerlink\" title=\"swarm\"></a>swarm</h1><ul>\n<li>link: <a href=\"https://docs.docker.com/engine/swarm/\">https://docs.docker.com/engine/swarm/</a></li>\n<li>tut: <a href=\"https://docs.docker.com/engine/swarm/swarm-tutorial/\">https://docs.docker.com/engine/swarm/swarm-tutorial/</a></li>\n<li>cli: <a href=\"https://docs.docker.com/engine/swarm/#swarm-mode-cli-commands\">https://docs.docker.com/engine/swarm/#swarm-mode-cli-commands</a></li>\n</ul>\n<h1 id=\"swarmkit\"><a href=\"#swarmkit\" class=\"headerlink\" title=\"swarmkit\"></a>swarmkit</h1>"},{"title":"mesos core","_content":"\n#  core","source":"_posts/docker-store-detail.md","raw":"---\ntitle: mesos core\ncategories:\n- docker\ntags:\n- core\n---\n\n#  core","slug":"docker-store-detail","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:32:26.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm3ai003921sv5f0f4edv","content":"<h1 id=\"core\"><a href=\"#core\" class=\"headerlink\" title=\"core\"></a>core</h1>","excerpt":"","more":"<h1 id=\"core\"><a href=\"#core\" class=\"headerlink\" title=\"core\"></a>core</h1>"},{"title":"docker swarm","_content":"\n# about \n\n\n\n# docker swarm command\n\n- docker-machine ls\n\n- docker-machine create -d virtualbox local\n\n- $(docker-machine env local) \n\n- docker run swarm create\n\n        $  docker-machine create -d virtualbox --swarm --swarm-master --swarm-discovery token://63e7a1adb607ce4db056a29b1f5d30cf swarm-master \n\n        $(docker-machine env --swarm swarm-master)\n\n- docker-machine ls\n","source":"_posts/docker-swarm-detail.md","raw":"---\ntitle: docker swarm\ncategories:\n- docker\ntags:\n- detail\n---\n\n# about \n\n\n\n# docker swarm command\n\n- docker-machine ls\n\n- docker-machine create -d virtualbox local\n\n- $(docker-machine env local) \n\n- docker run swarm create\n\n        $  docker-machine create -d virtualbox --swarm --swarm-master --swarm-discovery token://63e7a1adb607ce4db056a29b1f5d30cf swarm-master \n\n        $(docker-machine env --swarm swarm-master)\n\n- docker-machine ls\n","slug":"docker-swarm-detail","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-02-10T06:26:58.000Z","_id":"ciypwm3al003d21svs6i4yavl","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><h1 id=\"docker-swarm-command\"><a href=\"#docker-swarm-command\" class=\"headerlink\" title=\"docker swarm command\"></a>docker swarm command</h1><ul>\n<li><p>docker-machine ls</p>\n</li>\n<li><p>docker-machine create -d virtualbox local</p>\n</li>\n<li><p>$(docker-machine env local) </p>\n</li>\n<li><p>docker run swarm create</p>\n<pre><code>$  docker-machine create -d virtualbox --swarm --swarm-master --swarm-discovery token://63e7a1adb607ce4db056a29b1f5d30cf swarm-master \n\n$(docker-machine env --swarm swarm-master)\n</code></pre></li>\n<li><p>docker-machine ls</p>\n</li>\n</ul>\n","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><h1 id=\"docker-swarm-command\"><a href=\"#docker-swarm-command\" class=\"headerlink\" title=\"docker swarm command\"></a>docker swarm command</h1><ul>\n<li><p>docker-machine ls</p>\n</li>\n<li><p>docker-machine create -d virtualbox local</p>\n</li>\n<li><p>$(docker-machine env local) </p>\n</li>\n<li><p>docker run swarm create</p>\n<pre><code>$  docker-machine create -d virtualbox --swarm --swarm-master --swarm-discovery token://63e7a1adb607ce4db056a29b1f5d30cf swarm-master \n\n$(docker-machine env --swarm swarm-master)\n</code></pre></li>\n<li><p>docker-machine ls</p>\n</li>\n</ul>\n"},{"title":"openstack core","_content":"\n# about\n\n- link: https://github.com/docker/swarmkit\n\n# ","source":"_posts/docker-swarmkit-detail.md","raw":"---\ntitle: openstack core\ncategories:\n- iaas\ntags:\n- core\n- openstack\n---\n\n# about\n\n- link: https://github.com/docker/swarmkit\n\n# ","slug":"docker-swarmkit-detail","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-15T08:04:04.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm3an003h21sv7b0gw30l","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><ul>\n<li>link: <a href=\"https://github.com/docker/swarmkit\" target=\"_blank\" rel=\"external\">https://github.com/docker/swarmkit</a></li>\n</ul>\n<p># </p>\n","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><ul>\n<li>link: <a href=\"https://github.com/docker/swarmkit\">https://github.com/docker/swarmkit</a></li>\n</ul>\n<p># </p>\n"},{"title":"docker yaml file","_content":"\n# about\n\n# link\n\nhttp://www.yaml.org/\nhttps://en.wikipedia.org/wiki/YAML","source":"_posts/docker-yaml-detail.md","raw":"---\ntitle: docker yaml file\ncategories:\n- docker\ntags:\n- detail\n---\n\n# about\n\n# link\n\nhttp://www.yaml.org/\nhttps://en.wikipedia.org/wiki/YAML","slug":"docker-yaml-detail","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-15T05:31:36.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm3ao003l21svrwyk2i8l","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><p><a href=\"http://www.yaml.org/\" target=\"_blank\" rel=\"external\">http://www.yaml.org/</a><br><a href=\"https://en.wikipedia.org/wiki/YAML\" target=\"_blank\" rel=\"external\">https://en.wikipedia.org/wiki/YAML</a></p>\n","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><p><a href=\"http://www.yaml.org/\">http://www.yaml.org/</a><br><a href=\"https://en.wikipedia.org/wiki/YAML\">https://en.wikipedia.org/wiki/YAML</a></p>\n"},{"title":"Hello World","_content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n","source":"_posts/hello-world.md","raw":"---\ntitle: Hello World\ncategories:\n- tmp\n---\nWelcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n","slug":"hello-world","published":1,"date":"2017-01-11T09:26:51.000Z","updated":"2017-01-11T16:32:46.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm3as003p21sv6fbp39yp","content":"<p>Welcome to <a href=\"https://hexo.io/\" target=\"_blank\" rel=\"external\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\" target=\"_blank\" rel=\"external\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\" target=\"_blank\" rel=\"external\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\" target=\"_blank\" rel=\"external\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo new <span class=\"string\">\"My New Post\"</span></div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\" target=\"_blank\" rel=\"external\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo server</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\" target=\"_blank\" rel=\"external\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo generate</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\" target=\"_blank\" rel=\"external\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo deploy</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/deployment.html\" target=\"_blank\" rel=\"external\">Deployment</a></p>\n","excerpt":"","more":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo new <span class=\"string\">\"My New Post\"</span></div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo server</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo generate</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">$ hexo deploy</div></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/deployment.html\">Deployment</a></p>\n"},{"title":"docker swarm mode","_content":"\n# link\n\n- tut: https://docs.docker.com/engine/swarm/swarm-tutorial/\n\n\n# about\n\nDocker Engine 1.12 includes swarm mode for natively managing a cluster of Docker Engines called a swarm. Use the Docker CLI to create a swarm, deploy application services to a swarm, and manage swarm behavior.\n\n# feature\n\n    - Cluster management integrated with Docker Engine: \n\n    Use the Docker Engine CLI to create a swarm of Docker Engines where you can deploy application services. You don’t need additional orchestration software to create or manage a swarm.\n\n    - Decentralized design: \n\n    Instead of handling differentiation between node roles at deployment time, the Docker Engine handles any specialization at runtime. You can deploy both kinds of nodes, managers and workers, using the Docker Engine. This means you can build an entire swarm from a single disk image.\n\n    - Declarative service model: \n\n    Docker Engine uses a declarative approach to let you define the desired state of the various services in your application stack. For example, you might describe an application comprised of a web front end service with message queueing services and a database backend.\n\n    - Scaling: \n\n    For each service, you can declare the number of tasks you want to run. When you scale up or down, the swarm manager automatically adapts by adding or removing tasks to maintain the desired state.\n\n    - Desired state reconciliation: \n\n    The swarm manager node constantly monitors the cluster state and reconciles any differences between the actual state your expressed desired state. For example, if you set up a service to run 10 replicas of a container, and a worker machine hosting two of those replicas crashes, the manager will create two new replicas to replace the replicas that crashed. The swarm manager assigns the new replicas to workers that are running and available.\n\n    - Multi-host networking: \n\n    You can specify an overlay network for your services. The swarm manager automatically assigns addresses to the containers on the overlay network when it initializes or updates the application.\n\n    - Service discovery: \n\n    Swarm manager nodes assign each service in the swarm a unique DNS name and load balances running containers. You can query every container running in the swarm through a DNS server embedded in the swarm.\n\n    - Load balancing: \n\n    You can expose the ports for services to an external load balancer. Internally, the swarm lets you specify how to distribute service containers between nodes.\n\n    - Secure by default: \n\n    Each node in the swarm enforces TLS mutual authentication and encryption to secure communications between itself and all other nodes. You have the option to use self-signed root certificates or certificates from a custom root CA.\n\n    - Rolling updates: \n\n    At rollout time you can apply service updates to nodes incrementally. The swarm manager lets you control the delay between service deployment to different sets of nodes. If anything goes wrong, you can roll-back a task to a previous version of the service.\n\n\n# key concepts\n\n- link: https://docs.docker.com/engine/swarm/key-concepts/ (Docker Engine 1.12.)\n\n- What is a swarm?\n\n    The cluster management and orchestration features embedded in the Docker Engine are built using SwarmKit. Docker engines participating in a cluster are running in swarm mode. You enable swarm mode for an engine by either initializing a swarm or joining an existing swarm.\n\n    A swarm is a cluster of Docker engines, or nodes, where you deploy services. The Docker Engine CLI and API include commands to manage swarm nodes (e.g., add or remove nodes), and deploy and orchestrate services across the swarm.\n\n    When you run Docker without using swarm mode, you execute container commands. When you run the Docker in swarm mode, you orchestrate services. You can run swarm services and standalone containers on the same Docker instances.\n\n- What is a node?\n\n    A node is an instance of the Docker engine participating in the swarm. You can also think of this as a Docker node. You can run one or more nodes on a single physical computer or cloud server, but production swarm deployments typically include Docker nodes distributed across multiple physical and cloud machines.\n\n    To deploy your application to a swarm, you submit a service definition to a manager node. The manager node dispatches units of work called tasks to worker nodes.\n\n    Manager nodes also perform the orchestration and cluster management functions required to maintain the desired state of the swarm. Manager nodes elect a single leader to conduct orchestration tasks.\n\n    Worker nodes receive and execute tasks dispatched from manager nodes. By default manager nodes also run services as worker nodes, but you can configure them to run manager tasks exclusively and be manager-only nodes. An agent runs on each worker node and reports on the tasks assigned to it. The worker node notifies the manager node of the current state of its assigned tasks so that the manager can maintain the desired state of each worker.\n\n- Services and tasks\n\n    A service is the definition of the tasks to execute on the worker nodes. It is the central structure of the swarm system and the primary root of user interaction with the swarm.\n\n    When you create a service, you specify which container image to use and which commands to execute inside running containers.\n\n    In the replicated services model, the swarm manager distributes a specific number of replica tasks among the nodes based upon the scale you set in the desired state.\n\n    For global services, the swarm runs one task for the service on every available node in the cluster.\n\n    A task carries a Docker container and the commands to run inside the container. It is the atomic scheduling unit of swarm. Manager nodes assign tasks to worker nodes according to the number of replicas set in the service scale. Once a task is assigned to a node, it cannot move to another node. It can only run on the assigned node or fail.\n\n- Load balancing\n\n    The swarm manager uses ingress load balancing to expose the services you want to make available externally to the swarm. The swarm manager can automatically assign the service a PublishedPort or you can configure a PublishedPort for the service. You can specify any unused port. If you do not specify a port, the swarm manager assigns the service a port in the 30000-32767 range.\n\n    External components, such as cloud load balancers, can access the service on the PublishedPort of any node in the cluster whether or not the node is currently running the task for the service. All nodes in the swarm route ingress connections to a running task instance.\n\n    Swarm mode has an internal DNS component that automatically assigns each service in the swarm a DNS entry. The swarm manager uses internal load balancing to distribute requests among services within the cluster based upon the DNS name of the service.\n\n# docker swarm mode command\n\n- swarm init\n\n- swarm join\n\n- service create\n\n- service inspect\n\n- service ls\n\n- service rm\n\n- service scale\n\n- service ps\n\n- service update","source":"_posts/docker-swarmnext-detail.md","raw":"---\ntitle: docker swarm mode\ncategories:\n- docker\ntags:\n- core\n- swarm\n---\n\n# link\n\n- tut: https://docs.docker.com/engine/swarm/swarm-tutorial/\n\n\n# about\n\nDocker Engine 1.12 includes swarm mode for natively managing a cluster of Docker Engines called a swarm. Use the Docker CLI to create a swarm, deploy application services to a swarm, and manage swarm behavior.\n\n# feature\n\n    - Cluster management integrated with Docker Engine: \n\n    Use the Docker Engine CLI to create a swarm of Docker Engines where you can deploy application services. You don’t need additional orchestration software to create or manage a swarm.\n\n    - Decentralized design: \n\n    Instead of handling differentiation between node roles at deployment time, the Docker Engine handles any specialization at runtime. You can deploy both kinds of nodes, managers and workers, using the Docker Engine. This means you can build an entire swarm from a single disk image.\n\n    - Declarative service model: \n\n    Docker Engine uses a declarative approach to let you define the desired state of the various services in your application stack. For example, you might describe an application comprised of a web front end service with message queueing services and a database backend.\n\n    - Scaling: \n\n    For each service, you can declare the number of tasks you want to run. When you scale up or down, the swarm manager automatically adapts by adding or removing tasks to maintain the desired state.\n\n    - Desired state reconciliation: \n\n    The swarm manager node constantly monitors the cluster state and reconciles any differences between the actual state your expressed desired state. For example, if you set up a service to run 10 replicas of a container, and a worker machine hosting two of those replicas crashes, the manager will create two new replicas to replace the replicas that crashed. The swarm manager assigns the new replicas to workers that are running and available.\n\n    - Multi-host networking: \n\n    You can specify an overlay network for your services. The swarm manager automatically assigns addresses to the containers on the overlay network when it initializes or updates the application.\n\n    - Service discovery: \n\n    Swarm manager nodes assign each service in the swarm a unique DNS name and load balances running containers. You can query every container running in the swarm through a DNS server embedded in the swarm.\n\n    - Load balancing: \n\n    You can expose the ports for services to an external load balancer. Internally, the swarm lets you specify how to distribute service containers between nodes.\n\n    - Secure by default: \n\n    Each node in the swarm enforces TLS mutual authentication and encryption to secure communications between itself and all other nodes. You have the option to use self-signed root certificates or certificates from a custom root CA.\n\n    - Rolling updates: \n\n    At rollout time you can apply service updates to nodes incrementally. The swarm manager lets you control the delay between service deployment to different sets of nodes. If anything goes wrong, you can roll-back a task to a previous version of the service.\n\n\n# key concepts\n\n- link: https://docs.docker.com/engine/swarm/key-concepts/ (Docker Engine 1.12.)\n\n- What is a swarm?\n\n    The cluster management and orchestration features embedded in the Docker Engine are built using SwarmKit. Docker engines participating in a cluster are running in swarm mode. You enable swarm mode for an engine by either initializing a swarm or joining an existing swarm.\n\n    A swarm is a cluster of Docker engines, or nodes, where you deploy services. The Docker Engine CLI and API include commands to manage swarm nodes (e.g., add or remove nodes), and deploy and orchestrate services across the swarm.\n\n    When you run Docker without using swarm mode, you execute container commands. When you run the Docker in swarm mode, you orchestrate services. You can run swarm services and standalone containers on the same Docker instances.\n\n- What is a node?\n\n    A node is an instance of the Docker engine participating in the swarm. You can also think of this as a Docker node. You can run one or more nodes on a single physical computer or cloud server, but production swarm deployments typically include Docker nodes distributed across multiple physical and cloud machines.\n\n    To deploy your application to a swarm, you submit a service definition to a manager node. The manager node dispatches units of work called tasks to worker nodes.\n\n    Manager nodes also perform the orchestration and cluster management functions required to maintain the desired state of the swarm. Manager nodes elect a single leader to conduct orchestration tasks.\n\n    Worker nodes receive and execute tasks dispatched from manager nodes. By default manager nodes also run services as worker nodes, but you can configure them to run manager tasks exclusively and be manager-only nodes. An agent runs on each worker node and reports on the tasks assigned to it. The worker node notifies the manager node of the current state of its assigned tasks so that the manager can maintain the desired state of each worker.\n\n- Services and tasks\n\n    A service is the definition of the tasks to execute on the worker nodes. It is the central structure of the swarm system and the primary root of user interaction with the swarm.\n\n    When you create a service, you specify which container image to use and which commands to execute inside running containers.\n\n    In the replicated services model, the swarm manager distributes a specific number of replica tasks among the nodes based upon the scale you set in the desired state.\n\n    For global services, the swarm runs one task for the service on every available node in the cluster.\n\n    A task carries a Docker container and the commands to run inside the container. It is the atomic scheduling unit of swarm. Manager nodes assign tasks to worker nodes according to the number of replicas set in the service scale. Once a task is assigned to a node, it cannot move to another node. It can only run on the assigned node or fail.\n\n- Load balancing\n\n    The swarm manager uses ingress load balancing to expose the services you want to make available externally to the swarm. The swarm manager can automatically assign the service a PublishedPort or you can configure a PublishedPort for the service. You can specify any unused port. If you do not specify a port, the swarm manager assigns the service a port in the 30000-32767 range.\n\n    External components, such as cloud load balancers, can access the service on the PublishedPort of any node in the cluster whether or not the node is currently running the task for the service. All nodes in the swarm route ingress connections to a running task instance.\n\n    Swarm mode has an internal DNS component that automatically assigns each service in the swarm a DNS entry. The swarm manager uses internal load balancing to distribute requests among services within the cluster based upon the DNS name of the service.\n\n# docker swarm mode command\n\n- swarm init\n\n- swarm join\n\n- service create\n\n- service inspect\n\n- service ls\n\n- service rm\n\n- service scale\n\n- service ps\n\n- service update","slug":"docker-swarmnext-detail","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-15T13:29:09.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm3aw003s21svc9u1t1uj","content":"<h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><ul>\n<li>tut: <a href=\"https://docs.docker.com/engine/swarm/swarm-tutorial/\" target=\"_blank\" rel=\"external\">https://docs.docker.com/engine/swarm/swarm-tutorial/</a></li>\n</ul>\n<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><p>Docker Engine 1.12 includes swarm mode for natively managing a cluster of Docker Engines called a swarm. Use the Docker CLI to create a swarm, deploy application services to a swarm, and manage swarm behavior.</p>\n<h1 id=\"feature\"><a href=\"#feature\" class=\"headerlink\" title=\"feature\"></a>feature</h1><pre><code>- Cluster management integrated with Docker Engine: \n\nUse the Docker Engine CLI to create a swarm of Docker Engines where you can deploy application services. You don’t need additional orchestration software to create or manage a swarm.\n\n- Decentralized design: \n\nInstead of handling differentiation between node roles at deployment time, the Docker Engine handles any specialization at runtime. You can deploy both kinds of nodes, managers and workers, using the Docker Engine. This means you can build an entire swarm from a single disk image.\n\n- Declarative service model: \n\nDocker Engine uses a declarative approach to let you define the desired state of the various services in your application stack. For example, you might describe an application comprised of a web front end service with message queueing services and a database backend.\n\n- Scaling: \n\nFor each service, you can declare the number of tasks you want to run. When you scale up or down, the swarm manager automatically adapts by adding or removing tasks to maintain the desired state.\n\n- Desired state reconciliation: \n\nThe swarm manager node constantly monitors the cluster state and reconciles any differences between the actual state your expressed desired state. For example, if you set up a service to run 10 replicas of a container, and a worker machine hosting two of those replicas crashes, the manager will create two new replicas to replace the replicas that crashed. The swarm manager assigns the new replicas to workers that are running and available.\n\n- Multi-host networking: \n\nYou can specify an overlay network for your services. The swarm manager automatically assigns addresses to the containers on the overlay network when it initializes or updates the application.\n\n- Service discovery: \n\nSwarm manager nodes assign each service in the swarm a unique DNS name and load balances running containers. You can query every container running in the swarm through a DNS server embedded in the swarm.\n\n- Load balancing: \n\nYou can expose the ports for services to an external load balancer. Internally, the swarm lets you specify how to distribute service containers between nodes.\n\n- Secure by default: \n\nEach node in the swarm enforces TLS mutual authentication and encryption to secure communications between itself and all other nodes. You have the option to use self-signed root certificates or certificates from a custom root CA.\n\n- Rolling updates: \n\nAt rollout time you can apply service updates to nodes incrementally. The swarm manager lets you control the delay between service deployment to different sets of nodes. If anything goes wrong, you can roll-back a task to a previous version of the service.\n</code></pre><h1 id=\"key-concepts\"><a href=\"#key-concepts\" class=\"headerlink\" title=\"key concepts\"></a>key concepts</h1><ul>\n<li><p>link: <a href=\"https://docs.docker.com/engine/swarm/key-concepts/\" target=\"_blank\" rel=\"external\">https://docs.docker.com/engine/swarm/key-concepts/</a> (Docker Engine 1.12.)</p>\n</li>\n<li><p>What is a swarm?</p>\n<p>  The cluster management and orchestration features embedded in the Docker Engine are built using SwarmKit. Docker engines participating in a cluster are running in swarm mode. You enable swarm mode for an engine by either initializing a swarm or joining an existing swarm.</p>\n<p>  A swarm is a cluster of Docker engines, or nodes, where you deploy services. The Docker Engine CLI and API include commands to manage swarm nodes (e.g., add or remove nodes), and deploy and orchestrate services across the swarm.</p>\n<p>  When you run Docker without using swarm mode, you execute container commands. When you run the Docker in swarm mode, you orchestrate services. You can run swarm services and standalone containers on the same Docker instances.</p>\n</li>\n<li><p>What is a node?</p>\n<p>  A node is an instance of the Docker engine participating in the swarm. You can also think of this as a Docker node. You can run one or more nodes on a single physical computer or cloud server, but production swarm deployments typically include Docker nodes distributed across multiple physical and cloud machines.</p>\n<p>  To deploy your application to a swarm, you submit a service definition to a manager node. The manager node dispatches units of work called tasks to worker nodes.</p>\n<p>  Manager nodes also perform the orchestration and cluster management functions required to maintain the desired state of the swarm. Manager nodes elect a single leader to conduct orchestration tasks.</p>\n<p>  Worker nodes receive and execute tasks dispatched from manager nodes. By default manager nodes also run services as worker nodes, but you can configure them to run manager tasks exclusively and be manager-only nodes. An agent runs on each worker node and reports on the tasks assigned to it. The worker node notifies the manager node of the current state of its assigned tasks so that the manager can maintain the desired state of each worker.</p>\n</li>\n<li><p>Services and tasks</p>\n<p>  A service is the definition of the tasks to execute on the worker nodes. It is the central structure of the swarm system and the primary root of user interaction with the swarm.</p>\n<p>  When you create a service, you specify which container image to use and which commands to execute inside running containers.</p>\n<p>  In the replicated services model, the swarm manager distributes a specific number of replica tasks among the nodes based upon the scale you set in the desired state.</p>\n<p>  For global services, the swarm runs one task for the service on every available node in the cluster.</p>\n<p>  A task carries a Docker container and the commands to run inside the container. It is the atomic scheduling unit of swarm. Manager nodes assign tasks to worker nodes according to the number of replicas set in the service scale. Once a task is assigned to a node, it cannot move to another node. It can only run on the assigned node or fail.</p>\n</li>\n<li><p>Load balancing</p>\n<p>  The swarm manager uses ingress load balancing to expose the services you want to make available externally to the swarm. The swarm manager can automatically assign the service a PublishedPort or you can configure a PublishedPort for the service. You can specify any unused port. If you do not specify a port, the swarm manager assigns the service a port in the 30000-32767 range.</p>\n<p>  External components, such as cloud load balancers, can access the service on the PublishedPort of any node in the cluster whether or not the node is currently running the task for the service. All nodes in the swarm route ingress connections to a running task instance.</p>\n<p>  Swarm mode has an internal DNS component that automatically assigns each service in the swarm a DNS entry. The swarm manager uses internal load balancing to distribute requests among services within the cluster based upon the DNS name of the service.</p>\n</li>\n</ul>\n<h1 id=\"docker-swarm-mode-command\"><a href=\"#docker-swarm-mode-command\" class=\"headerlink\" title=\"docker swarm mode command\"></a>docker swarm mode command</h1><ul>\n<li><p>swarm init</p>\n</li>\n<li><p>swarm join</p>\n</li>\n<li><p>service create</p>\n</li>\n<li><p>service inspect</p>\n</li>\n<li><p>service ls</p>\n</li>\n<li><p>service rm</p>\n</li>\n<li><p>service scale</p>\n</li>\n<li><p>service ps</p>\n</li>\n<li><p>service update</p>\n</li>\n</ul>\n","excerpt":"","more":"<h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><ul>\n<li>tut: <a href=\"https://docs.docker.com/engine/swarm/swarm-tutorial/\">https://docs.docker.com/engine/swarm/swarm-tutorial/</a></li>\n</ul>\n<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><p>Docker Engine 1.12 includes swarm mode for natively managing a cluster of Docker Engines called a swarm. Use the Docker CLI to create a swarm, deploy application services to a swarm, and manage swarm behavior.</p>\n<h1 id=\"feature\"><a href=\"#feature\" class=\"headerlink\" title=\"feature\"></a>feature</h1><pre><code>- Cluster management integrated with Docker Engine: \n\nUse the Docker Engine CLI to create a swarm of Docker Engines where you can deploy application services. You don’t need additional orchestration software to create or manage a swarm.\n\n- Decentralized design: \n\nInstead of handling differentiation between node roles at deployment time, the Docker Engine handles any specialization at runtime. You can deploy both kinds of nodes, managers and workers, using the Docker Engine. This means you can build an entire swarm from a single disk image.\n\n- Declarative service model: \n\nDocker Engine uses a declarative approach to let you define the desired state of the various services in your application stack. For example, you might describe an application comprised of a web front end service with message queueing services and a database backend.\n\n- Scaling: \n\nFor each service, you can declare the number of tasks you want to run. When you scale up or down, the swarm manager automatically adapts by adding or removing tasks to maintain the desired state.\n\n- Desired state reconciliation: \n\nThe swarm manager node constantly monitors the cluster state and reconciles any differences between the actual state your expressed desired state. For example, if you set up a service to run 10 replicas of a container, and a worker machine hosting two of those replicas crashes, the manager will create two new replicas to replace the replicas that crashed. The swarm manager assigns the new replicas to workers that are running and available.\n\n- Multi-host networking: \n\nYou can specify an overlay network for your services. The swarm manager automatically assigns addresses to the containers on the overlay network when it initializes or updates the application.\n\n- Service discovery: \n\nSwarm manager nodes assign each service in the swarm a unique DNS name and load balances running containers. You can query every container running in the swarm through a DNS server embedded in the swarm.\n\n- Load balancing: \n\nYou can expose the ports for services to an external load balancer. Internally, the swarm lets you specify how to distribute service containers between nodes.\n\n- Secure by default: \n\nEach node in the swarm enforces TLS mutual authentication and encryption to secure communications between itself and all other nodes. You have the option to use self-signed root certificates or certificates from a custom root CA.\n\n- Rolling updates: \n\nAt rollout time you can apply service updates to nodes incrementally. The swarm manager lets you control the delay between service deployment to different sets of nodes. If anything goes wrong, you can roll-back a task to a previous version of the service.\n</code></pre><h1 id=\"key-concepts\"><a href=\"#key-concepts\" class=\"headerlink\" title=\"key concepts\"></a>key concepts</h1><ul>\n<li><p>link: <a href=\"https://docs.docker.com/engine/swarm/key-concepts/\">https://docs.docker.com/engine/swarm/key-concepts/</a> (Docker Engine 1.12.)</p>\n</li>\n<li><p>What is a swarm?</p>\n<p>  The cluster management and orchestration features embedded in the Docker Engine are built using SwarmKit. Docker engines participating in a cluster are running in swarm mode. You enable swarm mode for an engine by either initializing a swarm or joining an existing swarm.</p>\n<p>  A swarm is a cluster of Docker engines, or nodes, where you deploy services. The Docker Engine CLI and API include commands to manage swarm nodes (e.g., add or remove nodes), and deploy and orchestrate services across the swarm.</p>\n<p>  When you run Docker without using swarm mode, you execute container commands. When you run the Docker in swarm mode, you orchestrate services. You can run swarm services and standalone containers on the same Docker instances.</p>\n</li>\n<li><p>What is a node?</p>\n<p>  A node is an instance of the Docker engine participating in the swarm. You can also think of this as a Docker node. You can run one or more nodes on a single physical computer or cloud server, but production swarm deployments typically include Docker nodes distributed across multiple physical and cloud machines.</p>\n<p>  To deploy your application to a swarm, you submit a service definition to a manager node. The manager node dispatches units of work called tasks to worker nodes.</p>\n<p>  Manager nodes also perform the orchestration and cluster management functions required to maintain the desired state of the swarm. Manager nodes elect a single leader to conduct orchestration tasks.</p>\n<p>  Worker nodes receive and execute tasks dispatched from manager nodes. By default manager nodes also run services as worker nodes, but you can configure them to run manager tasks exclusively and be manager-only nodes. An agent runs on each worker node and reports on the tasks assigned to it. The worker node notifies the manager node of the current state of its assigned tasks so that the manager can maintain the desired state of each worker.</p>\n</li>\n<li><p>Services and tasks</p>\n<p>  A service is the definition of the tasks to execute on the worker nodes. It is the central structure of the swarm system and the primary root of user interaction with the swarm.</p>\n<p>  When you create a service, you specify which container image to use and which commands to execute inside running containers.</p>\n<p>  In the replicated services model, the swarm manager distributes a specific number of replica tasks among the nodes based upon the scale you set in the desired state.</p>\n<p>  For global services, the swarm runs one task for the service on every available node in the cluster.</p>\n<p>  A task carries a Docker container and the commands to run inside the container. It is the atomic scheduling unit of swarm. Manager nodes assign tasks to worker nodes according to the number of replicas set in the service scale. Once a task is assigned to a node, it cannot move to another node. It can only run on the assigned node or fail.</p>\n</li>\n<li><p>Load balancing</p>\n<p>  The swarm manager uses ingress load balancing to expose the services you want to make available externally to the swarm. The swarm manager can automatically assign the service a PublishedPort or you can configure a PublishedPort for the service. You can specify any unused port. If you do not specify a port, the swarm manager assigns the service a port in the 30000-32767 range.</p>\n<p>  External components, such as cloud load balancers, can access the service on the PublishedPort of any node in the cluster whether or not the node is currently running the task for the service. All nodes in the swarm route ingress connections to a running task instance.</p>\n<p>  Swarm mode has an internal DNS component that automatically assigns each service in the swarm a DNS entry. The swarm manager uses internal load balancing to distribute requests among services within the cluster based upon the DNS name of the service.</p>\n</li>\n</ul>\n<h1 id=\"docker-swarm-mode-command\"><a href=\"#docker-swarm-mode-command\" class=\"headerlink\" title=\"docker swarm mode command\"></a>docker swarm mode command</h1><ul>\n<li><p>swarm init</p>\n</li>\n<li><p>swarm join</p>\n</li>\n<li><p>service create</p>\n</li>\n<li><p>service inspect</p>\n</li>\n<li><p>service ls</p>\n</li>\n<li><p>service rm</p>\n</li>\n<li><p>service scale</p>\n</li>\n<li><p>service ps</p>\n</li>\n<li><p>service update</p>\n</li>\n</ul>\n"},{"title":"openstack core","_content":"\n# core","source":"_posts/iot-portal.md","raw":"---\ntitle: openstack core\ncategories:\n- iaas\ntags:\n- core\n- openstack\n---\n\n# core","slug":"iot-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:33:08.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm3ay003w21svpn73kvxk","content":"<h1 id=\"core\"><a href=\"#core\" class=\"headerlink\" title=\"core\"></a>core</h1>","excerpt":"","more":"<h1 id=\"core\"><a href=\"#core\" class=\"headerlink\" title=\"core\"></a>core</h1>"},{"title":"cloud architect portal","_content":"\n# about cloud architect\n\n# resource\n\n","source":"_posts/jd-architect-portal.md","raw":"---\ntitle: cloud architect portal\ncategories:\n- cloud\ntags:\n- portal\n- architect\n---\n\n# about cloud architect\n\n# resource\n\n","slug":"jd-architect-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-13T11:48:21.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm3b2004021svioptmeei","content":"<h1 id=\"about-cloud-architect\"><a href=\"#about-cloud-architect\" class=\"headerlink\" title=\"about cloud architect\"></a>about cloud architect</h1><h1 id=\"resource\"><a href=\"#resource\" class=\"headerlink\" title=\"resource\"></a>resource</h1>","excerpt":"","more":"<h1 id=\"about-cloud-architect\"><a href=\"#about-cloud-architect\" class=\"headerlink\" title=\"about cloud architect\"></a>about cloud architect</h1><h1 id=\"resource\"><a href=\"#resource\" class=\"headerlink\" title=\"resource\"></a>resource</h1>"},{"title":"python portal","_content":"\n# 123","source":"_posts/jd-fullstack-portal.md","raw":"---\ntitle: python portal\ncategories:\n- python\ntags:\n- portal\n---\n\n# 123","slug":"jd-fullstack-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:35:54.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm3b4004421svqhoqjae5","content":"<h1 id=\"123\"><a href=\"#123\" class=\"headerlink\" title=\"123\"></a>123</h1>","excerpt":"","more":"<h1 id=\"123\"><a href=\"#123\" class=\"headerlink\" title=\"123\"></a>123</h1>"},{"title":"career portal","_content":"\n# 2017小目标\n\n- Docker & PaaS Mastring\n- IaaS Mastring\n\n# guide\n    怎样花两年时间去面试一个人 http://mindhacks.cn/2011/11/04/how-to-interview-a-person-for-two-years/\n\n# resource\n\ndouban\n系统开发工程师(云平台)\n职位描述:\n1. 负责基于Docker的PaaS平台设计、开发、优化和维护，解决全面Docker化过程中遇到的各种问题\n2. 负责豆瓣服务化框架的开发和维护\n3. 能充分理解产品需求，提供基础设施软件支持\n4. 分析系统瓶颈,解决各种疑难杂症\n职位要求:\n1. 计算机基础扎实，有较好的数据结构与算法功底，熟悉Linux系统编程\n2. 熟练掌握至少一种编程语言 （Python/Golang/C/C++), 愿意深入学习和使用Python/Golang\n3. 有系统工程师经验或者熟悉系统软件的开发，调试，Bug 定位的经验以及相关工具\n4.在以下其中的任何一个领域有相关经验或者知识储备\n-  docker容器化技术：掌握docker运行机制，对大规模分布式容器编排感兴趣\n-  服务化框架开发经验\n- 云平台或PaaS平台开发经验\n- 分布式任务调度：熟悉Mesos 、Marathon、Kubernetes等\n- 熟悉大流量、高并发、高性能的分布式系统的设计及应用\n \n优先条件:\n1. 具有良好的代码风格，工作严谨，责任感强，良好的自我驱动力\n2. 良好的沟通能力和团队合作精神\n\n系统开发工程师(大数据方向)\n职位描述:\n1. 负责豆瓣核心分布式系统架构的设计、开发、维护\n2. 参与豆瓣基础高性能通用计算平台的设计、开发和维护\n3. 构建和维护支撑豆瓣海量数据分析和机器学习任务的基础设施\n4. 开发、定制、调试、优化系统软件，分析系统瓶颈，解决各种疑难杂症\n职位要求:\n1. 有系统工程师经验，熟悉系统软件的开发调试，能够使用相关工具分析和定位常见 Bug\n2. 熟练掌握至少以下一种编程语言 （C，C++，Golang, Python）\n3. 在以下任何一个领域有相关经验和知识储备\n  - 高性能计算：良好的数学基础，熟悉基本的计算机体系结构，掌握高性能计算的基本方法和常见模型，有OpenMP，MPI，Graphlab或类似平台高性能计算经验者优先，能够理解和实现并行算法者尤佳\n  - 分布式计算：对 Spark，Hadoop，Impala 等大数据分析系统有较深入的理解以及使用经验，有此类系统开发经验者优先\n  - 分布式存储：对至少一种分布式存储系统的架构和实现有深入理解，有相关开发、优化经验者优先， 在分布式查询优化、列存、索引等方面有深入研究者尤佳。 \n优先条件:\n1. 能够与人顺畅沟通，有充沛的技术热情\n2. 重视代码质量，懂得如何测试以及写测试，懂得如何写可测试的代码，了解如何持续交付\n3. 可以快速学习新知识，有多种不同范式编程语言使用经验者优先\n","source":"_posts/jd--portal.md","raw":"---\ntitle: career portal\ncategories:\n- career\ntags:\n- portal\n---\n\n# 2017小目标\n\n- Docker & PaaS Mastring\n- IaaS Mastring\n\n# guide\n    怎样花两年时间去面试一个人 http://mindhacks.cn/2011/11/04/how-to-interview-a-person-for-two-years/\n\n# resource\n\ndouban\n系统开发工程师(云平台)\n职位描述:\n1. 负责基于Docker的PaaS平台设计、开发、优化和维护，解决全面Docker化过程中遇到的各种问题\n2. 负责豆瓣服务化框架的开发和维护\n3. 能充分理解产品需求，提供基础设施软件支持\n4. 分析系统瓶颈,解决各种疑难杂症\n职位要求:\n1. 计算机基础扎实，有较好的数据结构与算法功底，熟悉Linux系统编程\n2. 熟练掌握至少一种编程语言 （Python/Golang/C/C++), 愿意深入学习和使用Python/Golang\n3. 有系统工程师经验或者熟悉系统软件的开发，调试，Bug 定位的经验以及相关工具\n4.在以下其中的任何一个领域有相关经验或者知识储备\n-  docker容器化技术：掌握docker运行机制，对大规模分布式容器编排感兴趣\n-  服务化框架开发经验\n- 云平台或PaaS平台开发经验\n- 分布式任务调度：熟悉Mesos 、Marathon、Kubernetes等\n- 熟悉大流量、高并发、高性能的分布式系统的设计及应用\n \n优先条件:\n1. 具有良好的代码风格，工作严谨，责任感强，良好的自我驱动力\n2. 良好的沟通能力和团队合作精神\n\n系统开发工程师(大数据方向)\n职位描述:\n1. 负责豆瓣核心分布式系统架构的设计、开发、维护\n2. 参与豆瓣基础高性能通用计算平台的设计、开发和维护\n3. 构建和维护支撑豆瓣海量数据分析和机器学习任务的基础设施\n4. 开发、定制、调试、优化系统软件，分析系统瓶颈，解决各种疑难杂症\n职位要求:\n1. 有系统工程师经验，熟悉系统软件的开发调试，能够使用相关工具分析和定位常见 Bug\n2. 熟练掌握至少以下一种编程语言 （C，C++，Golang, Python）\n3. 在以下任何一个领域有相关经验和知识储备\n  - 高性能计算：良好的数学基础，熟悉基本的计算机体系结构，掌握高性能计算的基本方法和常见模型，有OpenMP，MPI，Graphlab或类似平台高性能计算经验者优先，能够理解和实现并行算法者尤佳\n  - 分布式计算：对 Spark，Hadoop，Impala 等大数据分析系统有较深入的理解以及使用经验，有此类系统开发经验者优先\n  - 分布式存储：对至少一种分布式存储系统的架构和实现有深入理解，有相关开发、优化经验者优先， 在分布式查询优化、列存、索引等方面有深入研究者尤佳。 \n优先条件:\n1. 能够与人顺畅沟通，有充沛的技术热情\n2. 重视代码质量，懂得如何测试以及写测试，懂得如何写可测试的代码，了解如何持续交付\n3. 可以快速学习新知识，有多种不同范式编程语言使用经验者优先\n","slug":"jd--portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-02-27T03:17:18.000Z","_id":"ciypwm3b7004721sv1225ic1c","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"2017小目标\"><a href=\"#2017小目标\" class=\"headerlink\" title=\"2017小目标\"></a>2017小目标</h1><ul>\n<li>Docker &amp; PaaS Mastring</li>\n<li>IaaS Mastring</li>\n</ul>\n<h1 id=\"guide\"><a href=\"#guide\" class=\"headerlink\" title=\"guide\"></a>guide</h1><pre><code>怎样花两年时间去面试一个人 http://mindhacks.cn/2011/11/04/how-to-interview-a-person-for-two-years/\n</code></pre><h1 id=\"resource\"><a href=\"#resource\" class=\"headerlink\" title=\"resource\"></a>resource</h1><p>douban<br>系统开发工程师(云平台)<br>职位描述:</p>\n<ol>\n<li>负责基于Docker的PaaS平台设计、开发、优化和维护，解决全面Docker化过程中遇到的各种问题</li>\n<li>负责豆瓣服务化框架的开发和维护</li>\n<li>能充分理解产品需求，提供基础设施软件支持</li>\n<li>分析系统瓶颈,解决各种疑难杂症<br>职位要求:</li>\n<li>计算机基础扎实，有较好的数据结构与算法功底，熟悉Linux系统编程</li>\n<li>熟练掌握至少一种编程语言 （Python/Golang/C/C++), 愿意深入学习和使用Python/Golang</li>\n<li>有系统工程师经验或者熟悉系统软件的开发，调试，Bug 定位的经验以及相关工具<br>4.在以下其中的任何一个领域有相关经验或者知识储备</li>\n</ol>\n<ul>\n<li>docker容器化技术：掌握docker运行机制，对大规模分布式容器编排感兴趣</li>\n<li>服务化框架开发经验</li>\n<li>云平台或PaaS平台开发经验</li>\n<li>分布式任务调度：熟悉Mesos 、Marathon、Kubernetes等</li>\n<li>熟悉大流量、高并发、高性能的分布式系统的设计及应用</li>\n</ul>\n<p>优先条件:</p>\n<ol>\n<li>具有良好的代码风格，工作严谨，责任感强，良好的自我驱动力</li>\n<li>良好的沟通能力和团队合作精神</li>\n</ol>\n<p>系统开发工程师(大数据方向)<br>职位描述:</p>\n<ol>\n<li>负责豆瓣核心分布式系统架构的设计、开发、维护</li>\n<li>参与豆瓣基础高性能通用计算平台的设计、开发和维护</li>\n<li>构建和维护支撑豆瓣海量数据分析和机器学习任务的基础设施</li>\n<li>开发、定制、调试、优化系统软件，分析系统瓶颈，解决各种疑难杂症<br>职位要求:</li>\n<li>有系统工程师经验，熟悉系统软件的开发调试，能够使用相关工具分析和定位常见 Bug</li>\n<li>熟练掌握至少以下一种编程语言 （C，C++，Golang, Python）</li>\n<li>在以下任何一个领域有相关经验和知识储备<ul>\n<li>高性能计算：良好的数学基础，熟悉基本的计算机体系结构，掌握高性能计算的基本方法和常见模型，有OpenMP，MPI，Graphlab或类似平台高性能计算经验者优先，能够理解和实现并行算法者尤佳</li>\n<li>分布式计算：对 Spark，Hadoop，Impala 等大数据分析系统有较深入的理解以及使用经验，有此类系统开发经验者优先</li>\n<li>分布式存储：对至少一种分布式存储系统的架构和实现有深入理解，有相关开发、优化经验者优先， 在分布式查询优化、列存、索引等方面有深入研究者尤佳。<br>优先条件:</li>\n</ul>\n</li>\n<li>能够与人顺畅沟通，有充沛的技术热情</li>\n<li>重视代码质量，懂得如何测试以及写测试，懂得如何写可测试的代码，了解如何持续交付</li>\n<li>可以快速学习新知识，有多种不同范式编程语言使用经验者优先</li>\n</ol>\n","excerpt":"","more":"<h1 id=\"2017小目标\"><a href=\"#2017小目标\" class=\"headerlink\" title=\"2017小目标\"></a>2017小目标</h1><ul>\n<li>Docker &amp; PaaS Mastring</li>\n<li>IaaS Mastring</li>\n</ul>\n<h1 id=\"guide\"><a href=\"#guide\" class=\"headerlink\" title=\"guide\"></a>guide</h1><pre><code>怎样花两年时间去面试一个人 http://mindhacks.cn/2011/11/04/how-to-interview-a-person-for-two-years/\n</code></pre><h1 id=\"resource\"><a href=\"#resource\" class=\"headerlink\" title=\"resource\"></a>resource</h1><p>douban<br>系统开发工程师(云平台)<br>职位描述:</p>\n<ol>\n<li>负责基于Docker的PaaS平台设计、开发、优化和维护，解决全面Docker化过程中遇到的各种问题</li>\n<li>负责豆瓣服务化框架的开发和维护</li>\n<li>能充分理解产品需求，提供基础设施软件支持</li>\n<li>分析系统瓶颈,解决各种疑难杂症<br>职位要求:</li>\n<li>计算机基础扎实，有较好的数据结构与算法功底，熟悉Linux系统编程</li>\n<li>熟练掌握至少一种编程语言 （Python/Golang/C/C++), 愿意深入学习和使用Python/Golang</li>\n<li>有系统工程师经验或者熟悉系统软件的开发，调试，Bug 定位的经验以及相关工具<br>4.在以下其中的任何一个领域有相关经验或者知识储备</li>\n</ol>\n<ul>\n<li>docker容器化技术：掌握docker运行机制，对大规模分布式容器编排感兴趣</li>\n<li>服务化框架开发经验</li>\n<li>云平台或PaaS平台开发经验</li>\n<li>分布式任务调度：熟悉Mesos 、Marathon、Kubernetes等</li>\n<li>熟悉大流量、高并发、高性能的分布式系统的设计及应用</li>\n</ul>\n<p>优先条件:</p>\n<ol>\n<li>具有良好的代码风格，工作严谨，责任感强，良好的自我驱动力</li>\n<li>良好的沟通能力和团队合作精神</li>\n</ol>\n<p>系统开发工程师(大数据方向)<br>职位描述:</p>\n<ol>\n<li>负责豆瓣核心分布式系统架构的设计、开发、维护</li>\n<li>参与豆瓣基础高性能通用计算平台的设计、开发和维护</li>\n<li>构建和维护支撑豆瓣海量数据分析和机器学习任务的基础设施</li>\n<li>开发、定制、调试、优化系统软件，分析系统瓶颈，解决各种疑难杂症<br>职位要求:</li>\n<li>有系统工程师经验，熟悉系统软件的开发调试，能够使用相关工具分析和定位常见 Bug</li>\n<li>熟练掌握至少以下一种编程语言 （C，C++，Golang, Python）</li>\n<li>在以下任何一个领域有相关经验和知识储备<ul>\n<li>高性能计算：良好的数学基础，熟悉基本的计算机体系结构，掌握高性能计算的基本方法和常见模型，有OpenMP，MPI，Graphlab或类似平台高性能计算经验者优先，能够理解和实现并行算法者尤佳</li>\n<li>分布式计算：对 Spark，Hadoop，Impala 等大数据分析系统有较深入的理解以及使用经验，有此类系统开发经验者优先</li>\n<li>分布式存储：对至少一种分布式存储系统的架构和实现有深入理解，有相关开发、优化经验者优先， 在分布式查询优化、列存、索引等方面有深入研究者尤佳。<br>优先条件:</li>\n</ul>\n</li>\n<li>能够与人顺畅沟通，有充沛的技术热情</li>\n<li>重视代码质量，懂得如何测试以及写测试，懂得如何写可测试的代码，了解如何持续交付</li>\n<li>可以快速学习新知识，有多种不同范式编程语言使用经验者优先</li>\n</ol>\n"},{"title":"cx portal","_content":"\n","source":"_posts/lang--portal.md","raw":"---\ntitle: cx portal\ncategories:\n- cx\ntags:\n- portal\n---\n\n","slug":"lang--portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:31:54.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm3ba004c21svocwumvw5","content":"","excerpt":"","more":""},{"title":"cx portal","_content":"\n","source":"_posts/lang-cx-portal.md","raw":"---\ntitle: cx portal\ncategories:\n- cx\ntags:\n- portal\n---\n\n","slug":"lang-cx-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:31:54.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm3bc004f21svgewuytn4","content":"","excerpt":"","more":""},{"title":"java portal","_content":"\n#  portal\n\njava one:https://www.oracle.com/javaone\n\n# spring boot\nhttp://www.infoq.com/cn/articles/microframeworks1-spring-boot","source":"_posts/lang-java-portal.md","raw":"---\ntitle: java portal\ncategories:\n- java\ntags:\n- portal\n---\n\n#  portal\n\njava one:https://www.oracle.com/javaone\n\n# spring boot\nhttp://www.infoq.com/cn/articles/microframeworks1-spring-boot","slug":"lang-java-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-02-27T03:10:14.000Z","_id":"ciypwm3be004k21svfutfhg13","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"portal\"><a href=\"#portal\" class=\"headerlink\" title=\"portal\"></a>portal</h1><p>java one:<a href=\"https://www.oracle.com/javaone\" target=\"_blank\" rel=\"external\">https://www.oracle.com/javaone</a></p>\n<h1 id=\"spring-boot\"><a href=\"#spring-boot\" class=\"headerlink\" title=\"spring boot\"></a>spring boot</h1><p><a href=\"http://www.infoq.com/cn/articles/microframeworks1-spring-boot\" target=\"_blank\" rel=\"external\">http://www.infoq.com/cn/articles/microframeworks1-spring-boot</a></p>\n","excerpt":"","more":"<h1 id=\"portal\"><a href=\"#portal\" class=\"headerlink\" title=\"portal\"></a>portal</h1><p>java one:<a href=\"https://www.oracle.com/javaone\">https://www.oracle.com/javaone</a></p>\n<h1 id=\"spring-boot\"><a href=\"#spring-boot\" class=\"headerlink\" title=\"spring boot\"></a>spring boot</h1><p><a href=\"http://www.infoq.com/cn/articles/microframeworks1-spring-boot\">http://www.infoq.com/cn/articles/microframeworks1-spring-boot</a></p>\n"},{"title":"nodejs core","_content":"\n\n        ","source":"_posts/lang-js-nodejs-core.md","raw":"---\ntitle: nodejs core\ncategories:\n- nodejs\ntags:\n- core\n---\n\n\n        ","slug":"lang-js-nodejs-core","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:33:42.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm3bg004n21sv7l1ddxrz","content":"","excerpt":"","more":""},{"title":"js portal","_content":"\n# about\nvs: https://da-14.com/blog/reactjs-vs-angular-comparison-which-better\nhttps://hackernoon.com/5-best-javascript-frameworks-in-2017-7a63b3870282?gi=47ffee68edc6\n\nhttps://colorlib.com/wp/javascript-frameworks/\nhttps://medium.com/@ZombieCodeKill/choosing-a-javascript-framework-535745d0ab90#.bj206ygou\n\n#jquery\nhttp://jquery.com/\n        ","source":"_posts/lang-js-portal.md","raw":"---\ntitle: js portal\ncategories:\n- js\ntags:\n- portal\n---\n\n# about\nvs: https://da-14.com/blog/reactjs-vs-angular-comparison-which-better\nhttps://hackernoon.com/5-best-javascript-frameworks-in-2017-7a63b3870282?gi=47ffee68edc6\n\nhttps://colorlib.com/wp/javascript-frameworks/\nhttps://medium.com/@ZombieCodeKill/choosing-a-javascript-framework-535745d0ab90#.bj206ygou\n\n#jquery\nhttp://jquery.com/\n        ","slug":"lang-js-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-02-26T17:10:53.000Z","_id":"ciypwm3bj004r21svidlm72tf","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><p>vs: <a href=\"https://da-14.com/blog/reactjs-vs-angular-comparison-which-better\" target=\"_blank\" rel=\"external\">https://da-14.com/blog/reactjs-vs-angular-comparison-which-better</a><br><a href=\"https://hackernoon.com/5-best-javascript-frameworks-in-2017-7a63b3870282?gi=47ffee68edc6\" target=\"_blank\" rel=\"external\">https://hackernoon.com/5-best-javascript-frameworks-in-2017-7a63b3870282?gi=47ffee68edc6</a></p>\n<p><a href=\"https://colorlib.com/wp/javascript-frameworks/\" target=\"_blank\" rel=\"external\">https://colorlib.com/wp/javascript-frameworks/</a><br><a href=\"https://medium.com/@ZombieCodeKill/choosing-a-javascript-framework-535745d0ab90#.bj206ygou\" target=\"_blank\" rel=\"external\">https://medium.com/@ZombieCodeKill/choosing-a-javascript-framework-535745d0ab90#.bj206ygou</a></p>\n<p>#jquery<br><a href=\"http://jquery.com/\" target=\"_blank\" rel=\"external\">http://jquery.com/</a></p>\n","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><p>vs: <a href=\"https://da-14.com/blog/reactjs-vs-angular-comparison-which-better\">https://da-14.com/blog/reactjs-vs-angular-comparison-which-better</a><br><a href=\"https://hackernoon.com/5-best-javascript-frameworks-in-2017-7a63b3870282?gi=47ffee68edc6\">https://hackernoon.com/5-best-javascript-frameworks-in-2017-7a63b3870282?gi=47ffee68edc6</a></p>\n<p><a href=\"https://colorlib.com/wp/javascript-frameworks/\">https://colorlib.com/wp/javascript-frameworks/</a><br><a href=\"https://medium.com/@ZombieCodeKill/choosing-a-javascript-framework-535745d0ab90#.bj206ygou\">https://medium.com/@ZombieCodeKill/choosing-a-javascript-framework-535745d0ab90#.bj206ygou</a></p>\n<p>#jquery<br><a href=\"http://jquery.com/\">http://jquery.com/</a></p>\n"},{"title":"django core","_content":"\n# about\n\n    Full stack web frameworks.The most popular web framework in Python.\n\n    Django makes it easier to build better Web apps more quickly and with less code.\n    \n    Django is a high-level Python Web framework that encourages rapid development and clean, pragmatic design. Built by experienced developers, it takes care of much of the hassle of Web development, so you can focus on writing your app without needing to reinvent the wheel. It’s free and open source.\n\n    Ridiculously fast: Django was designed to help developers take applications from concept to completion as quickly as possible.\n\n    Reassuringly secure: Django takes security seriously and helps developers avoid many common security mistakes.\n\n    Exceedingly scalable: Some of the busiest sites on the Web leverage Django’s ability to quickly and flexibly scale.\n\n# link\n\n    - official: https://www.djangoproject.com/\n    - tut: https://docs.djangoproject.com/en/1.10/intro/tutorial01/\n    - awesome: https://github.com/rosarior/awesome-django\n    - community: http://django-china.cn/\n\n# install\n\n    - [install on mac (bare metal)]()\n    - [install on mac (docker)]()\n\n# read\n\n    - djangobook: http://docs.30c.org/djangobook2/; http://djangobook.py3k.cn/2.0/\n\n# mooc\n    \n    - http://www.imooc.com/learn/790\n\n    \n\n","source":"_posts/lang-python-django-core.md","raw":"---\ntitle: django core\ncategories:\n- python\ntags:\n- core\n- django\n---\n\n# about\n\n    Full stack web frameworks.The most popular web framework in Python.\n\n    Django makes it easier to build better Web apps more quickly and with less code.\n    \n    Django is a high-level Python Web framework that encourages rapid development and clean, pragmatic design. Built by experienced developers, it takes care of much of the hassle of Web development, so you can focus on writing your app without needing to reinvent the wheel. It’s free and open source.\n\n    Ridiculously fast: Django was designed to help developers take applications from concept to completion as quickly as possible.\n\n    Reassuringly secure: Django takes security seriously and helps developers avoid many common security mistakes.\n\n    Exceedingly scalable: Some of the busiest sites on the Web leverage Django’s ability to quickly and flexibly scale.\n\n# link\n\n    - official: https://www.djangoproject.com/\n    - tut: https://docs.djangoproject.com/en/1.10/intro/tutorial01/\n    - awesome: https://github.com/rosarior/awesome-django\n    - community: http://django-china.cn/\n\n# install\n\n    - [install on mac (bare metal)]()\n    - [install on mac (docker)]()\n\n# read\n\n    - djangobook: http://docs.30c.org/djangobook2/; http://djangobook.py3k.cn/2.0/\n\n# mooc\n    \n    - http://www.imooc.com/learn/790\n\n    \n\n","slug":"lang-python-django-core","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-12T09:47:39.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm3bo004u21sve7kv8amn","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><pre><code>Full stack web frameworks.The most popular web framework in Python.\n\nDjango makes it easier to build better Web apps more quickly and with less code.\n\nDjango is a high-level Python Web framework that encourages rapid development and clean, pragmatic design. Built by experienced developers, it takes care of much of the hassle of Web development, so you can focus on writing your app without needing to reinvent the wheel. It’s free and open source.\n\nRidiculously fast: Django was designed to help developers take applications from concept to completion as quickly as possible.\n\nReassuringly secure: Django takes security seriously and helps developers avoid many common security mistakes.\n\nExceedingly scalable: Some of the busiest sites on the Web leverage Django’s ability to quickly and flexibly scale.\n</code></pre><h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><pre><code>- official: https://www.djangoproject.com/\n- tut: https://docs.djangoproject.com/en/1.10/intro/tutorial01/\n- awesome: https://github.com/rosarior/awesome-django\n- community: http://django-china.cn/\n</code></pre><h1 id=\"install\"><a href=\"#install\" class=\"headerlink\" title=\"install\"></a>install</h1><pre><code>- [install on mac (bare metal)]()\n- [install on mac (docker)]()\n</code></pre><h1 id=\"read\"><a href=\"#read\" class=\"headerlink\" title=\"read\"></a>read</h1><pre><code>- djangobook: http://docs.30c.org/djangobook2/; http://djangobook.py3k.cn/2.0/\n</code></pre><h1 id=\"mooc\"><a href=\"#mooc\" class=\"headerlink\" title=\"mooc\"></a>mooc</h1><pre><code>- http://www.imooc.com/learn/790\n</code></pre>","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><pre><code>Full stack web frameworks.The most popular web framework in Python.\n\nDjango makes it easier to build better Web apps more quickly and with less code.\n\nDjango is a high-level Python Web framework that encourages rapid development and clean, pragmatic design. Built by experienced developers, it takes care of much of the hassle of Web development, so you can focus on writing your app without needing to reinvent the wheel. It’s free and open source.\n\nRidiculously fast: Django was designed to help developers take applications from concept to completion as quickly as possible.\n\nReassuringly secure: Django takes security seriously and helps developers avoid many common security mistakes.\n\nExceedingly scalable: Some of the busiest sites on the Web leverage Django’s ability to quickly and flexibly scale.\n</code></pre><h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><pre><code>- official: https://www.djangoproject.com/\n- tut: https://docs.djangoproject.com/en/1.10/intro/tutorial01/\n- awesome: https://github.com/rosarior/awesome-django\n- community: http://django-china.cn/\n</code></pre><h1 id=\"install\"><a href=\"#install\" class=\"headerlink\" title=\"install\"></a>install</h1><pre><code>- [install on mac (bare metal)]()\n- [install on mac (docker)]()\n</code></pre><h1 id=\"read\"><a href=\"#read\" class=\"headerlink\" title=\"read\"></a>read</h1><pre><code>- djangobook: http://docs.30c.org/djangobook2/; http://djangobook.py3k.cn/2.0/\n</code></pre><h1 id=\"mooc\"><a href=\"#mooc\" class=\"headerlink\" title=\"mooc\"></a>mooc</h1><pre><code>- http://www.imooc.com/learn/790\n</code></pre>"},{"title":"install django on mac","update":"2017-01-12T09:43:23.000Z","_content":"\n# install on mac (bare metal)\n\n# install on mac (docker)\n- download & install docker for mac;\n    - link: https://docs.docker.com/compose/django/;\n    - Define the project components;\n        1. create folder;\n        2. create Dockerfile;\n            ```\n             FROM python:2.7\n             ENV PYTHONUNBUFFERED 1\n             RUN mkdir /code\n             WORKDIR /code\n             ADD requirements.txt /code/\n             RUN pip install -r requirements.txt\n             ADD . /code/\n            ```\n        3. create requirements.txt;\n            ```\n             Django\n             psycopg2\n            ```\n        4. create docker-compose.yml\n            ```\n             version: '2'\n             services:\n               db:\n                 image: postgres\n               web:\n                 build: .\n                 command: python manage.py runserver 0.0.0.0:8000\n                 volumes:\n                   - .:/code\n                 ports:\n                   - \"8000:8000\"\n                 depends_on:\n                   - db\n            ```\n    - Create a Django project;\n        1. goto root dir;\n        2. docker-compose run web django-admin.py startproject composeexample .\n        3. ls -l; sudo chown -R $USER:$USER .;\n    - Connect the database;\n        1. edit composeexample/settings.py;\n        ```\n        DATABASES = {\n             'default': {\n                 'ENGINE': 'django.db.backends.postgresql',\n                 'NAME': 'postgres',\n                 'USER': 'postgres',\n                 'HOST': 'db',\n                 'PORT': 5432,\n             }\n         }\n        ```\n        2. $ docker-compose up","source":"_posts/lang-python-django-install.md","raw":"---\ntitle: install django on mac\nupdate: 2017-01-12 17:43:23\ncategories:\n- python\ntags: \n- install\n- python\n- django\n- docker\n---\n\n# install on mac (bare metal)\n\n# install on mac (docker)\n- download & install docker for mac;\n    - link: https://docs.docker.com/compose/django/;\n    - Define the project components;\n        1. create folder;\n        2. create Dockerfile;\n            ```\n             FROM python:2.7\n             ENV PYTHONUNBUFFERED 1\n             RUN mkdir /code\n             WORKDIR /code\n             ADD requirements.txt /code/\n             RUN pip install -r requirements.txt\n             ADD . /code/\n            ```\n        3. create requirements.txt;\n            ```\n             Django\n             psycopg2\n            ```\n        4. create docker-compose.yml\n            ```\n             version: '2'\n             services:\n               db:\n                 image: postgres\n               web:\n                 build: .\n                 command: python manage.py runserver 0.0.0.0:8000\n                 volumes:\n                   - .:/code\n                 ports:\n                   - \"8000:8000\"\n                 depends_on:\n                   - db\n            ```\n    - Create a Django project;\n        1. goto root dir;\n        2. docker-compose run web django-admin.py startproject composeexample .\n        3. ls -l; sudo chown -R $USER:$USER .;\n    - Connect the database;\n        1. edit composeexample/settings.py;\n        ```\n        DATABASES = {\n             'default': {\n                 'ENGINE': 'django.db.backends.postgresql',\n                 'NAME': 'postgres',\n                 'USER': 'postgres',\n                 'HOST': 'db',\n                 'PORT': 5432,\n             }\n         }\n        ```\n        2. $ docker-compose up","slug":"lang-python-django-install","published":1,"date":"2017-01-12T09:43:23.000Z","updated":"2017-01-12T09:49:02.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm3bs004x21svpb3i7i4k","content":"<h1 id=\"install-on-mac-bare-metal\"><a href=\"#install-on-mac-bare-metal\" class=\"headerlink\" title=\"install on mac (bare metal)\"></a>install on mac (bare metal)</h1><h1 id=\"install-on-mac-docker\"><a href=\"#install-on-mac-docker\" class=\"headerlink\" title=\"install on mac (docker)\"></a>install on mac (docker)</h1><ul>\n<li><p>download &amp; install docker for mac;</p>\n<ul>\n<li>link: <a href=\"https://docs.docker.com/compose/django/\" target=\"_blank\" rel=\"external\">https://docs.docker.com/compose/django/</a>;</li>\n<li><p>Define the project components;</p>\n<ol>\n<li>create folder;</li>\n<li><p>create Dockerfile;</p>\n <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">FROM python:2.7</div><div class=\"line\">ENV PYTHONUNBUFFERED 1</div><div class=\"line\">RUN mkdir /code</div><div class=\"line\">WORKDIR /code</div><div class=\"line\">ADD requirements.txt /code/</div><div class=\"line\">RUN pip install -r requirements.txt</div><div class=\"line\">ADD . /code/</div></pre></td></tr></table></figure>\n</li>\n<li><p>create requirements.txt;</p>\n <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">Django</div><div class=\"line\">psycopg2</div></pre></td></tr></table></figure>\n</li>\n<li><p>create docker-compose.yml</p>\n <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\">version: &apos;2&apos;</div><div class=\"line\">services:</div><div class=\"line\">  db:</div><div class=\"line\">    image: postgres</div><div class=\"line\">  web:</div><div class=\"line\">    build: .</div><div class=\"line\">    command: python manage.py runserver 0.0.0.0:8000</div><div class=\"line\">    volumes:</div><div class=\"line\">      - .:/code</div><div class=\"line\">    ports:</div><div class=\"line\">      - &quot;8000:8000&quot;</div><div class=\"line\">    depends_on:</div><div class=\"line\">      - db</div></pre></td></tr></table></figure>\n</li>\n</ol>\n</li>\n<li><p>Create a Django project;</p>\n<ol>\n<li>goto root dir;</li>\n<li>docker-compose run web django-admin.py startproject composeexample .</li>\n<li>ls -l; sudo chown -R $USER:$USER .;</li>\n</ol>\n</li>\n<li><p>Connect the database;</p>\n<ol>\n<li><p>edit composeexample/settings.py;</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">DATABASES = &#123;</div><div class=\"line\">     &apos;default&apos;: &#123;</div><div class=\"line\">         &apos;ENGINE&apos;: &apos;django.db.backends.postgresql&apos;,</div><div class=\"line\">         &apos;NAME&apos;: &apos;postgres&apos;,</div><div class=\"line\">         &apos;USER&apos;: &apos;postgres&apos;,</div><div class=\"line\">         &apos;HOST&apos;: &apos;db&apos;,</div><div class=\"line\">         &apos;PORT&apos;: 5432,</div><div class=\"line\">     &#125;</div><div class=\"line\"> &#125;</div></pre></td></tr></table></figure>\n</li>\n<li><p>$ docker-compose up</p>\n</li>\n</ol>\n</li>\n</ul>\n</li>\n</ul>\n","excerpt":"","more":"<h1 id=\"install-on-mac-bare-metal\"><a href=\"#install-on-mac-bare-metal\" class=\"headerlink\" title=\"install on mac (bare metal)\"></a>install on mac (bare metal)</h1><h1 id=\"install-on-mac-docker\"><a href=\"#install-on-mac-docker\" class=\"headerlink\" title=\"install on mac (docker)\"></a>install on mac (docker)</h1><ul>\n<li><p>download &amp; install docker for mac;</p>\n<ul>\n<li>link: <a href=\"https://docs.docker.com/compose/django/\">https://docs.docker.com/compose/django/</a>;</li>\n<li><p>Define the project components;</p>\n<ol>\n<li>create folder;</li>\n<li><p>create Dockerfile;</p>\n <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">FROM python:2.7</div><div class=\"line\">ENV PYTHONUNBUFFERED 1</div><div class=\"line\">RUN mkdir /code</div><div class=\"line\">WORKDIR /code</div><div class=\"line\">ADD requirements.txt /code/</div><div class=\"line\">RUN pip install -r requirements.txt</div><div class=\"line\">ADD . /code/</div></pre></td></tr></table></figure>\n</li>\n<li><p>create requirements.txt;</p>\n <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">Django</div><div class=\"line\">psycopg2</div></pre></td></tr></table></figure>\n</li>\n<li><p>create docker-compose.yml</p>\n <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\">version: &apos;2&apos;</div><div class=\"line\">services:</div><div class=\"line\">  db:</div><div class=\"line\">    image: postgres</div><div class=\"line\">  web:</div><div class=\"line\">    build: .</div><div class=\"line\">    command: python manage.py runserver 0.0.0.0:8000</div><div class=\"line\">    volumes:</div><div class=\"line\">      - .:/code</div><div class=\"line\">    ports:</div><div class=\"line\">      - &quot;8000:8000&quot;</div><div class=\"line\">    depends_on:</div><div class=\"line\">      - db</div></pre></td></tr></table></figure>\n</li>\n</ol>\n</li>\n<li><p>Create a Django project;</p>\n<ol>\n<li>goto root dir;</li>\n<li>docker-compose run web django-admin.py startproject composeexample .</li>\n<li>ls -l; sudo chown -R $USER:$USER .;</li>\n</ol>\n</li>\n<li><p>Connect the database;</p>\n<ol>\n<li><p>edit composeexample/settings.py;</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">DATABASES = &#123;</div><div class=\"line\">     &apos;default&apos;: &#123;</div><div class=\"line\">         &apos;ENGINE&apos;: &apos;django.db.backends.postgresql&apos;,</div><div class=\"line\">         &apos;NAME&apos;: &apos;postgres&apos;,</div><div class=\"line\">         &apos;USER&apos;: &apos;postgres&apos;,</div><div class=\"line\">         &apos;HOST&apos;: &apos;db&apos;,</div><div class=\"line\">         &apos;PORT&apos;: 5432,</div><div class=\"line\">     &#125;</div><div class=\"line\"> &#125;</div></pre></td></tr></table></figure>\n</li>\n<li><p>$ docker-compose up</p>\n</li>\n</ol>\n</li>\n</ul>\n</li>\n</ul>\n"},{"title":"python portal","_content":"\n# about\n\n# link\n\n    - official: https://www.python.org/\n    - pep: http://zh-google-styleguide.readthedocs.org/en/latest/google-python-styleguide/python_style_rules/（至少过一遍，否则视野会局限，可以不看Python书但需要熟读官方手册）\n    - awesome\n    https://github.com/vinta/awesome-python\n    https://github.com/Junnplus/awesome-python-books\n# doc\n    https://learnpythonthehardway.org/book/\n    http://www.learnpython.org/\n# tut\n    http://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000\n\n# faq\n    https://www.oschina.net/question/tag/python","source":"_posts/lang-python-portal.md","raw":"---\ntitle: python portal\ncategories:\n- python\ntags:\n- portal\n---\n\n# about\n\n# link\n\n    - official: https://www.python.org/\n    - pep: http://zh-google-styleguide.readthedocs.org/en/latest/google-python-styleguide/python_style_rules/（至少过一遍，否则视野会局限，可以不看Python书但需要熟读官方手册）\n    - awesome\n    https://github.com/vinta/awesome-python\n    https://github.com/Junnplus/awesome-python-books\n# doc\n    https://learnpythonthehardway.org/book/\n    http://www.learnpython.org/\n# tut\n    http://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000\n\n# faq\n    https://www.oschina.net/question/tag/python","slug":"lang-python-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-02-27T03:11:35.000Z","_id":"ciypwm3bw005221sv9xsoxqtn","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><pre><code>- official: https://www.python.org/\n- pep: http://zh-google-styleguide.readthedocs.org/en/latest/google-python-styleguide/python_style_rules/（至少过一遍，否则视野会局限，可以不看Python书但需要熟读官方手册）\n- awesome\nhttps://github.com/vinta/awesome-python\nhttps://github.com/Junnplus/awesome-python-books\n</code></pre><h1 id=\"doc\"><a href=\"#doc\" class=\"headerlink\" title=\"doc\"></a>doc</h1><pre><code>https://learnpythonthehardway.org/book/\nhttp://www.learnpython.org/\n</code></pre><h1 id=\"tut\"><a href=\"#tut\" class=\"headerlink\" title=\"tut\"></a>tut</h1><pre><code>http://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000\n</code></pre><h1 id=\"faq\"><a href=\"#faq\" class=\"headerlink\" title=\"faq\"></a>faq</h1><pre><code>https://www.oschina.net/question/tag/python\n</code></pre>","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><pre><code>- official: https://www.python.org/\n- pep: http://zh-google-styleguide.readthedocs.org/en/latest/google-python-styleguide/python_style_rules/（至少过一遍，否则视野会局限，可以不看Python书但需要熟读官方手册）\n- awesome\nhttps://github.com/vinta/awesome-python\nhttps://github.com/Junnplus/awesome-python-books\n</code></pre><h1 id=\"doc\"><a href=\"#doc\" class=\"headerlink\" title=\"doc\"></a>doc</h1><pre><code>https://learnpythonthehardway.org/book/\nhttp://www.learnpython.org/\n</code></pre><h1 id=\"tut\"><a href=\"#tut\" class=\"headerlink\" title=\"tut\"></a>tut</h1><pre><code>http://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000\n</code></pre><h1 id=\"faq\"><a href=\"#faq\" class=\"headerlink\" title=\"faq\"></a>faq</h1><pre><code>https://www.oschina.net/question/tag/python\n</code></pre>"},{"title":"microservice portal","_content":"\n# about\n\n- current\n\n    - 多层架构\n\n            表现层，业务逻辑层，数据层\n            分层设计与优化，合理设计接口，每层可再细分，功能模块可重用\n    \n    - 单体模式\n\n            monolith，是目前主流打包方式\n            一个单独的java war文件，rails或node中一个单独的目录\n            优势：业界熟练使用，生态健全，外围工具丰富。易于开发，测试，部署\n            开始项目最简单快捷的方式，充分利用已有代码和工具，不必担心分布式部署。\n            但是应用工程变得负责，敏捷和部署举步维艰，启动时间长，难以采用新技术。可靠性差。\n\n- 微服务架构\n\n    - 优势\n\n            由多个独立运行的微小服务构成\n            使用轻量级通讯机制\n                独立构建部署\n            每个服务保持独立性\n                构建，部署，扩容，容错，数据管理\n            敏捷最大化\n                代码运行速度更高，更短的反馈周期，更简单的使用方法，快速应对变化\n            可以使用不同技术\n                每个服务可以使用独立技术栈\n                易于重构，分散式管理\n            高效团队\n                小规模团队\n                责任明晰，便捷清晰\n                围绕业务功能进行组织，非常灵活\n\n    - 不足\n\n            过度关注服务大小，可能过度拆分\n            分布式系统的构建与部署问题\n            分布式的数据架构\n            测试的复杂度\n            改动带来的沟通成本\n\n# link\n\n    main: http://microservices.io/patterns/microservices.html\n    http://microservices.io/\n\n    conf:\n        http://2017.qconbeijing.com/track/58\n\n# project\n\n    iron.io: Microservices For The Enterprise\n    http://www.iron.io/ \n\n# clusterup\n\n    about\n        life cycle management GUI for docker microservices\n        Real-time monitoring of Docker containers and applications\n        Manage and monitor your app pre-production. We provide app analytics\n    link\n        https://clusterup.io/\n        ","source":"_posts/microservice-portal.md","raw":"---\ntitle: microservice portal\ncategories:\n- microservice\ntags:\n- portal\n---\n\n# about\n\n- current\n\n    - 多层架构\n\n            表现层，业务逻辑层，数据层\n            分层设计与优化，合理设计接口，每层可再细分，功能模块可重用\n    \n    - 单体模式\n\n            monolith，是目前主流打包方式\n            一个单独的java war文件，rails或node中一个单独的目录\n            优势：业界熟练使用，生态健全，外围工具丰富。易于开发，测试，部署\n            开始项目最简单快捷的方式，充分利用已有代码和工具，不必担心分布式部署。\n            但是应用工程变得负责，敏捷和部署举步维艰，启动时间长，难以采用新技术。可靠性差。\n\n- 微服务架构\n\n    - 优势\n\n            由多个独立运行的微小服务构成\n            使用轻量级通讯机制\n                独立构建部署\n            每个服务保持独立性\n                构建，部署，扩容，容错，数据管理\n            敏捷最大化\n                代码运行速度更高，更短的反馈周期，更简单的使用方法，快速应对变化\n            可以使用不同技术\n                每个服务可以使用独立技术栈\n                易于重构，分散式管理\n            高效团队\n                小规模团队\n                责任明晰，便捷清晰\n                围绕业务功能进行组织，非常灵活\n\n    - 不足\n\n            过度关注服务大小，可能过度拆分\n            分布式系统的构建与部署问题\n            分布式的数据架构\n            测试的复杂度\n            改动带来的沟通成本\n\n# link\n\n    main: http://microservices.io/patterns/microservices.html\n    http://microservices.io/\n\n    conf:\n        http://2017.qconbeijing.com/track/58\n\n# project\n\n    iron.io: Microservices For The Enterprise\n    http://www.iron.io/ \n\n# clusterup\n\n    about\n        life cycle management GUI for docker microservices\n        Real-time monitoring of Docker containers and applications\n        Manage and monitor your app pre-production. We provide app analytics\n    link\n        https://clusterup.io/\n        ","slug":"microservice-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-02-04T08:22:33.000Z","_id":"ciypwm3bz005521sv6q6whiyk","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><ul>\n<li><p>current</p>\n<ul>\n<li><p>多层架构</p>\n<pre><code>表现层，业务逻辑层，数据层\n分层设计与优化，合理设计接口，每层可再细分，功能模块可重用\n</code></pre></li>\n<li><p>单体模式</p>\n<pre><code>monolith，是目前主流打包方式\n一个单独的java war文件，rails或node中一个单独的目录\n优势：业界熟练使用，生态健全，外围工具丰富。易于开发，测试，部署\n开始项目最简单快捷的方式，充分利用已有代码和工具，不必担心分布式部署。\n但是应用工程变得负责，敏捷和部署举步维艰，启动时间长，难以采用新技术。可靠性差。\n</code></pre></li>\n</ul>\n</li>\n<li><p>微服务架构</p>\n<ul>\n<li><p>优势</p>\n<pre><code>由多个独立运行的微小服务构成\n使用轻量级通讯机制\n    独立构建部署\n每个服务保持独立性\n    构建，部署，扩容，容错，数据管理\n敏捷最大化\n    代码运行速度更高，更短的反馈周期，更简单的使用方法，快速应对变化\n可以使用不同技术\n    每个服务可以使用独立技术栈\n    易于重构，分散式管理\n高效团队\n    小规模团队\n    责任明晰，便捷清晰\n    围绕业务功能进行组织，非常灵活\n</code></pre></li>\n<li><p>不足</p>\n<pre><code>过度关注服务大小，可能过度拆分\n分布式系统的构建与部署问题\n分布式的数据架构\n测试的复杂度\n改动带来的沟通成本\n</code></pre></li>\n</ul>\n</li>\n</ul>\n<h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><pre><code>main: http://microservices.io/patterns/microservices.html\nhttp://microservices.io/\n\nconf:\n    http://2017.qconbeijing.com/track/58\n</code></pre><h1 id=\"project\"><a href=\"#project\" class=\"headerlink\" title=\"project\"></a>project</h1><pre><code>iron.io: Microservices For The Enterprise\nhttp://www.iron.io/ \n</code></pre><h1 id=\"clusterup\"><a href=\"#clusterup\" class=\"headerlink\" title=\"clusterup\"></a>clusterup</h1><pre><code>about\n    life cycle management GUI for docker microservices\n    Real-time monitoring of Docker containers and applications\n    Manage and monitor your app pre-production. We provide app analytics\nlink\n    https://clusterup.io/\n</code></pre>","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><ul>\n<li><p>current</p>\n<ul>\n<li><p>多层架构</p>\n<pre><code>表现层，业务逻辑层，数据层\n分层设计与优化，合理设计接口，每层可再细分，功能模块可重用\n</code></pre></li>\n<li><p>单体模式</p>\n<pre><code>monolith，是目前主流打包方式\n一个单独的java war文件，rails或node中一个单独的目录\n优势：业界熟练使用，生态健全，外围工具丰富。易于开发，测试，部署\n开始项目最简单快捷的方式，充分利用已有代码和工具，不必担心分布式部署。\n但是应用工程变得负责，敏捷和部署举步维艰，启动时间长，难以采用新技术。可靠性差。\n</code></pre></li>\n</ul>\n</li>\n<li><p>微服务架构</p>\n<ul>\n<li><p>优势</p>\n<pre><code>由多个独立运行的微小服务构成\n使用轻量级通讯机制\n    独立构建部署\n每个服务保持独立性\n    构建，部署，扩容，容错，数据管理\n敏捷最大化\n    代码运行速度更高，更短的反馈周期，更简单的使用方法，快速应对变化\n可以使用不同技术\n    每个服务可以使用独立技术栈\n    易于重构，分散式管理\n高效团队\n    小规模团队\n    责任明晰，便捷清晰\n    围绕业务功能进行组织，非常灵活\n</code></pre></li>\n<li><p>不足</p>\n<pre><code>过度关注服务大小，可能过度拆分\n分布式系统的构建与部署问题\n分布式的数据架构\n测试的复杂度\n改动带来的沟通成本\n</code></pre></li>\n</ul>\n</li>\n</ul>\n<h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><pre><code>main: http://microservices.io/patterns/microservices.html\nhttp://microservices.io/\n\nconf:\n    http://2017.qconbeijing.com/track/58\n</code></pre><h1 id=\"project\"><a href=\"#project\" class=\"headerlink\" title=\"project\"></a>project</h1><pre><code>iron.io: Microservices For The Enterprise\nhttp://www.iron.io/ \n</code></pre><h1 id=\"clusterup\"><a href=\"#clusterup\" class=\"headerlink\" title=\"clusterup\"></a>clusterup</h1><pre><code>about\n    life cycle management GUI for docker microservices\n    Real-time monitoring of Docker containers and applications\n    Manage and monitor your app pre-production. We provide app analytics\nlink\n    https://clusterup.io/\n</code></pre>"},{"title":"ruby on rails core","_content":"\n# about\n\n# link\n\nhttps://ruby-china.org/","source":"_posts/lang-ruby-rails-core.md","raw":"---\ntitle: ruby on rails core\ncategories:\n- ruby\ntags:\n- core\n- rails\n---\n\n# about\n\n# link\n\nhttps://ruby-china.org/","slug":"lang-ruby-rails-core","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-02-26T16:39:29.000Z","_id":"ciypwm3c2005921sv50p1s021","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><p><a href=\"https://ruby-china.org/\" target=\"_blank\" rel=\"external\">https://ruby-china.org/</a></p>\n","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><p><a href=\"https://ruby-china.org/\">https://ruby-china.org/</a></p>\n"},{"title":"http core","_content":"\n#  core","source":"_posts/network-http-core.md","raw":"---\ntitle: http core\ncategories:\n- network\ntags:\n- core\n- http\n---\n\n#  core","slug":"network-http-core","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:34:31.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm3c4005d21sv93q4gepl","content":"<h1 id=\"core\"><a href=\"#core\" class=\"headerlink\" title=\"core\"></a>core</h1>","excerpt":"","more":"<h1 id=\"core\"><a href=\"#core\" class=\"headerlink\" title=\"core\"></a>core</h1>"},{"title":"network portal","_content":"\n# about\n\n\n# websocket\n","source":"_posts/network-portal.md","raw":"---\ntitle: network portal\ncategories:\n- network\ntags:\n- portal\n---\n\n# about\n\n\n# websocket\n","slug":"network-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-15T13:29:19.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm3c7005h21svamd3gs1p","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><h1 id=\"websocket\"><a href=\"#websocket\" class=\"headerlink\" title=\"websocket\"></a>websocket</h1>","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><h1 id=\"websocket\"><a href=\"#websocket\" class=\"headerlink\" title=\"websocket\"></a>websocket</h1>"},{"title":"linux portal","_content":"\n#  ","source":"_posts/system-linux-portal.md","raw":"---\ntitle: linux portal\ncategories:\n- linux\ntags:\n- portal\n---\n\n#  ","slug":"system-linux-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-11T16:34:06.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciypwm3cb005l21svlf0gc6a4","content":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\" \"></a> </h1>","excerpt":"","more":"<h1 id=\"\"><a href=\"#\" class=\"headerlink\" title=\" \"></a> </h1>"},{"title":"blockchain portal","_content":"\n# blockchain\n\nzhihu: https://www.zhihu.com/question/37290469\nIBM: https://github.com/IBM-Blockchain\n101:\n1. https://www.ibm.com/developerworks/cloud/library/cl-ibm-blockchain-101-quick-start-guide-for-developers-bluemix-trs/index.html\n2. http://blog.codinglabs.org/articles/bitcoin-mechanism-make-easy.html\n3. http://blockchaindev.org/article/introduce_blockchain.html\n\n# coin:\nhttp://www.laiteb.com/\nhttp://ethfans.org/\n\n\n# project\nhttps://github.com/IBM-Blockchain/fabric-images\n","source":"_posts/blockchain-portal.md","raw":"---\ntitle: blockchain portal\ncategories:\n- blockchain\ntags:\n- portal\n- blockchain\n---\n\n# blockchain\n\nzhihu: https://www.zhihu.com/question/37290469\nIBM: https://github.com/IBM-Blockchain\n101:\n1. https://www.ibm.com/developerworks/cloud/library/cl-ibm-blockchain-101-quick-start-guide-for-developers-bluemix-trs/index.html\n2. http://blog.codinglabs.org/articles/bitcoin-mechanism-make-easy.html\n3. http://blockchaindev.org/article/introduce_blockchain.html\n\n# coin:\nhttp://www.laiteb.com/\nhttp://ethfans.org/\n\n\n# project\nhttps://github.com/IBM-Blockchain/fabric-images\n","slug":"blockchain-portal","published":1,"date":"2017-04-02T00:50:53.000Z","updated":"2017-04-03T02:46:03.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj11isjlc0000bcsvn0f1zmvv","content":"<h1 id=\"blockchain\"><a href=\"#blockchain\" class=\"headerlink\" title=\"blockchain\"></a>blockchain</h1><p>zhihu: <a href=\"https://www.zhihu.com/question/37290469\" target=\"_blank\" rel=\"external\">https://www.zhihu.com/question/37290469</a><br>IBM: <a href=\"https://github.com/IBM-Blockchain\" target=\"_blank\" rel=\"external\">https://github.com/IBM-Blockchain</a><br>101:</p>\n<ol>\n<li><a href=\"https://www.ibm.com/developerworks/cloud/library/cl-ibm-blockchain-101-quick-start-guide-for-developers-bluemix-trs/index.html\" target=\"_blank\" rel=\"external\">https://www.ibm.com/developerworks/cloud/library/cl-ibm-blockchain-101-quick-start-guide-for-developers-bluemix-trs/index.html</a></li>\n<li><a href=\"http://blog.codinglabs.org/articles/bitcoin-mechanism-make-easy.html\" target=\"_blank\" rel=\"external\">http://blog.codinglabs.org/articles/bitcoin-mechanism-make-easy.html</a></li>\n<li><a href=\"http://blockchaindev.org/article/introduce_blockchain.html\" target=\"_blank\" rel=\"external\">http://blockchaindev.org/article/introduce_blockchain.html</a></li>\n</ol>\n<h1 id=\"coin\"><a href=\"#coin\" class=\"headerlink\" title=\"coin:\"></a>coin:</h1><p><a href=\"http://www.laiteb.com/\" target=\"_blank\" rel=\"external\">http://www.laiteb.com/</a><br><a href=\"http://ethfans.org/\" target=\"_blank\" rel=\"external\">http://ethfans.org/</a></p>\n<h1 id=\"project\"><a href=\"#project\" class=\"headerlink\" title=\"project\"></a>project</h1><p><a href=\"https://github.com/IBM-Blockchain/fabric-images\" target=\"_blank\" rel=\"external\">https://github.com/IBM-Blockchain/fabric-images</a></p>\n","excerpt":"","more":"<h1 id=\"blockchain\"><a href=\"#blockchain\" class=\"headerlink\" title=\"blockchain\"></a>blockchain</h1><p>zhihu: <a href=\"https://www.zhihu.com/question/37290469\">https://www.zhihu.com/question/37290469</a><br>IBM: <a href=\"https://github.com/IBM-Blockchain\">https://github.com/IBM-Blockchain</a><br>101:</p>\n<ol>\n<li><a href=\"https://www.ibm.com/developerworks/cloud/library/cl-ibm-blockchain-101-quick-start-guide-for-developers-bluemix-trs/index.html\">https://www.ibm.com/developerworks/cloud/library/cl-ibm-blockchain-101-quick-start-guide-for-developers-bluemix-trs/index.html</a></li>\n<li><a href=\"http://blog.codinglabs.org/articles/bitcoin-mechanism-make-easy.html\">http://blog.codinglabs.org/articles/bitcoin-mechanism-make-easy.html</a></li>\n<li><a href=\"http://blockchaindev.org/article/introduce_blockchain.html\">http://blockchaindev.org/article/introduce_blockchain.html</a></li>\n</ol>\n<h1 id=\"coin\"><a href=\"#coin\" class=\"headerlink\" title=\"coin:\"></a>coin:</h1><p><a href=\"http://www.laiteb.com/\">http://www.laiteb.com/</a><br><a href=\"http://ethfans.org/\">http://ethfans.org/</a></p>\n<h1 id=\"project\"><a href=\"#project\" class=\"headerlink\" title=\"project\"></a>project</h1><p><a href=\"https://github.com/IBM-Blockchain/fabric-images\">https://github.com/IBM-Blockchain/fabric-images</a></p>\n"},{"title":"community","_content":"# awesome\nhttps://github.com/sindresorhus/awesome\n\n# info\nhttps://hackernoon.com/\n\n# coding\nhttp://blog.forecho.com/\n\n# guide\nhttp://manishearth.github.io/blog/2016/01/03/making-your-open-source-project-newcomer-friendly/","source":"_posts/community.md","raw":"---\ntitle: community\ncategories:\n- community\ntags:\n- opensource\n---\n# awesome\nhttps://github.com/sindresorhus/awesome\n\n# info\nhttps://hackernoon.com/\n\n# coding\nhttp://blog.forecho.com/\n\n# guide\nhttp://manishearth.github.io/blog/2016/01/03/making-your-open-source-project-newcomer-friendly/","slug":"community","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-02-27T02:54:40.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj11isjll0001bcsvhyi9d342","content":"<h1 id=\"awesome\"><a href=\"#awesome\" class=\"headerlink\" title=\"awesome\"></a>awesome</h1><p><a href=\"https://github.com/sindresorhus/awesome\" target=\"_blank\" rel=\"external\">https://github.com/sindresorhus/awesome</a></p>\n<h1 id=\"info\"><a href=\"#info\" class=\"headerlink\" title=\"info\"></a>info</h1><p><a href=\"https://hackernoon.com/\" target=\"_blank\" rel=\"external\">https://hackernoon.com/</a></p>\n<h1 id=\"coding\"><a href=\"#coding\" class=\"headerlink\" title=\"coding\"></a>coding</h1><p><a href=\"http://blog.forecho.com/\" target=\"_blank\" rel=\"external\">http://blog.forecho.com/</a></p>\n<h1 id=\"guide\"><a href=\"#guide\" class=\"headerlink\" title=\"guide\"></a>guide</h1><p><a href=\"http://manishearth.github.io/blog/2016/01/03/making-your-open-source-project-newcomer-friendly/\" target=\"_blank\" rel=\"external\">http://manishearth.github.io/blog/2016/01/03/making-your-open-source-project-newcomer-friendly/</a></p>\n","excerpt":"","more":"<h1 id=\"awesome\"><a href=\"#awesome\" class=\"headerlink\" title=\"awesome\"></a>awesome</h1><p><a href=\"https://github.com/sindresorhus/awesome\">https://github.com/sindresorhus/awesome</a></p>\n<h1 id=\"info\"><a href=\"#info\" class=\"headerlink\" title=\"info\"></a>info</h1><p><a href=\"https://hackernoon.com/\">https://hackernoon.com/</a></p>\n<h1 id=\"coding\"><a href=\"#coding\" class=\"headerlink\" title=\"coding\"></a>coding</h1><p><a href=\"http://blog.forecho.com/\">http://blog.forecho.com/</a></p>\n<h1 id=\"guide\"><a href=\"#guide\" class=\"headerlink\" title=\"guide\"></a>guide</h1><p><a href=\"http://manishearth.github.io/blog/2016/01/03/making-your-open-source-project-newcomer-friendly/\">http://manishearth.github.io/blog/2016/01/03/making-your-open-source-project-newcomer-friendly/</a></p>\n"},{"title":"k8s core","_content":"\n#  Alibaba Cloud\n\nguide: https://yq.aliyun.com/articles/68921?spm=5176.100240.searchblog.20.kVq4L7\n\n## prepare\n\n1. create vpc\nvpc, vswitch,\n\n2. create 2 ecs\nCentOS 7.2-x64, Ubuntu 16.04 x64\ncreate security group;\n\n3. connect ecs 1 as master\n$ssh root@112.74.175.225\n$hostname # ecs instance name\nexample: \n    $curl -L 'http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh' | bash -s nice --node-type master --key-id $ACCESS_KEY_ID --key-secret $ACCESS_KEY_SECRET --region $REGION --discovery token://\n\ninstall:\n    $curl -L 'http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh' | bash -s nice --node-type master --key-id pFR3aVQnXvjZzwKn --key-secret Xil10SqrVumfSfd7ESNjSIcNlsrm1V --region cn-shenzhen --discovery token://\nuninstall:\n    $curl -L 'http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh' | bash -s nice --node-type down\nlog：\n    $ journalctl -u kubelet -f\n\nkubeadm join --discovery token://3f3793:697efc725dcda9e8@192.168.1.237:9898\n\n4. connect ecs 2 3 as node\ninstall: \n    $ curl -L 'http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh' | bash -s nice --node-type node --key-id pFR3aVQnXvjZzwKn --key-secret Xil10SqrVumfSfd7ESNjSIcNlsrm1V --region cn-shenzhen --discovery token://3f3793:697efc725dcda9e8@192.168.1.237:9898\n\n# kubectl get nodes\nNAME                      STATUS         AGE       VERSION\nizwz92didsxigwodbj5mztz   Ready          27m       v1.6.0-alpha.0.2229+88fbc68ad99479-dirty\nizwz99k8xfd8rnweyyp1y2z   Ready          25m       v1.6.0-alpha.0.2229+88fbc68ad99479-dirty\nizwz99o2kucvflwkkpf9d2z   Ready,master   40m       v1.6.0-alpha.0.2229+88fbc68ad99479-dirty\n\n5. network setting\ngoto master:\n# curl -sSL http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/conf/flannel-vpc.yml -o flannel-vpc.yml\n# vi flannel-vpc.yml \n# kubectl apply -f flannel-vpc.yml\nconfigmap \"kube-flannel-cfg\" created\ndaemonset \"kube-flannel-ds\" created\n\n# kubectl --namespace=kube-system get ds\nNAME              DESIRED   CURRENT   READY     NODE-SELECTOR                   AGE\nkube-flannel-ds   3         3         3         beta.kubernetes.io/arch=amd64   1m\nkube-proxy        3         3         3         <none>                          43m\n\n6. create application\ngoto master:\n# kubectl run nginx --image=registry.cn-hangzhou.aliyuncs.com/spacexnice/nginx:latest --replicas=2 --labels run=nginx\ndeployment \"nginx\" created\n\n# kubectl get po\nNAME                     READY     STATUS    RESTARTS   AGE\nnginx-3579028506-rglm7   1/1       Running   0          38s\nnginx-3579028506-rtgsw   1/1       Running   0          38s\n\n# kubectl expose deployment nginx --port=80 --target-port=80 --type=LoadBalancer\nservice \"nginx\" exposed\n\n# kubectl get svc\nNAME         CLUSTER-IP     EXTERNAL-IP     PORT(S)        AGE\nkubernetes   172.19.0.1     <none>          443/TCP        46m\nnginx        172.19.3.137   120.77.128.24   80:32392/TCP   17s\n\nNOTICE：\n1. kubectl 命令的配置目前只放在了master上，但这并不意味着创建的应用都在Master上， 这是集群范围的\n如果想其他地方执行Kubectl命令的话，把/etc/kubernetes/admin.conf copy过去就可以了\n\n2. dockerfile\nFrom ubuntu:16.04\nRUN  apt update && apt install nginx\nENTRYPOINT  nginx\nCMD [\"-g\",\"daemon off;\"]\n\n","source":"_posts/docker-k8s-install.md","raw":"---\ntitle: k8s core\ncategories:\n- docker\ntags:\n- core\n---\n\n#  Alibaba Cloud\n\nguide: https://yq.aliyun.com/articles/68921?spm=5176.100240.searchblog.20.kVq4L7\n\n## prepare\n\n1. create vpc\nvpc, vswitch,\n\n2. create 2 ecs\nCentOS 7.2-x64, Ubuntu 16.04 x64\ncreate security group;\n\n3. connect ecs 1 as master\n$ssh root@112.74.175.225\n$hostname # ecs instance name\nexample: \n    $curl -L 'http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh' | bash -s nice --node-type master --key-id $ACCESS_KEY_ID --key-secret $ACCESS_KEY_SECRET --region $REGION --discovery token://\n\ninstall:\n    $curl -L 'http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh' | bash -s nice --node-type master --key-id pFR3aVQnXvjZzwKn --key-secret Xil10SqrVumfSfd7ESNjSIcNlsrm1V --region cn-shenzhen --discovery token://\nuninstall:\n    $curl -L 'http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh' | bash -s nice --node-type down\nlog：\n    $ journalctl -u kubelet -f\n\nkubeadm join --discovery token://3f3793:697efc725dcda9e8@192.168.1.237:9898\n\n4. connect ecs 2 3 as node\ninstall: \n    $ curl -L 'http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh' | bash -s nice --node-type node --key-id pFR3aVQnXvjZzwKn --key-secret Xil10SqrVumfSfd7ESNjSIcNlsrm1V --region cn-shenzhen --discovery token://3f3793:697efc725dcda9e8@192.168.1.237:9898\n\n# kubectl get nodes\nNAME                      STATUS         AGE       VERSION\nizwz92didsxigwodbj5mztz   Ready          27m       v1.6.0-alpha.0.2229+88fbc68ad99479-dirty\nizwz99k8xfd8rnweyyp1y2z   Ready          25m       v1.6.0-alpha.0.2229+88fbc68ad99479-dirty\nizwz99o2kucvflwkkpf9d2z   Ready,master   40m       v1.6.0-alpha.0.2229+88fbc68ad99479-dirty\n\n5. network setting\ngoto master:\n# curl -sSL http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/conf/flannel-vpc.yml -o flannel-vpc.yml\n# vi flannel-vpc.yml \n# kubectl apply -f flannel-vpc.yml\nconfigmap \"kube-flannel-cfg\" created\ndaemonset \"kube-flannel-ds\" created\n\n# kubectl --namespace=kube-system get ds\nNAME              DESIRED   CURRENT   READY     NODE-SELECTOR                   AGE\nkube-flannel-ds   3         3         3         beta.kubernetes.io/arch=amd64   1m\nkube-proxy        3         3         3         <none>                          43m\n\n6. create application\ngoto master:\n# kubectl run nginx --image=registry.cn-hangzhou.aliyuncs.com/spacexnice/nginx:latest --replicas=2 --labels run=nginx\ndeployment \"nginx\" created\n\n# kubectl get po\nNAME                     READY     STATUS    RESTARTS   AGE\nnginx-3579028506-rglm7   1/1       Running   0          38s\nnginx-3579028506-rtgsw   1/1       Running   0          38s\n\n# kubectl expose deployment nginx --port=80 --target-port=80 --type=LoadBalancer\nservice \"nginx\" exposed\n\n# kubectl get svc\nNAME         CLUSTER-IP     EXTERNAL-IP     PORT(S)        AGE\nkubernetes   172.19.0.1     <none>          443/TCP        46m\nnginx        172.19.3.137   120.77.128.24   80:32392/TCP   17s\n\nNOTICE：\n1. kubectl 命令的配置目前只放在了master上，但这并不意味着创建的应用都在Master上， 这是集群范围的\n如果想其他地方执行Kubectl命令的话，把/etc/kubernetes/admin.conf copy过去就可以了\n\n2. dockerfile\nFrom ubuntu:16.04\nRUN  apt update && apt install nginx\nENTRYPOINT  nginx\nCMD [\"-g\",\"daemon off;\"]\n\n","slug":"docker-k8s-install","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-02-18T10:36:42.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj11isjnf0004bcsvmdovywqw","content":"<h1 id=\"Alibaba-Cloud\"><a href=\"#Alibaba-Cloud\" class=\"headerlink\" title=\"Alibaba Cloud\"></a>Alibaba Cloud</h1><p>guide: <a href=\"https://yq.aliyun.com/articles/68921?spm=5176.100240.searchblog.20.kVq4L7\" target=\"_blank\" rel=\"external\">https://yq.aliyun.com/articles/68921?spm=5176.100240.searchblog.20.kVq4L7</a></p>\n<h2 id=\"prepare\"><a href=\"#prepare\" class=\"headerlink\" title=\"prepare\"></a>prepare</h2><ol>\n<li><p>create vpc<br>vpc, vswitch,</p>\n</li>\n<li><p>create 2 ecs<br>CentOS 7.2-x64, Ubuntu 16.04 x64<br>create security group;</p>\n</li>\n<li><p>connect ecs 1 as master<br>$ssh root@112.74.175.225<br>$hostname # ecs instance name<br>example:<br> $curl -L ‘<a href=\"http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh\" target=\"_blank\" rel=\"external\">http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh</a>‘ | bash -s nice –node-type master –key-id $ACCESS_KEY_ID –key-secret $ACCESS_KEY_SECRET –region $REGION –discovery token://</p>\n</li>\n</ol>\n<p>install:<br>    $curl -L ‘<a href=\"http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh\" target=\"_blank\" rel=\"external\">http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh</a>‘ | bash -s nice –node-type master –key-id pFR3aVQnXvjZzwKn –key-secret Xil10SqrVumfSfd7ESNjSIcNlsrm1V –region cn-shenzhen –discovery token://<br>uninstall:<br>    $curl -L ‘<a href=\"http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh\" target=\"_blank\" rel=\"external\">http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh</a>‘ | bash -s nice –node-type down<br>log：<br>    $ journalctl -u kubelet -f</p>\n<p>kubeadm join –discovery token://3f3793:697efc725dcda9e8@192.168.1.237:9898</p>\n<ol>\n<li>connect ecs 2 3 as node<br>install:<br> $ curl -L ‘<a href=\"http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh\" target=\"_blank\" rel=\"external\">http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh</a>‘ | bash -s nice –node-type node –key-id pFR3aVQnXvjZzwKn –key-secret Xil10SqrVumfSfd7ESNjSIcNlsrm1V –region cn-shenzhen –discovery token://3f3793:697efc725dcda9e8@192.168.1.237:9898</li>\n</ol>\n<h1 id=\"kubectl-get-nodes\"><a href=\"#kubectl-get-nodes\" class=\"headerlink\" title=\"kubectl get nodes\"></a>kubectl get nodes</h1><p>NAME                      STATUS         AGE       VERSION<br>izwz92didsxigwodbj5mztz   Ready          27m       v1.6.0-alpha.0.2229+88fbc68ad99479-dirty<br>izwz99k8xfd8rnweyyp1y2z   Ready          25m       v1.6.0-alpha.0.2229+88fbc68ad99479-dirty<br>izwz99o2kucvflwkkpf9d2z   Ready,master   40m       v1.6.0-alpha.0.2229+88fbc68ad99479-dirty</p>\n<ol>\n<li>network setting<br>goto master:<h1 id=\"curl-sSL-http-aliacs-k8s-oss-cn-hangzhou-aliyuncs-com-conf-flannel-vpc-yml-o-flannel-vpc-yml\"><a href=\"#curl-sSL-http-aliacs-k8s-oss-cn-hangzhou-aliyuncs-com-conf-flannel-vpc-yml-o-flannel-vpc-yml\" class=\"headerlink\" title=\"curl -sSL http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/conf/flannel-vpc.yml -o flannel-vpc.yml\"></a>curl -sSL <a href=\"http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/conf/flannel-vpc.yml\" target=\"_blank\" rel=\"external\">http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/conf/flannel-vpc.yml</a> -o flannel-vpc.yml</h1><h1 id=\"vi-flannel-vpc-yml\"><a href=\"#vi-flannel-vpc-yml\" class=\"headerlink\" title=\"vi flannel-vpc.yml\"></a>vi flannel-vpc.yml</h1><h1 id=\"kubectl-apply-f-flannel-vpc-yml\"><a href=\"#kubectl-apply-f-flannel-vpc-yml\" class=\"headerlink\" title=\"kubectl apply -f flannel-vpc.yml\"></a>kubectl apply -f flannel-vpc.yml</h1>configmap “kube-flannel-cfg” created<br>daemonset “kube-flannel-ds” created</li>\n</ol>\n<h1 id=\"kubectl-–namespace-kube-system-get-ds\"><a href=\"#kubectl-–namespace-kube-system-get-ds\" class=\"headerlink\" title=\"kubectl –namespace=kube-system get ds\"></a>kubectl –namespace=kube-system get ds</h1><p>NAME              DESIRED   CURRENT   READY     NODE-SELECTOR                   AGE<br>kube-flannel-ds   3         3         3         beta.kubernetes.io/arch=amd64   1m<br>kube-proxy        3         3         3         <none>                          43m</none></p>\n<ol>\n<li>create application<br>goto master:<h1 id=\"kubectl-run-nginx-–image-registry-cn-hangzhou-aliyuncs-com-spacexnice-nginx-latest-–replicas-2-–labels-run-nginx\"><a href=\"#kubectl-run-nginx-–image-registry-cn-hangzhou-aliyuncs-com-spacexnice-nginx-latest-–replicas-2-–labels-run-nginx\" class=\"headerlink\" title=\"kubectl run nginx –image=registry.cn-hangzhou.aliyuncs.com/spacexnice/nginx:latest –replicas=2 –labels run=nginx\"></a>kubectl run nginx –image=registry.cn-hangzhou.aliyuncs.com/spacexnice/nginx:latest –replicas=2 –labels run=nginx</h1>deployment “nginx” created</li>\n</ol>\n<h1 id=\"kubectl-get-po\"><a href=\"#kubectl-get-po\" class=\"headerlink\" title=\"kubectl get po\"></a>kubectl get po</h1><p>NAME                     READY     STATUS    RESTARTS   AGE<br>nginx-3579028506-rglm7   1/1       Running   0          38s<br>nginx-3579028506-rtgsw   1/1       Running   0          38s</p>\n<h1 id=\"kubectl-expose-deployment-nginx-–port-80-–target-port-80-–type-LoadBalancer\"><a href=\"#kubectl-expose-deployment-nginx-–port-80-–target-port-80-–type-LoadBalancer\" class=\"headerlink\" title=\"kubectl expose deployment nginx –port=80 –target-port=80 –type=LoadBalancer\"></a>kubectl expose deployment nginx –port=80 –target-port=80 –type=LoadBalancer</h1><p>service “nginx” exposed</p>\n<h1 id=\"kubectl-get-svc\"><a href=\"#kubectl-get-svc\" class=\"headerlink\" title=\"kubectl get svc\"></a>kubectl get svc</h1><p>NAME         CLUSTER-IP     EXTERNAL-IP     PORT(S)        AGE<br>kubernetes   172.19.0.1     <none>          443/TCP        46m<br>nginx        172.19.3.137   120.77.128.24   80:32392/TCP   17s</none></p>\n<p>NOTICE：</p>\n<ol>\n<li><p>kubectl 命令的配置目前只放在了master上，但这并不意味着创建的应用都在Master上， 这是集群范围的<br>如果想其他地方执行Kubectl命令的话，把/etc/kubernetes/admin.conf copy过去就可以了</p>\n</li>\n<li><p>dockerfile<br>From ubuntu:16.04<br>RUN  apt update &amp;&amp; apt install nginx<br>ENTRYPOINT  nginx<br>CMD [“-g”,”daemon off;”]</p>\n</li>\n</ol>\n","excerpt":"","more":"<h1 id=\"Alibaba-Cloud\"><a href=\"#Alibaba-Cloud\" class=\"headerlink\" title=\"Alibaba Cloud\"></a>Alibaba Cloud</h1><p>guide: <a href=\"https://yq.aliyun.com/articles/68921?spm=5176.100240.searchblog.20.kVq4L7\">https://yq.aliyun.com/articles/68921?spm=5176.100240.searchblog.20.kVq4L7</a></p>\n<h2 id=\"prepare\"><a href=\"#prepare\" class=\"headerlink\" title=\"prepare\"></a>prepare</h2><ol>\n<li><p>create vpc<br>vpc, vswitch,</p>\n</li>\n<li><p>create 2 ecs<br>CentOS 7.2-x64, Ubuntu 16.04 x64<br>create security group;</p>\n</li>\n<li><p>connect ecs 1 as master<br>$ssh root@112.74.175.225<br>$hostname # ecs instance name<br>example:<br> $curl -L ‘<a href=\"http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh\">http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh</a>‘ | bash -s nice –node-type master –key-id $ACCESS_KEY_ID –key-secret $ACCESS_KEY_SECRET –region $REGION –discovery token://</p>\n</li>\n</ol>\n<p>install:<br>    $curl -L ‘<a href=\"http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh\">http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh</a>‘ | bash -s nice –node-type master –key-id pFR3aVQnXvjZzwKn –key-secret Xil10SqrVumfSfd7ESNjSIcNlsrm1V –region cn-shenzhen –discovery token://<br>uninstall:<br>    $curl -L ‘<a href=\"http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh\">http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh</a>‘ | bash -s nice –node-type down<br>log：<br>    $ journalctl -u kubelet -f</p>\n<p>kubeadm join –discovery token://3f3793:697efc725dcda9e8@192.168.1.237:9898</p>\n<ol>\n<li>connect ecs 2 3 as node<br>install:<br> $ curl -L ‘<a href=\"http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh\">http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/installer/kubemgr.sh</a>‘ | bash -s nice –node-type node –key-id pFR3aVQnXvjZzwKn –key-secret Xil10SqrVumfSfd7ESNjSIcNlsrm1V –region cn-shenzhen –discovery token://3f3793:697efc725dcda9e8@192.168.1.237:9898</li>\n</ol>\n<h1 id=\"kubectl-get-nodes\"><a href=\"#kubectl-get-nodes\" class=\"headerlink\" title=\"kubectl get nodes\"></a>kubectl get nodes</h1><p>NAME                      STATUS         AGE       VERSION<br>izwz92didsxigwodbj5mztz   Ready          27m       v1.6.0-alpha.0.2229+88fbc68ad99479-dirty<br>izwz99k8xfd8rnweyyp1y2z   Ready          25m       v1.6.0-alpha.0.2229+88fbc68ad99479-dirty<br>izwz99o2kucvflwkkpf9d2z   Ready,master   40m       v1.6.0-alpha.0.2229+88fbc68ad99479-dirty</p>\n<ol>\n<li>network setting<br>goto master:<h1 id=\"curl-sSL-http-aliacs-k8s-oss-cn-hangzhou-aliyuncs-com-conf-flannel-vpc-yml-o-flannel-vpc-yml\"><a href=\"#curl-sSL-http-aliacs-k8s-oss-cn-hangzhou-aliyuncs-com-conf-flannel-vpc-yml-o-flannel-vpc-yml\" class=\"headerlink\" title=\"curl -sSL http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/conf/flannel-vpc.yml -o flannel-vpc.yml\"></a>curl -sSL <a href=\"http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/conf/flannel-vpc.yml\">http://aliacs-k8s.oss-cn-hangzhou.aliyuncs.com/conf/flannel-vpc.yml</a> -o flannel-vpc.yml</h1><h1 id=\"vi-flannel-vpc-yml\"><a href=\"#vi-flannel-vpc-yml\" class=\"headerlink\" title=\"vi flannel-vpc.yml\"></a>vi flannel-vpc.yml</h1><h1 id=\"kubectl-apply-f-flannel-vpc-yml\"><a href=\"#kubectl-apply-f-flannel-vpc-yml\" class=\"headerlink\" title=\"kubectl apply -f flannel-vpc.yml\"></a>kubectl apply -f flannel-vpc.yml</h1>configmap “kube-flannel-cfg” created<br>daemonset “kube-flannel-ds” created</li>\n</ol>\n<h1 id=\"kubectl-–namespace-kube-system-get-ds\"><a href=\"#kubectl-–namespace-kube-system-get-ds\" class=\"headerlink\" title=\"kubectl –namespace=kube-system get ds\"></a>kubectl –namespace=kube-system get ds</h1><p>NAME              DESIRED   CURRENT   READY     NODE-SELECTOR                   AGE<br>kube-flannel-ds   3         3         3         beta.kubernetes.io/arch=amd64   1m<br>kube-proxy        3         3         3         <none>                          43m</p>\n<ol>\n<li>create application<br>goto master:<h1 id=\"kubectl-run-nginx-–image-registry-cn-hangzhou-aliyuncs-com-spacexnice-nginx-latest-–replicas-2-–labels-run-nginx\"><a href=\"#kubectl-run-nginx-–image-registry-cn-hangzhou-aliyuncs-com-spacexnice-nginx-latest-–replicas-2-–labels-run-nginx\" class=\"headerlink\" title=\"kubectl run nginx –image=registry.cn-hangzhou.aliyuncs.com/spacexnice/nginx:latest –replicas=2 –labels run=nginx\"></a>kubectl run nginx –image=registry.cn-hangzhou.aliyuncs.com/spacexnice/nginx:latest –replicas=2 –labels run=nginx</h1>deployment “nginx” created</li>\n</ol>\n<h1 id=\"kubectl-get-po\"><a href=\"#kubectl-get-po\" class=\"headerlink\" title=\"kubectl get po\"></a>kubectl get po</h1><p>NAME                     READY     STATUS    RESTARTS   AGE<br>nginx-3579028506-rglm7   1/1       Running   0          38s<br>nginx-3579028506-rtgsw   1/1       Running   0          38s</p>\n<h1 id=\"kubectl-expose-deployment-nginx-–port-80-–target-port-80-–type-LoadBalancer\"><a href=\"#kubectl-expose-deployment-nginx-–port-80-–target-port-80-–type-LoadBalancer\" class=\"headerlink\" title=\"kubectl expose deployment nginx –port=80 –target-port=80 –type=LoadBalancer\"></a>kubectl expose deployment nginx –port=80 –target-port=80 –type=LoadBalancer</h1><p>service “nginx” exposed</p>\n<h1 id=\"kubectl-get-svc\"><a href=\"#kubectl-get-svc\" class=\"headerlink\" title=\"kubectl get svc\"></a>kubectl get svc</h1><p>NAME         CLUSTER-IP     EXTERNAL-IP     PORT(S)        AGE<br>kubernetes   172.19.0.1     <none>          443/TCP        46m<br>nginx        172.19.3.137   120.77.128.24   80:32392/TCP   17s</p>\n<p>NOTICE：</p>\n<ol>\n<li><p>kubectl 命令的配置目前只放在了master上，但这并不意味着创建的应用都在Master上， 这是集群范围的<br>如果想其他地方执行Kubectl命令的话，把/etc/kubernetes/admin.conf copy过去就可以了</p>\n</li>\n<li><p>dockerfile<br>From ubuntu:16.04<br>RUN  apt update &amp;&amp; apt install nginx<br>ENTRYPOINT  nginx<br>CMD [“-g”,”daemon off;”]</p>\n</li>\n</ol>\n"},{"title":"cx portal","_content":"\n# about\n\n# link\ncommunity: http://golangtc.com/\ndoc: http://godoc.golangtc.com/\nblog: https://blog.golang.org/\n\n# env\nhttp://golangtc.com/t/587edd57b09ecc2e18000169\n\n# doc\nhttp://godoc.golangtc.com/doc/effective_go.html\n\n","source":"_posts/lang-go-portal.md","raw":"---\ntitle: cx portal\ncategories:\n- cx\ntags:\n- portal\n---\n\n# about\n\n# link\ncommunity: http://golangtc.com/\ndoc: http://godoc.golangtc.com/\nblog: https://blog.golang.org/\n\n# env\nhttp://golangtc.com/t/587edd57b09ecc2e18000169\n\n# doc\nhttp://godoc.golangtc.com/doc/effective_go.html\n\n","slug":"lang-go-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-02-26T17:11:05.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj11isjnk0005bcsv2dbd47px","content":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><p>community: <a href=\"http://golangtc.com/\" target=\"_blank\" rel=\"external\">http://golangtc.com/</a><br>doc: <a href=\"http://godoc.golangtc.com/\" target=\"_blank\" rel=\"external\">http://godoc.golangtc.com/</a><br>blog: <a href=\"https://blog.golang.org/\" target=\"_blank\" rel=\"external\">https://blog.golang.org/</a></p>\n<h1 id=\"env\"><a href=\"#env\" class=\"headerlink\" title=\"env\"></a>env</h1><p><a href=\"http://golangtc.com/t/587edd57b09ecc2e18000169\" target=\"_blank\" rel=\"external\">http://golangtc.com/t/587edd57b09ecc2e18000169</a></p>\n<h1 id=\"doc\"><a href=\"#doc\" class=\"headerlink\" title=\"doc\"></a>doc</h1><p><a href=\"http://godoc.golangtc.com/doc/effective_go.html\" target=\"_blank\" rel=\"external\">http://godoc.golangtc.com/doc/effective_go.html</a></p>\n","excerpt":"","more":"<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><p>community: <a href=\"http://golangtc.com/\">http://golangtc.com/</a><br>doc: <a href=\"http://godoc.golangtc.com/\">http://godoc.golangtc.com/</a><br>blog: <a href=\"https://blog.golang.org/\">https://blog.golang.org/</a></p>\n<h1 id=\"env\"><a href=\"#env\" class=\"headerlink\" title=\"env\"></a>env</h1><p><a href=\"http://golangtc.com/t/587edd57b09ecc2e18000169\">http://golangtc.com/t/587edd57b09ecc2e18000169</a></p>\n<h1 id=\"doc\"><a href=\"#doc\" class=\"headerlink\" title=\"doc\"></a>doc</h1><p><a href=\"http://godoc.golangtc.com/doc/effective_go.html\">http://godoc.golangtc.com/doc/effective_go.html</a></p>\n"},{"title":"php portal","_content":"\n# framework\nyii\nlaravel: https://laravel.com/\ncakephp\nthinkphp: https://github.com/top-think/think\n\n# yii\nhttp://www.yiiframework.com/\nhttp://www.yiichina.com/\n\nperformance： yii+workerman、yii+swoole\napi：https://github.com/bolasblack/http-api-guide\n\n\n\n# laravel\nhttps://laravel-china.org/\ndocker: http://www.jianshu.com/p/2ad17b1df2c0\nvs:http://www.danbp.org/p/node/79\n\n# cakephp\nhttps://cakephp.org/\nhttps://book.cakephp.org/3.0/en/index.html\n\n\n\n\n","source":"_posts/lang-php-portal.md","raw":"---\ntitle: php portal\ncategories:\n- php\ntags:\n- portal\n---\n\n# framework\nyii\nlaravel: https://laravel.com/\ncakephp\nthinkphp: https://github.com/top-think/think\n\n# yii\nhttp://www.yiiframework.com/\nhttp://www.yiichina.com/\n\nperformance： yii+workerman、yii+swoole\napi：https://github.com/bolasblack/http-api-guide\n\n\n\n# laravel\nhttps://laravel-china.org/\ndocker: http://www.jianshu.com/p/2ad17b1df2c0\nvs:http://www.danbp.org/p/node/79\n\n# cakephp\nhttps://cakephp.org/\nhttps://book.cakephp.org/3.0/en/index.html\n\n\n\n\n","slug":"lang-php-portal","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-02-26T16:39:41.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj11isjnr0007bcsv5hm8gpd7","content":"<h1 id=\"framework\"><a href=\"#framework\" class=\"headerlink\" title=\"framework\"></a>framework</h1><p>yii<br>laravel: <a href=\"https://laravel.com/\" target=\"_blank\" rel=\"external\">https://laravel.com/</a><br>cakephp<br>thinkphp: <a href=\"https://github.com/top-think/think\" target=\"_blank\" rel=\"external\">https://github.com/top-think/think</a></p>\n<h1 id=\"yii\"><a href=\"#yii\" class=\"headerlink\" title=\"yii\"></a>yii</h1><p><a href=\"http://www.yiiframework.com/\" target=\"_blank\" rel=\"external\">http://www.yiiframework.com/</a><br><a href=\"http://www.yiichina.com/\" target=\"_blank\" rel=\"external\">http://www.yiichina.com/</a></p>\n<p>performance： yii+workerman、yii+swoole<br>api：<a href=\"https://github.com/bolasblack/http-api-guide\" target=\"_blank\" rel=\"external\">https://github.com/bolasblack/http-api-guide</a></p>\n<h1 id=\"laravel\"><a href=\"#laravel\" class=\"headerlink\" title=\"laravel\"></a>laravel</h1><p><a href=\"https://laravel-china.org/\" target=\"_blank\" rel=\"external\">https://laravel-china.org/</a><br>docker: <a href=\"http://www.jianshu.com/p/2ad17b1df2c0\" target=\"_blank\" rel=\"external\">http://www.jianshu.com/p/2ad17b1df2c0</a><br>vs:<a href=\"http://www.danbp.org/p/node/79\" target=\"_blank\" rel=\"external\">http://www.danbp.org/p/node/79</a></p>\n<h1 id=\"cakephp\"><a href=\"#cakephp\" class=\"headerlink\" title=\"cakephp\"></a>cakephp</h1><p><a href=\"https://cakephp.org/\" target=\"_blank\" rel=\"external\">https://cakephp.org/</a><br><a href=\"https://book.cakephp.org/3.0/en/index.html\" target=\"_blank\" rel=\"external\">https://book.cakephp.org/3.0/en/index.html</a></p>\n","excerpt":"","more":"<h1 id=\"framework\"><a href=\"#framework\" class=\"headerlink\" title=\"framework\"></a>framework</h1><p>yii<br>laravel: <a href=\"https://laravel.com/\">https://laravel.com/</a><br>cakephp<br>thinkphp: <a href=\"https://github.com/top-think/think\">https://github.com/top-think/think</a></p>\n<h1 id=\"yii\"><a href=\"#yii\" class=\"headerlink\" title=\"yii\"></a>yii</h1><p><a href=\"http://www.yiiframework.com/\">http://www.yiiframework.com/</a><br><a href=\"http://www.yiichina.com/\">http://www.yiichina.com/</a></p>\n<p>performance： yii+workerman、yii+swoole<br>api：<a href=\"https://github.com/bolasblack/http-api-guide\">https://github.com/bolasblack/http-api-guide</a></p>\n<h1 id=\"laravel\"><a href=\"#laravel\" class=\"headerlink\" title=\"laravel\"></a>laravel</h1><p><a href=\"https://laravel-china.org/\">https://laravel-china.org/</a><br>docker: <a href=\"http://www.jianshu.com/p/2ad17b1df2c0\">http://www.jianshu.com/p/2ad17b1df2c0</a><br>vs:<a href=\"http://www.danbp.org/p/node/79\">http://www.danbp.org/p/node/79</a></p>\n<h1 id=\"cakephp\"><a href=\"#cakephp\" class=\"headerlink\" title=\"cakephp\"></a>cakephp</h1><p><a href=\"https://cakephp.org/\">https://cakephp.org/</a><br><a href=\"https://book.cakephp.org/3.0/en/index.html\">https://book.cakephp.org/3.0/en/index.html</a></p>\n"},{"title":"docker swarm mode","_content":"\n# link\n\n- tut: https://docs.docker.com/engine/swarm/swarm-tutorial/\n\n\n# about\n\nDocker Engine 1.12 includes swarm mode for natively managing a cluster of Docker Engines called a swarm. Use the Docker CLI to create a swarm, deploy application services to a swarm, and manage swarm behavior.\n\n# feature\n\n    - Cluster management integrated with Docker Engine: \n\n    Use the Docker Engine CLI to create a swarm of Docker Engines where you can deploy application services. You don’t need additional orchestration software to create or manage a swarm.\n\n    - Decentralized design: \n\n    Instead of handling differentiation between node roles at deployment time, the Docker Engine handles any specialization at runtime. You can deploy both kinds of nodes, managers and workers, using the Docker Engine. This means you can build an entire swarm from a single disk image.\n\n    - Declarative service model: \n\n    Docker Engine uses a declarative approach to let you define the desired state of the various services in your application stack. For example, you might describe an application comprised of a web front end service with message queueing services and a database backend.\n\n    - Scaling: \n\n    For each service, you can declare the number of tasks you want to run. When you scale up or down, the swarm manager automatically adapts by adding or removing tasks to maintain the desired state.\n\n    - Desired state reconciliation: \n\n    The swarm manager node constantly monitors the cluster state and reconciles any differences between the actual state your expressed desired state. For example, if you set up a service to run 10 replicas of a container, and a worker machine hosting two of those replicas crashes, the manager will create two new replicas to replace the replicas that crashed. The swarm manager assigns the new replicas to workers that are running and available.\n\n    - Multi-host networking: \n\n    You can specify an overlay network for your services. The swarm manager automatically assigns addresses to the containers on the overlay network when it initializes or updates the application.\n\n    - Service discovery: \n\n    Swarm manager nodes assign each service in the swarm a unique DNS name and load balances running containers. You can query every container running in the swarm through a DNS server embedded in the swarm.\n\n    - Load balancing: \n\n    You can expose the ports for services to an external load balancer. Internally, the swarm lets you specify how to distribute service containers between nodes.\n\n    - Secure by default: \n\n    Each node in the swarm enforces TLS mutual authentication and encryption to secure communications between itself and all other nodes. You have the option to use self-signed root certificates or certificates from a custom root CA.\n\n    - Rolling updates: \n\n    At rollout time you can apply service updates to nodes incrementally. The swarm manager lets you control the delay between service deployment to different sets of nodes. If anything goes wrong, you can roll-back a task to a previous version of the service.\n\n\n# key concepts\n\n- link: https://docs.docker.com/engine/swarm/key-concepts/ (Docker Engine 1.12.)\n\n- What is a swarm?\n\n    The cluster management and orchestration features embedded in the Docker Engine are built using SwarmKit. Docker engines participating in a cluster are running in swarm mode. You enable swarm mode for an engine by either initializing a swarm or joining an existing swarm.\n\n    A swarm is a cluster of Docker engines, or nodes, where you deploy services. The Docker Engine CLI and API include commands to manage swarm nodes (e.g., add or remove nodes), and deploy and orchestrate services across the swarm.\n\n    When you run Docker without using swarm mode, you execute container commands. When you run the Docker in swarm mode, you orchestrate services. You can run swarm services and standalone containers on the same Docker instances.\n\n- What is a node?\n\n    A node is an instance of the Docker engine participating in the swarm. You can also think of this as a Docker node. You can run one or more nodes on a single physical computer or cloud server, but production swarm deployments typically include Docker nodes distributed across multiple physical and cloud machines.\n\n    To deploy your application to a swarm, you submit a service definition to a manager node. The manager node dispatches units of work called tasks to worker nodes.\n\n    Manager nodes also perform the orchestration and cluster management functions required to maintain the desired state of the swarm. Manager nodes elect a single leader to conduct orchestration tasks.\n\n    Worker nodes receive and execute tasks dispatched from manager nodes. By default manager nodes also run services as worker nodes, but you can configure them to run manager tasks exclusively and be manager-only nodes. An agent runs on each worker node and reports on the tasks assigned to it. The worker node notifies the manager node of the current state of its assigned tasks so that the manager can maintain the desired state of each worker.\n\n- Services and tasks\n\n    A service is the definition of the tasks to execute on the worker nodes. It is the central structure of the swarm system and the primary root of user interaction with the swarm.\n\n    When you create a service, you specify which container image to use and which commands to execute inside running containers.\n\n    In the replicated services model, the swarm manager distributes a specific number of replica tasks among the nodes based upon the scale you set in the desired state.\n\n    For global services, the swarm runs one task for the service on every available node in the cluster.\n\n    A task carries a Docker container and the commands to run inside the container. It is the atomic scheduling unit of swarm. Manager nodes assign tasks to worker nodes according to the number of replicas set in the service scale. Once a task is assigned to a node, it cannot move to another node. It can only run on the assigned node or fail.\n\n- Load balancing\n\n    The swarm manager uses ingress load balancing to expose the services you want to make available externally to the swarm. The swarm manager can automatically assign the service a PublishedPort or you can configure a PublishedPort for the service. You can specify any unused port. If you do not specify a port, the swarm manager assigns the service a port in the 30000-32767 range.\n\n    External components, such as cloud load balancers, can access the service on the PublishedPort of any node in the cluster whether or not the node is currently running the task for the service. All nodes in the swarm route ingress connections to a running task instance.\n\n    Swarm mode has an internal DNS component that automatically assigns each service in the swarm a DNS entry. The swarm manager uses internal load balancing to distribute requests among services within the cluster based upon the DNS name of the service.\n\n# docker swarm mode command\n\n- swarm init\n\n- swarm join\n\n- service create\n\n- service inspect\n\n- service ls\n\n- service rm\n\n- service scale\n\n- service ps\n\n- service update","source":"_posts/docker-mesos.md","raw":"---\ntitle: docker swarm mode\ncategories:\n- docker\ntags:\n- core\n- swarm\n---\n\n# link\n\n- tut: https://docs.docker.com/engine/swarm/swarm-tutorial/\n\n\n# about\n\nDocker Engine 1.12 includes swarm mode for natively managing a cluster of Docker Engines called a swarm. Use the Docker CLI to create a swarm, deploy application services to a swarm, and manage swarm behavior.\n\n# feature\n\n    - Cluster management integrated with Docker Engine: \n\n    Use the Docker Engine CLI to create a swarm of Docker Engines where you can deploy application services. You don’t need additional orchestration software to create or manage a swarm.\n\n    - Decentralized design: \n\n    Instead of handling differentiation between node roles at deployment time, the Docker Engine handles any specialization at runtime. You can deploy both kinds of nodes, managers and workers, using the Docker Engine. This means you can build an entire swarm from a single disk image.\n\n    - Declarative service model: \n\n    Docker Engine uses a declarative approach to let you define the desired state of the various services in your application stack. For example, you might describe an application comprised of a web front end service with message queueing services and a database backend.\n\n    - Scaling: \n\n    For each service, you can declare the number of tasks you want to run. When you scale up or down, the swarm manager automatically adapts by adding or removing tasks to maintain the desired state.\n\n    - Desired state reconciliation: \n\n    The swarm manager node constantly monitors the cluster state and reconciles any differences between the actual state your expressed desired state. For example, if you set up a service to run 10 replicas of a container, and a worker machine hosting two of those replicas crashes, the manager will create two new replicas to replace the replicas that crashed. The swarm manager assigns the new replicas to workers that are running and available.\n\n    - Multi-host networking: \n\n    You can specify an overlay network for your services. The swarm manager automatically assigns addresses to the containers on the overlay network when it initializes or updates the application.\n\n    - Service discovery: \n\n    Swarm manager nodes assign each service in the swarm a unique DNS name and load balances running containers. You can query every container running in the swarm through a DNS server embedded in the swarm.\n\n    - Load balancing: \n\n    You can expose the ports for services to an external load balancer. Internally, the swarm lets you specify how to distribute service containers between nodes.\n\n    - Secure by default: \n\n    Each node in the swarm enforces TLS mutual authentication and encryption to secure communications between itself and all other nodes. You have the option to use self-signed root certificates or certificates from a custom root CA.\n\n    - Rolling updates: \n\n    At rollout time you can apply service updates to nodes incrementally. The swarm manager lets you control the delay between service deployment to different sets of nodes. If anything goes wrong, you can roll-back a task to a previous version of the service.\n\n\n# key concepts\n\n- link: https://docs.docker.com/engine/swarm/key-concepts/ (Docker Engine 1.12.)\n\n- What is a swarm?\n\n    The cluster management and orchestration features embedded in the Docker Engine are built using SwarmKit. Docker engines participating in a cluster are running in swarm mode. You enable swarm mode for an engine by either initializing a swarm or joining an existing swarm.\n\n    A swarm is a cluster of Docker engines, or nodes, where you deploy services. The Docker Engine CLI and API include commands to manage swarm nodes (e.g., add or remove nodes), and deploy and orchestrate services across the swarm.\n\n    When you run Docker without using swarm mode, you execute container commands. When you run the Docker in swarm mode, you orchestrate services. You can run swarm services and standalone containers on the same Docker instances.\n\n- What is a node?\n\n    A node is an instance of the Docker engine participating in the swarm. You can also think of this as a Docker node. You can run one or more nodes on a single physical computer or cloud server, but production swarm deployments typically include Docker nodes distributed across multiple physical and cloud machines.\n\n    To deploy your application to a swarm, you submit a service definition to a manager node. The manager node dispatches units of work called tasks to worker nodes.\n\n    Manager nodes also perform the orchestration and cluster management functions required to maintain the desired state of the swarm. Manager nodes elect a single leader to conduct orchestration tasks.\n\n    Worker nodes receive and execute tasks dispatched from manager nodes. By default manager nodes also run services as worker nodes, but you can configure them to run manager tasks exclusively and be manager-only nodes. An agent runs on each worker node and reports on the tasks assigned to it. The worker node notifies the manager node of the current state of its assigned tasks so that the manager can maintain the desired state of each worker.\n\n- Services and tasks\n\n    A service is the definition of the tasks to execute on the worker nodes. It is the central structure of the swarm system and the primary root of user interaction with the swarm.\n\n    When you create a service, you specify which container image to use and which commands to execute inside running containers.\n\n    In the replicated services model, the swarm manager distributes a specific number of replica tasks among the nodes based upon the scale you set in the desired state.\n\n    For global services, the swarm runs one task for the service on every available node in the cluster.\n\n    A task carries a Docker container and the commands to run inside the container. It is the atomic scheduling unit of swarm. Manager nodes assign tasks to worker nodes according to the number of replicas set in the service scale. Once a task is assigned to a node, it cannot move to another node. It can only run on the assigned node or fail.\n\n- Load balancing\n\n    The swarm manager uses ingress load balancing to expose the services you want to make available externally to the swarm. The swarm manager can automatically assign the service a PublishedPort or you can configure a PublishedPort for the service. You can specify any unused port. If you do not specify a port, the swarm manager assigns the service a port in the 30000-32767 range.\n\n    External components, such as cloud load balancers, can access the service on the PublishedPort of any node in the cluster whether or not the node is currently running the task for the service. All nodes in the swarm route ingress connections to a running task instance.\n\n    Swarm mode has an internal DNS component that automatically assigns each service in the swarm a DNS entry. The swarm manager uses internal load balancing to distribute requests among services within the cluster based upon the DNS name of the service.\n\n# docker swarm mode command\n\n- swarm init\n\n- swarm join\n\n- service create\n\n- service inspect\n\n- service ls\n\n- service rm\n\n- service scale\n\n- service ps\n\n- service update","slug":"docker-mesos","published":1,"date":"2017-01-11T14:06:12.000Z","updated":"2017-01-15T13:29:09.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj11isjny000cbcsvon4zx324","content":"<h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><ul>\n<li>tut: <a href=\"https://docs.docker.com/engine/swarm/swarm-tutorial/\" target=\"_blank\" rel=\"external\">https://docs.docker.com/engine/swarm/swarm-tutorial/</a></li>\n</ul>\n<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><p>Docker Engine 1.12 includes swarm mode for natively managing a cluster of Docker Engines called a swarm. Use the Docker CLI to create a swarm, deploy application services to a swarm, and manage swarm behavior.</p>\n<h1 id=\"feature\"><a href=\"#feature\" class=\"headerlink\" title=\"feature\"></a>feature</h1><pre><code>- Cluster management integrated with Docker Engine: \n\nUse the Docker Engine CLI to create a swarm of Docker Engines where you can deploy application services. You don’t need additional orchestration software to create or manage a swarm.\n\n- Decentralized design: \n\nInstead of handling differentiation between node roles at deployment time, the Docker Engine handles any specialization at runtime. You can deploy both kinds of nodes, managers and workers, using the Docker Engine. This means you can build an entire swarm from a single disk image.\n\n- Declarative service model: \n\nDocker Engine uses a declarative approach to let you define the desired state of the various services in your application stack. For example, you might describe an application comprised of a web front end service with message queueing services and a database backend.\n\n- Scaling: \n\nFor each service, you can declare the number of tasks you want to run. When you scale up or down, the swarm manager automatically adapts by adding or removing tasks to maintain the desired state.\n\n- Desired state reconciliation: \n\nThe swarm manager node constantly monitors the cluster state and reconciles any differences between the actual state your expressed desired state. For example, if you set up a service to run 10 replicas of a container, and a worker machine hosting two of those replicas crashes, the manager will create two new replicas to replace the replicas that crashed. The swarm manager assigns the new replicas to workers that are running and available.\n\n- Multi-host networking: \n\nYou can specify an overlay network for your services. The swarm manager automatically assigns addresses to the containers on the overlay network when it initializes or updates the application.\n\n- Service discovery: \n\nSwarm manager nodes assign each service in the swarm a unique DNS name and load balances running containers. You can query every container running in the swarm through a DNS server embedded in the swarm.\n\n- Load balancing: \n\nYou can expose the ports for services to an external load balancer. Internally, the swarm lets you specify how to distribute service containers between nodes.\n\n- Secure by default: \n\nEach node in the swarm enforces TLS mutual authentication and encryption to secure communications between itself and all other nodes. You have the option to use self-signed root certificates or certificates from a custom root CA.\n\n- Rolling updates: \n\nAt rollout time you can apply service updates to nodes incrementally. The swarm manager lets you control the delay between service deployment to different sets of nodes. If anything goes wrong, you can roll-back a task to a previous version of the service.\n</code></pre><h1 id=\"key-concepts\"><a href=\"#key-concepts\" class=\"headerlink\" title=\"key concepts\"></a>key concepts</h1><ul>\n<li><p>link: <a href=\"https://docs.docker.com/engine/swarm/key-concepts/\" target=\"_blank\" rel=\"external\">https://docs.docker.com/engine/swarm/key-concepts/</a> (Docker Engine 1.12.)</p>\n</li>\n<li><p>What is a swarm?</p>\n<p>  The cluster management and orchestration features embedded in the Docker Engine are built using SwarmKit. Docker engines participating in a cluster are running in swarm mode. You enable swarm mode for an engine by either initializing a swarm or joining an existing swarm.</p>\n<p>  A swarm is a cluster of Docker engines, or nodes, where you deploy services. The Docker Engine CLI and API include commands to manage swarm nodes (e.g., add or remove nodes), and deploy and orchestrate services across the swarm.</p>\n<p>  When you run Docker without using swarm mode, you execute container commands. When you run the Docker in swarm mode, you orchestrate services. You can run swarm services and standalone containers on the same Docker instances.</p>\n</li>\n<li><p>What is a node?</p>\n<p>  A node is an instance of the Docker engine participating in the swarm. You can also think of this as a Docker node. You can run one or more nodes on a single physical computer or cloud server, but production swarm deployments typically include Docker nodes distributed across multiple physical and cloud machines.</p>\n<p>  To deploy your application to a swarm, you submit a service definition to a manager node. The manager node dispatches units of work called tasks to worker nodes.</p>\n<p>  Manager nodes also perform the orchestration and cluster management functions required to maintain the desired state of the swarm. Manager nodes elect a single leader to conduct orchestration tasks.</p>\n<p>  Worker nodes receive and execute tasks dispatched from manager nodes. By default manager nodes also run services as worker nodes, but you can configure them to run manager tasks exclusively and be manager-only nodes. An agent runs on each worker node and reports on the tasks assigned to it. The worker node notifies the manager node of the current state of its assigned tasks so that the manager can maintain the desired state of each worker.</p>\n</li>\n<li><p>Services and tasks</p>\n<p>  A service is the definition of the tasks to execute on the worker nodes. It is the central structure of the swarm system and the primary root of user interaction with the swarm.</p>\n<p>  When you create a service, you specify which container image to use and which commands to execute inside running containers.</p>\n<p>  In the replicated services model, the swarm manager distributes a specific number of replica tasks among the nodes based upon the scale you set in the desired state.</p>\n<p>  For global services, the swarm runs one task for the service on every available node in the cluster.</p>\n<p>  A task carries a Docker container and the commands to run inside the container. It is the atomic scheduling unit of swarm. Manager nodes assign tasks to worker nodes according to the number of replicas set in the service scale. Once a task is assigned to a node, it cannot move to another node. It can only run on the assigned node or fail.</p>\n</li>\n<li><p>Load balancing</p>\n<p>  The swarm manager uses ingress load balancing to expose the services you want to make available externally to the swarm. The swarm manager can automatically assign the service a PublishedPort or you can configure a PublishedPort for the service. You can specify any unused port. If you do not specify a port, the swarm manager assigns the service a port in the 30000-32767 range.</p>\n<p>  External components, such as cloud load balancers, can access the service on the PublishedPort of any node in the cluster whether or not the node is currently running the task for the service. All nodes in the swarm route ingress connections to a running task instance.</p>\n<p>  Swarm mode has an internal DNS component that automatically assigns each service in the swarm a DNS entry. The swarm manager uses internal load balancing to distribute requests among services within the cluster based upon the DNS name of the service.</p>\n</li>\n</ul>\n<h1 id=\"docker-swarm-mode-command\"><a href=\"#docker-swarm-mode-command\" class=\"headerlink\" title=\"docker swarm mode command\"></a>docker swarm mode command</h1><ul>\n<li><p>swarm init</p>\n</li>\n<li><p>swarm join</p>\n</li>\n<li><p>service create</p>\n</li>\n<li><p>service inspect</p>\n</li>\n<li><p>service ls</p>\n</li>\n<li><p>service rm</p>\n</li>\n<li><p>service scale</p>\n</li>\n<li><p>service ps</p>\n</li>\n<li><p>service update</p>\n</li>\n</ul>\n","excerpt":"","more":"<h1 id=\"link\"><a href=\"#link\" class=\"headerlink\" title=\"link\"></a>link</h1><ul>\n<li>tut: <a href=\"https://docs.docker.com/engine/swarm/swarm-tutorial/\">https://docs.docker.com/engine/swarm/swarm-tutorial/</a></li>\n</ul>\n<h1 id=\"about\"><a href=\"#about\" class=\"headerlink\" title=\"about\"></a>about</h1><p>Docker Engine 1.12 includes swarm mode for natively managing a cluster of Docker Engines called a swarm. Use the Docker CLI to create a swarm, deploy application services to a swarm, and manage swarm behavior.</p>\n<h1 id=\"feature\"><a href=\"#feature\" class=\"headerlink\" title=\"feature\"></a>feature</h1><pre><code>- Cluster management integrated with Docker Engine: \n\nUse the Docker Engine CLI to create a swarm of Docker Engines where you can deploy application services. You don’t need additional orchestration software to create or manage a swarm.\n\n- Decentralized design: \n\nInstead of handling differentiation between node roles at deployment time, the Docker Engine handles any specialization at runtime. You can deploy both kinds of nodes, managers and workers, using the Docker Engine. This means you can build an entire swarm from a single disk image.\n\n- Declarative service model: \n\nDocker Engine uses a declarative approach to let you define the desired state of the various services in your application stack. For example, you might describe an application comprised of a web front end service with message queueing services and a database backend.\n\n- Scaling: \n\nFor each service, you can declare the number of tasks you want to run. When you scale up or down, the swarm manager automatically adapts by adding or removing tasks to maintain the desired state.\n\n- Desired state reconciliation: \n\nThe swarm manager node constantly monitors the cluster state and reconciles any differences between the actual state your expressed desired state. For example, if you set up a service to run 10 replicas of a container, and a worker machine hosting two of those replicas crashes, the manager will create two new replicas to replace the replicas that crashed. The swarm manager assigns the new replicas to workers that are running and available.\n\n- Multi-host networking: \n\nYou can specify an overlay network for your services. The swarm manager automatically assigns addresses to the containers on the overlay network when it initializes or updates the application.\n\n- Service discovery: \n\nSwarm manager nodes assign each service in the swarm a unique DNS name and load balances running containers. You can query every container running in the swarm through a DNS server embedded in the swarm.\n\n- Load balancing: \n\nYou can expose the ports for services to an external load balancer. Internally, the swarm lets you specify how to distribute service containers between nodes.\n\n- Secure by default: \n\nEach node in the swarm enforces TLS mutual authentication and encryption to secure communications between itself and all other nodes. You have the option to use self-signed root certificates or certificates from a custom root CA.\n\n- Rolling updates: \n\nAt rollout time you can apply service updates to nodes incrementally. The swarm manager lets you control the delay between service deployment to different sets of nodes. If anything goes wrong, you can roll-back a task to a previous version of the service.\n</code></pre><h1 id=\"key-concepts\"><a href=\"#key-concepts\" class=\"headerlink\" title=\"key concepts\"></a>key concepts</h1><ul>\n<li><p>link: <a href=\"https://docs.docker.com/engine/swarm/key-concepts/\">https://docs.docker.com/engine/swarm/key-concepts/</a> (Docker Engine 1.12.)</p>\n</li>\n<li><p>What is a swarm?</p>\n<p>  The cluster management and orchestration features embedded in the Docker Engine are built using SwarmKit. Docker engines participating in a cluster are running in swarm mode. You enable swarm mode for an engine by either initializing a swarm or joining an existing swarm.</p>\n<p>  A swarm is a cluster of Docker engines, or nodes, where you deploy services. The Docker Engine CLI and API include commands to manage swarm nodes (e.g., add or remove nodes), and deploy and orchestrate services across the swarm.</p>\n<p>  When you run Docker without using swarm mode, you execute container commands. When you run the Docker in swarm mode, you orchestrate services. You can run swarm services and standalone containers on the same Docker instances.</p>\n</li>\n<li><p>What is a node?</p>\n<p>  A node is an instance of the Docker engine participating in the swarm. You can also think of this as a Docker node. You can run one or more nodes on a single physical computer or cloud server, but production swarm deployments typically include Docker nodes distributed across multiple physical and cloud machines.</p>\n<p>  To deploy your application to a swarm, you submit a service definition to a manager node. The manager node dispatches units of work called tasks to worker nodes.</p>\n<p>  Manager nodes also perform the orchestration and cluster management functions required to maintain the desired state of the swarm. Manager nodes elect a single leader to conduct orchestration tasks.</p>\n<p>  Worker nodes receive and execute tasks dispatched from manager nodes. By default manager nodes also run services as worker nodes, but you can configure them to run manager tasks exclusively and be manager-only nodes. An agent runs on each worker node and reports on the tasks assigned to it. The worker node notifies the manager node of the current state of its assigned tasks so that the manager can maintain the desired state of each worker.</p>\n</li>\n<li><p>Services and tasks</p>\n<p>  A service is the definition of the tasks to execute on the worker nodes. It is the central structure of the swarm system and the primary root of user interaction with the swarm.</p>\n<p>  When you create a service, you specify which container image to use and which commands to execute inside running containers.</p>\n<p>  In the replicated services model, the swarm manager distributes a specific number of replica tasks among the nodes based upon the scale you set in the desired state.</p>\n<p>  For global services, the swarm runs one task for the service on every available node in the cluster.</p>\n<p>  A task carries a Docker container and the commands to run inside the container. It is the atomic scheduling unit of swarm. Manager nodes assign tasks to worker nodes according to the number of replicas set in the service scale. Once a task is assigned to a node, it cannot move to another node. It can only run on the assigned node or fail.</p>\n</li>\n<li><p>Load balancing</p>\n<p>  The swarm manager uses ingress load balancing to expose the services you want to make available externally to the swarm. The swarm manager can automatically assign the service a PublishedPort or you can configure a PublishedPort for the service. You can specify any unused port. If you do not specify a port, the swarm manager assigns the service a port in the 30000-32767 range.</p>\n<p>  External components, such as cloud load balancers, can access the service on the PublishedPort of any node in the cluster whether or not the node is currently running the task for the service. All nodes in the swarm route ingress connections to a running task instance.</p>\n<p>  Swarm mode has an internal DNS component that automatically assigns each service in the swarm a DNS entry. The swarm manager uses internal load balancing to distribute requests among services within the cluster based upon the DNS name of the service.</p>\n</li>\n</ul>\n<h1 id=\"docker-swarm-mode-command\"><a href=\"#docker-swarm-mode-command\" class=\"headerlink\" title=\"docker swarm mode command\"></a>docker swarm mode command</h1><ul>\n<li><p>swarm init</p>\n</li>\n<li><p>swarm join</p>\n</li>\n<li><p>service create</p>\n</li>\n<li><p>service inspect</p>\n</li>\n<li><p>service ls</p>\n</li>\n<li><p>service rm</p>\n</li>\n<li><p>service scale</p>\n</li>\n<li><p>service ps</p>\n</li>\n<li><p>service update</p>\n</li>\n</ul>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"ciypwm370000621sv5wqww7o5","category_id":"ciypwm36s000321sv2ayq74mg","_id":"ciypwm37f000c21sv2in622km"},{"post_id":"ciypwm36l000121sv4vi8ieps","category_id":"ciypwm36s000321sv2ayq74mg","_id":"ciypwm37k000h21sv2rz3wkhl"},{"post_id":"ciypwm373000721sve8sra0p8","category_id":"ciypwm36s000321sv2ayq74mg","_id":"ciypwm37n000k21svh1v35cu8"},{"post_id":"ciypwm36p000221svq1phbbk0","category_id":"ciypwm36s000321sv2ayq74mg","_id":"ciypwm37r000p21svxoyermy2"},{"post_id":"ciypwm36y000521svf4gipt4m","category_id":"ciypwm36s000321sv2ayq74mg","_id":"ciypwm37v000t21svxfxwbwdz"},{"post_id":"ciypwm37s000r21svd81wuly4","category_id":"ciypwm37q000n21sv6dlm59y0","_id":"ciypwm380000z21svrp0qp30g"},{"post_id":"ciypwm37b000b21svs9suqb35","category_id":"ciypwm37q000n21sv6dlm59y0","_id":"ciypwm386001421svy7qnw6ow"},{"post_id":"ciypwm37g000e21sv4w08jqwz","category_id":"ciypwm37x000v21svrncf6pj1","_id":"ciypwm38e001921svvzpfj3wu"},{"post_id":"ciypwm37k000j21svx26uzv21","category_id":"ciypwm37q000n21sv6dlm59y0","_id":"ciypwm38k001g21sv7ribpger"},{"post_id":"ciypwm38b001821svqerj6t12","category_id":"ciypwm37q000n21sv6dlm59y0","_id":"ciypwm38o001l21svbbd3dx99"},{"post_id":"ciypwm37n000m21svlowv6kfh","category_id":"ciypwm37x000v21svrncf6pj1","_id":"ciypwm38q001o21svkwebhqmb"},{"post_id":"ciypwm37v000u21svtfny6hag","category_id":"ciypwm38k001h21svxlgxac0o","_id":"ciypwm390001v21sv5xbutbf0"},{"post_id":"ciypwm37z000y21sv7ku7mwqz","category_id":"ciypwm38k001h21svxlgxac0o","_id":"ciypwm396002221sv9m6lwult"},{"post_id":"ciypwm381001121svtap4yfqx","category_id":"ciypwm38k001h21svxlgxac0o","_id":"ciypwm39c002921svdo50m0mz"},{"post_id":"ciypwm39a002821svvdj89we5","category_id":"ciypwm396002421svh3qvjzup","_id":"ciypwm39h002g21svd8575f2r"},{"post_id":"ciypwm387001621sv0tyyvktn","category_id":"ciypwm396002421svh3qvjzup","_id":"ciypwm39l002l21sv3qvwb52d"},{"post_id":"ciypwm39c002b21svtsnp2crr","category_id":"ciypwm396002421svh3qvjzup","_id":"ciypwm39o002o21sv43kxy575"},{"post_id":"ciypwm39e002f21svqd89nzvh","category_id":"ciypwm396002421svh3qvjzup","_id":"ciypwm39s002s21svgisawsid"},{"post_id":"ciypwm38f001d21svebn6o2x0","category_id":"ciypwm396002421svh3qvjzup","_id":"ciypwm39x002v21sv6um6u400"},{"post_id":"ciypwm39i002i21sv9msto5pk","category_id":"ciypwm396002421svh3qvjzup","_id":"ciypwm3a2003021svvbegg3ew"},{"post_id":"ciypwm39m002n21svzvub4bz1","category_id":"ciypwm396002421svh3qvjzup","_id":"ciypwm3ad003321svfkzibr6y"},{"post_id":"ciypwm38h001f21sv8w2usj1k","category_id":"ciypwm396002421svh3qvjzup","_id":"ciypwm3ah003721svgctupmel"},{"post_id":"ciypwm39p002q21svs69ki7jr","category_id":"ciypwm396002421svh3qvjzup","_id":"ciypwm3ak003b21svezheb98a"},{"post_id":"ciypwm39s002u21svylq4icy5","category_id":"ciypwm396002421svh3qvjzup","_id":"ciypwm3am003f21svyfb1bso1"},{"post_id":"ciypwm38l001k21svmq9vzpvt","category_id":"ciypwm396002421svh3qvjzup","_id":"ciypwm3ao003j21svykzf8kik"},{"post_id":"ciypwm39x002x21svqqrk3m16","category_id":"ciypwm396002421svh3qvjzup","_id":"ciypwm3aq003n21sv57qqwtj4"},{"post_id":"ciypwm3a2003221svf7toen7b","category_id":"ciypwm396002421svh3qvjzup","_id":"ciypwm3av003q21sv4655peal"},{"post_id":"ciypwm38t001s21svt0sqa4iq","category_id":"ciypwm396002421svh3qvjzup","_id":"ciypwm3ax003u21svml52h4kq"},{"post_id":"ciypwm3ae003521svyblhs7lq","category_id":"ciypwm396002421svh3qvjzup","_id":"ciypwm3b1003y21svizhfs2et"},{"post_id":"ciypwm3ai003921sv5f0f4edv","category_id":"ciypwm396002421svh3qvjzup","_id":"ciypwm3b4004221svxr2czuu1"},{"post_id":"ciypwm38y001u21svs7mji43n","category_id":"ciypwm396002421svh3qvjzup","_id":"ciypwm3b6004521svmhcjtgvs"},{"post_id":"ciypwm3al003d21svs6i4yavl","category_id":"ciypwm396002421svh3qvjzup","_id":"ciypwm3b9004a21sv6b26t5hr"},{"post_id":"ciypwm3an003h21sv7b0gw30l","category_id":"ciypwm37x000v21svrncf6pj1","_id":"ciypwm3bc004d21sv2fk442tj"},{"post_id":"ciypwm391001y21svngtxbmr2","category_id":"ciypwm3am003e21svty5ib55t","_id":"ciypwm3be004h21sv0onu7yv3"},{"post_id":"ciypwm3ao003l21svrwyk2i8l","category_id":"ciypwm396002421svh3qvjzup","_id":"ciypwm3bf004l21sv9592jem1"},{"post_id":"ciypwm394002121svo7zb0zb3","category_id":"ciypwm396002421svh3qvjzup","_id":"ciypwm3bi004o21svkpp36ghb"},{"post_id":"ciypwm3aw003s21svc9u1t1uj","category_id":"ciypwm396002421svh3qvjzup","_id":"ciypwm3bo004s21sv0cjth70b"},{"post_id":"ciypwm3ay003w21svpn73kvxk","category_id":"ciypwm37x000v21svrncf6pj1","_id":"ciypwm3br004v21svmuq2po06"},{"post_id":"ciypwm396002521svmkmo7uob","category_id":"ciypwm396002421svh3qvjzup","_id":"ciypwm3bv005021svqe9z702f"},{"post_id":"ciypwm3b4004421svqhoqjae5","category_id":"ciypwm36s000321sv2ayq74mg","_id":"ciypwm3by005321svq3h76wzi"},{"post_id":"ciypwm3as003p21sv6fbp39yp","category_id":"ciypwm3b3004121svtkx85z4d","_id":"ciypwm3c1005721sv6rxllkw4"},{"post_id":"ciypwm3b2004021svioptmeei","category_id":"ciypwm3b9004921svnu7nqm17","_id":"ciypwm3c4005b21sv833gpiuv"},{"post_id":"ciypwm3be004k21svfutfhg13","category_id":"ciypwm38k001h21svxlgxac0o","_id":"ciypwm3c7005f21sv8sqlhn2u"},{"post_id":"ciypwm3bo004u21sve7kv8amn","category_id":"ciypwm36s000321sv2ayq74mg","_id":"ciypwm3cd005n21svt30z6gya"},{"post_id":"ciypwm3ba004c21svocwumvw5","category_id":"ciypwm3bj004q21sve4wap485","_id":"ciypwm3cd005q21svuo138kpp"},{"post_id":"ciypwm3bs004x21svpb3i7i4k","category_id":"ciypwm36s000321sv2ayq74mg","_id":"ciypwm3ce005s21sv4ndhbq84"},{"post_id":"ciypwm3bw005221sv9xsoxqtn","category_id":"ciypwm36s000321sv2ayq74mg","_id":"ciypwm3ce005w21svs54wwwg7"},{"post_id":"ciypwm3bc004f21svgewuytn4","category_id":"ciypwm3bj004q21sve4wap485","_id":"ciypwm3cg005y21sv6xgpjf9v"},{"post_id":"ciypwm3bg004n21sv7l1ddxrz","category_id":"ciypwm3c0005621svwg9s5r8n","_id":"ciypwm3ch006221sv5e2oylst"},{"post_id":"ciypwm3bj004r21svidlm72tf","category_id":"ciypwm3c6005e21sv23z2ug6y","_id":"ciypwm3ch006421svxmtw5qim"},{"post_id":"ciypwm3bz005521sv6q6whiyk","category_id":"ciypwm3cd005m21sva7pdlkq9","_id":"ciypwm3ci006721svt5y7rxqp"},{"post_id":"ciypwm3c2005921sv50p1s021","category_id":"ciypwm3ce005u21sv403nox6t","_id":"ciypwm3ci006a21svs32vbnui"},{"post_id":"ciypwm3c4005d21sv93q4gepl","category_id":"ciypwm3cg006021svbgr4bjye","_id":"ciypwm3cj006d21svikgkoqz0"},{"post_id":"ciypwm3c7005h21svamd3gs1p","category_id":"ciypwm3cg006021svbgr4bjye","_id":"ciypwm3cj006g21sv20sxx8yw"},{"post_id":"ciypwm3cb005l21svlf0gc6a4","category_id":"ciypwm3ci006c21svajl2sgwk","_id":"ciypwm3cj006j21sv3m3o43yb"},{"post_id":"cj11isjnf0004bcsvmdovywqw","category_id":"ciypwm396002421svh3qvjzup","_id":"cj11isjnw000abcsvec3m7xr0"},{"post_id":"cj11isjnk0005bcsv2dbd47px","category_id":"ciypwm3bj004q21sve4wap485","_id":"cj11isjo1000dbcsv5bniegkm"},{"post_id":"cj11isjlc0000bcsvn0f1zmvv","category_id":"cj11isjlr0002bcsv83i8b4xi","_id":"cj11isjo5000gbcsva1rbtuj9"},{"post_id":"cj11isjny000cbcsvon4zx324","category_id":"ciypwm396002421svh3qvjzup","_id":"cj11isjo6000ibcsv5470mthd"},{"post_id":"cj11isjll0001bcsvhyi9d342","category_id":"cj11isjnu0008bcsv1k7za5xf","_id":"cj11isjo6000kbcsv2dxvfxzn"},{"post_id":"cj11isjnr0007bcsv5hm8gpd7","category_id":"cj11isjo3000fbcsvru41hd89","_id":"cj11isjo7000nbcsv2kid09uv"},{"post_id":"ciypwm3b7004721sv1225ic1c","category_id":"cj11isjrj000pbcsvkw8id6en","_id":"cj11isjs0000qbcsvwzxulb5w"}],"PostTag":[{"post_id":"ciypwm370000621sv5wqww7o5","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm37a000a21svfax58516"},{"post_id":"ciypwm36l000121sv4vi8ieps","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm37f000d21sv7fk525fo"},{"post_id":"ciypwm373000721sve8sra0p8","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm37k000i21svnad3ijp2"},{"post_id":"ciypwm36p000221svq1phbbk0","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm37n000l21svsaspgr17"},{"post_id":"ciypwm37g000e21sv4w08jqwz","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm37r000q21svw41qxuqt"},{"post_id":"ciypwm36y000521svf4gipt4m","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm37u000s21svdyy1ze7s"},{"post_id":"ciypwm37s000r21svd81wuly4","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm37y000x21svbru4huzo"},{"post_id":"ciypwm37v000u21svtfny6hag","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm380001021svtybgpqsz"},{"post_id":"ciypwm37z000y21sv7ku7mwqz","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm386001521svutmja09k"},{"post_id":"ciypwm37b000b21svs9suqb35","tag_id":"ciypwm37r000o21svjcqqxc6i","_id":"ciypwm38a001721svbmdelerh"},{"post_id":"ciypwm37b000b21svs9suqb35","tag_id":"ciypwm37y000w21sv7akqrkp4","_id":"ciypwm38f001c21svvt4h2mm7"},{"post_id":"ciypwm381001121svtap4yfqx","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm38h001e21svn5ld5b06"},{"post_id":"ciypwm387001621sv0tyyvktn","tag_id":"ciypwm37r000o21svjcqqxc6i","_id":"ciypwm38k001i21svqkuzu8tv"},{"post_id":"ciypwm38b001821svqerj6t12","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm38o001m21svg8cviaig"},{"post_id":"ciypwm38f001d21svebn6o2x0","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm38s001p21svh9m4ut1m"},{"post_id":"ciypwm37k000j21svx26uzv21","tag_id":"ciypwm37r000o21svjcqqxc6i","_id":"ciypwm38x001t21svfn04cqko"},{"post_id":"ciypwm37k000j21svx26uzv21","tag_id":"ciypwm38f001b21svlvklr72p","_id":"ciypwm390001w21svzftkbwhz"},{"post_id":"ciypwm38t001s21svt0sqa4iq","tag_id":"ciypwm37r000o21svjcqqxc6i","_id":"ciypwm394002021svo8n48ckh"},{"post_id":"ciypwm37n000m21svlowv6kfh","tag_id":"ciypwm37r000o21svjcqqxc6i","_id":"ciypwm396002321sva7axz9ya"},{"post_id":"ciypwm37n000m21svlowv6kfh","tag_id":"ciypwm38s001r21svydvoae1d","_id":"ciypwm39a002721sv1pflqbvt"},{"post_id":"ciypwm391001y21svngtxbmr2","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm39c002a21svaw84tgku"},{"post_id":"ciypwm394002121svo7zb0zb3","tag_id":"ciypwm392001z21svrcw0otfu","_id":"ciypwm39e002e21svowo1d3vo"},{"post_id":"ciypwm38h001f21sv8w2usj1k","tag_id":"ciypwm392001z21svrcw0otfu","_id":"ciypwm39i002h21svuf36v2oh"},{"post_id":"ciypwm396002521svmkmo7uob","tag_id":"ciypwm392001z21svrcw0otfu","_id":"ciypwm39l002m21svnll9m813"},{"post_id":"ciypwm39a002821svvdj89we5","tag_id":"ciypwm392001z21svrcw0otfu","_id":"ciypwm39o002p21svotoxav00"},{"post_id":"ciypwm38l001k21svmq9vzpvt","tag_id":"ciypwm392001z21svrcw0otfu","_id":"ciypwm39s002t21sv69d0gzxb"},{"post_id":"ciypwm39e002f21svqd89nzvh","tag_id":"ciypwm392001z21svrcw0otfu","_id":"ciypwm39x002w21svj85eaeh5"},{"post_id":"ciypwm38y001u21svs7mji43n","tag_id":"ciypwm39e002d21sv64407bht","_id":"ciypwm3a2003121svv17iivnk"},{"post_id":"ciypwm39i002i21sv9msto5pk","tag_id":"ciypwm37r000o21svjcqqxc6i","_id":"ciypwm3ae003421svlmlryek5"},{"post_id":"ciypwm39m002n21svzvub4bz1","tag_id":"ciypwm392001z21svrcw0otfu","_id":"ciypwm3ah003821svmtv2qne3"},{"post_id":"ciypwm39c002b21svtsnp2crr","tag_id":"ciypwm37r000o21svjcqqxc6i","_id":"ciypwm3ak003c21svikhitx11"},{"post_id":"ciypwm39c002b21svtsnp2crr","tag_id":"ciypwm39l002k21svytq32fhf","_id":"ciypwm3am003g21sv3pyhg7yw"},{"post_id":"ciypwm39p002q21svs69ki7jr","tag_id":"ciypwm37r000o21svjcqqxc6i","_id":"ciypwm3ao003k21sv2n3jtmop"},{"post_id":"ciypwm3ae003521svyblhs7lq","tag_id":"ciypwm392001z21svrcw0otfu","_id":"ciypwm3ar003o21sve5zw7sjz"},{"post_id":"ciypwm39s002u21svylq4icy5","tag_id":"ciypwm37r000o21svjcqqxc6i","_id":"ciypwm3av003r21svguznlq80"},{"post_id":"ciypwm39s002u21svylq4icy5","tag_id":"ciypwm3a1002z21svtd90b889","_id":"ciypwm3ax003v21svqkqk8ot1"},{"post_id":"ciypwm3ai003921sv5f0f4edv","tag_id":"ciypwm37r000o21svjcqqxc6i","_id":"ciypwm3b1003z21svx4pe0b8t"},{"post_id":"ciypwm3al003d21svs6i4yavl","tag_id":"ciypwm392001z21svrcw0otfu","_id":"ciypwm3b4004321sv6e3a8xso"},{"post_id":"ciypwm39x002x21svqqrk3m16","tag_id":"ciypwm37r000o21svjcqqxc6i","_id":"ciypwm3b6004621sv1joiocxx"},{"post_id":"ciypwm39x002x21svqqrk3m16","tag_id":"ciypwm3ak003a21svg0ygjid2","_id":"ciypwm3b9004b21svgaibm1bp"},{"post_id":"ciypwm3an003h21sv7b0gw30l","tag_id":"ciypwm37r000o21svjcqqxc6i","_id":"ciypwm3bc004e21svh9h6tpw1"},{"post_id":"ciypwm3an003h21sv7b0gw30l","tag_id":"ciypwm38s001r21svydvoae1d","_id":"ciypwm3be004i21svu5c090n6"},{"post_id":"ciypwm3ao003l21svrwyk2i8l","tag_id":"ciypwm392001z21svrcw0otfu","_id":"ciypwm3bg004m21sv3asght9w"},{"post_id":"ciypwm3a2003221svf7toen7b","tag_id":"ciypwm37r000o21svjcqqxc6i","_id":"ciypwm3bi004p21svf6eui4kf"},{"post_id":"ciypwm3a2003221svf7toen7b","tag_id":"ciypwm3ao003i21svh8r94vgn","_id":"ciypwm3bo004t21sveo6cxfit"},{"post_id":"ciypwm3ay003w21svpn73kvxk","tag_id":"ciypwm37r000o21svjcqqxc6i","_id":"ciypwm3br004w21svoy8j8l2c"},{"post_id":"ciypwm3ay003w21svpn73kvxk","tag_id":"ciypwm38s001r21svydvoae1d","_id":"ciypwm3bv005121svtvhgykh1"},{"post_id":"ciypwm3b4004421svqhoqjae5","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm3by005421svwqniij1v"},{"post_id":"ciypwm3aw003s21svc9u1t1uj","tag_id":"ciypwm37r000o21svjcqqxc6i","_id":"ciypwm3c1005821svzddb5nmn"},{"post_id":"ciypwm3aw003s21svc9u1t1uj","tag_id":"ciypwm3b1003x21svxhs5szvn","_id":"ciypwm3c4005c21sv1swcumb5"},{"post_id":"ciypwm3ba004c21svocwumvw5","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm3c7005g21sv5397cxqt"},{"post_id":"ciypwm3b2004021svioptmeei","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm3cb005k21svp39qfahn"},{"post_id":"ciypwm3b2004021svioptmeei","tag_id":"ciypwm3b8004821sva121xkab","_id":"ciypwm3cd005o21svt9yhmohq"},{"post_id":"ciypwm3bc004f21svgewuytn4","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm3cd005r21sv2id72j8f"},{"post_id":"ciypwm3be004k21svfutfhg13","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm3ce005t21sv1bcnptke"},{"post_id":"ciypwm3b7004721sv1225ic1c","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm3ce005x21svaaixto5z"},{"post_id":"ciypwm3bg004n21sv7l1ddxrz","tag_id":"ciypwm37r000o21svjcqqxc6i","_id":"ciypwm3ch006321svis6jaywg"},{"post_id":"ciypwm3bj004r21svidlm72tf","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm3ch006521svaju372ai"},{"post_id":"ciypwm3bw005221sv9xsoxqtn","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm3ci006821sv2jnf3mrg"},{"post_id":"ciypwm3bz005521sv6q6whiyk","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm3ci006b21svxcw5s4lc"},{"post_id":"ciypwm3bo004u21sve7kv8amn","tag_id":"ciypwm37r000o21svjcqqxc6i","_id":"ciypwm3cj006e21sv7st5k6qa"},{"post_id":"ciypwm3bo004u21sve7kv8amn","tag_id":"ciypwm3bv004z21sveko5655v","_id":"ciypwm3cj006f21svanymt0fg"},{"post_id":"ciypwm3c7005h21svamd3gs1p","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm3cj006h21sv8g5sjcth"},{"post_id":"ciypwm3cb005l21svlf0gc6a4","tag_id":"ciypwm36x000421svx7frtbsl","_id":"ciypwm3cj006i21sv1tsxgij9"},{"post_id":"ciypwm3bs004x21svpb3i7i4k","tag_id":"ciypwm3c3005a21sv2ovu098c","_id":"ciypwm3cj006k21sv64jn4hkr"},{"post_id":"ciypwm3bs004x21svpb3i7i4k","tag_id":"ciypwm3ca005i21svhxwhovao","_id":"ciypwm3cj006l21svqskl39j0"},{"post_id":"ciypwm3bs004x21svpb3i7i4k","tag_id":"ciypwm3bv004z21sveko5655v","_id":"ciypwm3cj006m21sv7vrhnwhx"},{"post_id":"ciypwm3bs004x21svpb3i7i4k","tag_id":"ciypwm3ce005v21svhpc2mlny","_id":"ciypwm3cj006n21sv6iyou2rp"},{"post_id":"ciypwm3c2005921sv50p1s021","tag_id":"ciypwm37r000o21svjcqqxc6i","_id":"ciypwm3cj006o21svh2tjl2ht"},{"post_id":"ciypwm3c2005921sv50p1s021","tag_id":"ciypwm3ch006121svvkfcwhkd","_id":"ciypwm3cj006p21sv7bmlqcn8"},{"post_id":"ciypwm3c4005d21sv93q4gepl","tag_id":"ciypwm37r000o21svjcqqxc6i","_id":"ciypwm3ck006q21svda2o4obb"},{"post_id":"ciypwm3c4005d21sv93q4gepl","tag_id":"ciypwm3ci006921sv33rfphoh","_id":"ciypwm3ck006r21svm3332mu0"},{"post_id":"cj11isjnf0004bcsvmdovywqw","tag_id":"ciypwm37r000o21svjcqqxc6i","_id":"cj11isjnq0006bcsvphc2u46l"},{"post_id":"cj11isjnk0005bcsv2dbd47px","tag_id":"ciypwm36x000421svx7frtbsl","_id":"cj11isjnw000bbcsv9d6dsd3v"},{"post_id":"cj11isjlc0000bcsvn0f1zmvv","tag_id":"ciypwm36x000421svx7frtbsl","_id":"cj11isjo2000ebcsv6yndxhn3"},{"post_id":"cj11isjlc0000bcsvn0f1zmvv","tag_id":"cj11isjnc0003bcsvrgubpilm","_id":"cj11isjo5000hbcsvs088qnqq"},{"post_id":"cj11isjnr0007bcsv5hm8gpd7","tag_id":"ciypwm36x000421svx7frtbsl","_id":"cj11isjo6000jbcsvctuo8djo"},{"post_id":"cj11isjny000cbcsvon4zx324","tag_id":"ciypwm37r000o21svjcqqxc6i","_id":"cj11isjo6000lbcsv9bw9ysng"},{"post_id":"cj11isjny000cbcsvon4zx324","tag_id":"ciypwm3b1003x21svxhs5szvn","_id":"cj11isjo6000mbcsvqpm11z5x"},{"post_id":"cj11isjll0001bcsvhyi9d342","tag_id":"cj11isjnu0009bcsvra3ovxbm","_id":"cj11isjo7000obcsv0zi9u2ht"}],"Tag":[{"name":"portal","_id":"ciypwm36x000421svx7frtbsl"},{"name":"core","_id":"ciypwm37r000o21svjcqqxc6i"},{"name":"cloudfoundry","_id":"ciypwm37y000w21sv7akqrkp4"},{"name":"openshift","_id":"ciypwm38f001b21svlvklr72p"},{"name":"openstack","_id":"ciypwm38s001r21svydvoae1d"},{"name":"detail","_id":"ciypwm392001z21svrcw0otfu"},{"name":"cookbook","_id":"ciypwm39e002d21sv64407bht"},{"name":"filesystem","_id":"ciypwm39l002k21svytq32fhf"},{"name":"security","_id":"ciypwm3a1002z21svtd90b889"},{"name":"network","_id":"ciypwm3ak003a21svg0ygjid2"},{"name":"storage","_id":"ciypwm3ao003i21svh8r94vgn"},{"name":"swarm","_id":"ciypwm3b1003x21svxhs5szvn"},{"name":"architect","_id":"ciypwm3b8004821sva121xkab"},{"name":"django","_id":"ciypwm3bv004z21sveko5655v"},{"name":"install","_id":"ciypwm3c3005a21sv2ovu098c"},{"name":"python","_id":"ciypwm3ca005i21svhxwhovao"},{"name":"docker","_id":"ciypwm3ce005v21svhpc2mlny"},{"name":"rails","_id":"ciypwm3ch006121svvkfcwhkd"},{"name":"http","_id":"ciypwm3ci006921sv33rfphoh"},{"name":"blockchain","_id":"cj11isjnc0003bcsvrgubpilm"},{"name":"opensource","_id":"cj11isjnu0009bcsvra3ovxbm"}]}}